{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aexomir/AML_mistake_detection/blob/feat%2Ferror-types/notebooks/colab_error_type_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGHCNkL_vlgs"
      },
      "source": [
        "# AML/DAAI 2025 - Error Type Analysis on Colab\n",
        "\n",
        "This notebook runs error-type analysis on the mistake detection models.\n",
        "\n",
        "## What this notebook does:\n",
        "1. **Setup**: Clone repository and install dependencies\n",
        "2. **Load Data**: Features, checkpoints, and annotations\n",
        "3. **Run Evaluations**: Evaluate models with error-type analysis\n",
        "4. **View Results**: Display and download error-type analysis results\n",
        "\n",
        "## Prerequisites:\n",
        "You need to have:\n",
        "- Pre-extracted features (Omnivore and SlowFast) in `.npz` format\n",
        "- Checkpoints from the official release (`error_recognition_best` directory)\n",
        "- Annotation files (should be in the repository or uploaded separately)\n",
        "\n",
        "## Quick Start:\n",
        "1. Upload your data to Google Drive (or use direct upload)\n",
        "2. Configure paths in Section 1\n",
        "3. Run all cells sequentially\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb_gLLx7vlgt"
      },
      "source": [
        "## 1. Setup: Clone Repository & Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8iqJg5ZFvlgu",
        "outputId": "ebde2c84-96f3-4a85-bd90-07df94729bb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repository URL: https://github.com/aexomir/AML_mistake_detection.git\n",
            "Repository branch: feat/error-types\n",
            "Repository directory: aml_repo_v2\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# CONFIGURE YOUR REPOSITORY\n",
        "# ============================================\n",
        "# Option 1: Clone from GitHub (recommended)\n",
        "REPO_URL = \"https://github.com/aexomir/AML_mistake_detection.git\"\n",
        "REPO_BRANCH = \"feat/error-types\"  # Leave empty for default branch, or specify branch name\n",
        "\n",
        "# Option 2: Manual upload - set REPO_URL to empty string and upload files manually\n",
        "# REPO_URL = \"\"\n",
        "\n",
        "REPO_DIR = \"aml_repo_v2\"\n",
        "\n",
        "print(f\"Repository URL: {REPO_URL if REPO_URL else 'Manual upload mode'}\")\n",
        "print(f\"Repository branch: {REPO_BRANCH if REPO_BRANCH else 'default'}\")\n",
        "print(f\"Repository directory: {REPO_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ou4ioWUZvlgu",
        "outputId": "71f67a4f-e1d8-44f3-b319-78e7db2da5ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing existing aml_repo_v2 directory...\n",
            "Cloning repository from https://github.com/aexomir/AML_mistake_detection.git...\n",
            "✓ Repository cloned successfully\n",
            "Checking out branch: feat/error-types\n",
            "✓ Switched to branch: feat/error-types\n",
            "\n",
            "✓ Changed to directory: /content/aml_repo_v2\n",
            "\n",
            "Repository contents:\n",
            "total 6032\n",
            "drwxr-xr-x 8 root root    4096 Dec 15 20:41 .\n",
            "drwxr-xr-x 1 root root    4096 Dec 15 20:41 ..\n",
            "-rw-r--r-- 1 root root 6042142 Dec 15 20:41 3_Mistake_Detection.pdf\n",
            "-rw-r--r-- 1 root root   28923 Dec 15 20:41 base.py\n",
            "-rw-r--r-- 1 root root    1940 Dec 15 20:41 constants.py\n",
            "drwxr-xr-x 3 root root    4096 Dec 15 20:41 core\n",
            "drwxr-xr-x 2 root root    4096 Dec 15 20:41 dataloader\n",
            "-rw-r--r-- 1 root root    6148 Dec 15 20:41 .DS_Store\n",
            "drwxr-xr-x 2 root root    4096 Dec 15 20:41 er_annotations\n",
            "drwxr-xr-x 8 root root    4096 Dec 15 20:41 .git\n",
            "-rw-r--r-- 1 root root      65 Dec 15 20:41 .gitignore\n",
            "-rwxr-xr-x 1 root root    1904 Dec 15 20:41 install_deps.py\n",
            "-rw-r--r-- 1 root root   11357 Dec 15 20:41 LICENSE\n",
            "drwxr-xr-x 2 root root    4096 Dec 15 20:41 notebooks\n",
            "-rw-r--r-- 1 root root    2298 Dec 15 20:41 overview.md\n",
            "-rw-r--r-- 1 root root    2496 Dec 15 20:41 README.md\n",
            "-rw-r--r-- 1 root root     124 Dec 15 20:41 requirements-cpu.txt\n",
            "-rw-r--r-- 1 root root     193 Dec 15 20:41 requirements-cuda.txt\n",
            "-rw-r--r-- 1 root root     304 Dec 15 20:41 requirements.txt\n",
            "drwxr-xr-x 2 root root    4096 Dec 15 20:41 scripts\n",
            "-rw-r--r-- 1 root root    4596 Dec 15 20:41 step_guide.md\n",
            "-rwxr-xr-x 1 root root    1027 Dec 15 20:41 train_er.py\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Remove existing directory if it exists\n",
        "if os.path.exists(REPO_DIR):\n",
        "    print(f\"Removing existing {REPO_DIR} directory...\")\n",
        "    shutil.rmtree(REPO_DIR)\n",
        "\n",
        "# Clone repository\n",
        "if REPO_URL:\n",
        "    print(f\"Cloning repository from {REPO_URL}...\")\n",
        "    clone_cmd = f\"git clone {REPO_URL} {REPO_DIR}\"\n",
        "    result = os.system(clone_cmd)\n",
        "\n",
        "    if result != 0:\n",
        "        print(f\"⚠ Clone failed. Please check the URL or upload files manually.\")\n",
        "        os.makedirs(REPO_DIR, exist_ok=True)\n",
        "    else:\n",
        "        print(\"✓ Repository cloned successfully\")\n",
        "\n",
        "        # Checkout specific branch if specified\n",
        "        if REPO_BRANCH:\n",
        "            print(f\"Checking out branch: {REPO_BRANCH}\")\n",
        "            os.chdir(REPO_DIR)\n",
        "            os.system(f\"git checkout {REPO_BRANCH}\")\n",
        "            os.chdir('..')\n",
        "            print(f\"✓ Switched to branch: {REPO_BRANCH}\")\n",
        "else:\n",
        "    print(\"Manual upload mode: Creating directory...\")\n",
        "    os.makedirs(REPO_DIR, exist_ok=True)\n",
        "\n",
        "# Change to repository directory\n",
        "if os.path.exists(REPO_DIR):\n",
        "    os.chdir(REPO_DIR)\n",
        "    print(f\"\\n✓ Changed to directory: {os.getcwd()}\")\n",
        "    print(f\"\\nRepository contents:\")\n",
        "    !ls -la\n",
        "else:\n",
        "    print(f\"✗ Error: {REPO_DIR} directory not found!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Lko6-T_Dvlgv",
        "outputId": "31df3e57-47c9-47ff-8df3-996964b91925",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content/aml_repo_v2\n",
            "\n",
            "Checking repository structure...\n",
            "✓ Found: core/evaluate.py\n",
            "✓ Found: dataloader\n",
            "✓ Found: base.py\n",
            "✓ Found: constants.py\n",
            "\n",
            "✓ Repository structure looks good!\n"
          ]
        }
      ],
      "source": [
        "# Verify repository structure\n",
        "import os\n",
        "\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "print(f\"\\nChecking repository structure...\")\n",
        "\n",
        "required_items = [\n",
        "    'core/evaluate.py',\n",
        "    'dataloader',\n",
        "    'base.py',\n",
        "    'constants.py'\n",
        "]\n",
        "\n",
        "missing = []\n",
        "for item in required_items:\n",
        "    if os.path.exists(item):\n",
        "        print(f\"✓ Found: {item}\")\n",
        "    else:\n",
        "        print(f\"✗ Missing: {item}\")\n",
        "        missing.append(item)\n",
        "\n",
        "if missing:\n",
        "    print(f\"\\n⚠ Warning: Some required files/directories are missing!\")\n",
        "    print(f\"Please ensure all files are present before proceeding.\")\n",
        "else:\n",
        "    print(f\"\\n✓ Repository structure looks good!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NQVbB80Fvlgv",
        "outputId": "7f9af596-a7d0-48e3-823e-c962719490d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ All dependencies installed successfully\n",
            "\n",
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "CUDA device: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "# Colab comes with PyTorch pre-installed, so we'll work with that\n",
        "# Remove PyTorch version constraints to avoid conflicts\n",
        "if os.path.exists('requirements.txt'):\n",
        "    !sed -i '/^torch==/d' requirements.txt 2>/dev/null || true\n",
        "    !sed -i '/^torchvision==/d' requirements.txt 2>/dev/null || true\n",
        "\n",
        "# Install torcheval (required for evaluation metrics)\n",
        "!pip install -q torcheval\n",
        "\n",
        "# Install all remaining dependencies from requirements.txt\n",
        "if os.path.exists('requirements.txt'):\n",
        "    !pip install -q -r requirements.txt\n",
        "elif os.path.exists('requirements-cpu.txt'):\n",
        "    !pip install -q -r requirements-cpu.txt\n",
        "\n",
        "print(\"✓ All dependencies installed successfully\")\n",
        "\n",
        "# Verify PyTorch installation\n",
        "import torch\n",
        "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vypnzdiyvlgv"
      },
      "source": [
        "## 2. Load Data: Features, Checkpoints, and Annotations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QTHidulMvlgv",
        "outputId": "ef76ed54-45f6-4b9e-fb22-26718cd98375",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data paths configured:\n",
            "  Use Google Drive: True\n",
            "  Omnivore: /content/drive/MyDrive/AML_mistake_detection/omnivore.zip\n",
            "  SlowFast: /content/drive/MyDrive/AML_mistake_detection/slowfast.zip\n",
            "  Checkpoints: /content/drive/MyDrive/AML_mistake_detection/error_recognition_best.zip\n",
            "  Annotations: /content/drive/MyDrive/AML_mistake_detection/annotations\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# CONFIGURE DATA PATHS\n",
        "# ============================================\n",
        "# Option 1: From Google Drive (recommended for large files)\n",
        "USE_GOOGLE_DRIVE = True  # Set to False if uploading directly\n",
        "\n",
        "# Paths on Google Drive (update these to match your Drive structure)\n",
        "OMNIVORE_DRIVE_PATH = \"/content/drive/MyDrive/AML_mistake_detection/omnivore.zip\"  # Can be .zip or directory\n",
        "SLOWFAST_DRIVE_PATH = \"/content/drive/MyDrive/AML_mistake_detection/slowfast.zip\"  # Can be .zip or directory\n",
        "CHECKPOINTS_DRIVE_PATH = \"/content/drive/MyDrive/AML_mistake_detection/error_recognition_best.zip\"  # Can be .zip or directory\n",
        "ANNOTATIONS_DRIVE_PATH = \"/content/drive/MyDrive/AML_mistake_detection/annotations\"  # Optional if in repo\n",
        "\n",
        "# Option 2: Direct upload - set USE_GOOGLE_DRIVE = False and upload files in next cell\n",
        "\n",
        "print(\"Data paths configured:\")\n",
        "print(f\"  Use Google Drive: {USE_GOOGLE_DRIVE}\")\n",
        "print(f\"  Omnivore: {OMNIVORE_DRIVE_PATH}\")\n",
        "print(f\"  SlowFast: {SLOWFAST_DRIVE_PATH}\")\n",
        "print(f\"  Checkpoints: {CHECKPOINTS_DRIVE_PATH}\")\n",
        "print(f\"  Annotations: {ANNOTATIONS_DRIVE_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "b0owBF5Bvlgv",
        "outputId": "b5c4612f-d744-406b-91f9-bf5286046efe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✓ Google Drive mounted\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive if using it\n",
        "if USE_GOOGLE_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"✓ Google Drive mounted\")\n",
        "else:\n",
        "    print(\"⚠ Google Drive not mounted. Please upload files directly using the file browser.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6s_OBdINvlgv",
        "outputId": "7a508645-c9f0-4a18-b5d8-8ff09d5657bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Directory structure created\n"
          ]
        }
      ],
      "source": [
        "# Create data directory structure\n",
        "import os\n",
        "os.makedirs('data/video/omnivore', exist_ok=True)\n",
        "os.makedirs('data/video/slowfast', exist_ok=True)\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "os.makedirs('annotations/annotation_json', exist_ok=True)\n",
        "os.makedirs('annotations/data_splits', exist_ok=True)\n",
        "os.makedirs('er_annotations', exist_ok=True)\n",
        "os.makedirs('results', exist_ok=True)\n",
        "\n",
        "print(\"✓ Directory structure created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gQu4j97qvlgw",
        "outputId": "16b86876-debb-425b-8a50-dc6a47c61a72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Omnivore features from: /content/drive/MyDrive/AML_mistake_detection/omnivore.zip\n",
            "  Detected zip file, extracting...\n",
            "  ✓ Extracted and copied 384 .npz files\n",
            "Loading SlowFast features from: /content/drive/MyDrive/AML_mistake_detection/slowfast.zip\n",
            "  Detected zip file, extracting...\n",
            "  ✓ Extracted and copied 384 .npz files\n",
            "\n",
            "Feature file counts:\n",
            "  Omnivore: 384 .npz files\n",
            "  SlowFast: 384 .npz files\n"
          ]
        }
      ],
      "source": [
        "# Load features from Google Drive or direct upload\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import glob\n",
        "\n",
        "def load_features(source_path, dest_path, feature_name):\n",
        "    \"\"\"Load features from source (zip file or directory) to destination.\"\"\"\n",
        "    if not os.path.exists(source_path):\n",
        "        print(f\"⚠ {feature_name}: Source path not found: {source_path}\")\n",
        "        return False\n",
        "\n",
        "    print(f\"Loading {feature_name} features from: {source_path}\")\n",
        "\n",
        "    # Check if it's a zip file\n",
        "    is_zip = source_path.lower().endswith('.zip') or (os.path.isfile(source_path) and 'zip' in str(source_path))\n",
        "\n",
        "    if is_zip:\n",
        "        print(f\"  Detected zip file, extracting...\")\n",
        "        temp_zip = f'/tmp/{feature_name.lower()}.zip'\n",
        "        temp_extracted = f'/tmp/{feature_name.lower()}_extracted'\n",
        "\n",
        "        try:\n",
        "            shutil.copy(source_path, temp_zip)\n",
        "            subprocess.run(['unzip', '-q', temp_zip, '-d', temp_extracted], check=True)\n",
        "\n",
        "            # Find .npz files in extracted directory\n",
        "            npz_files = glob.glob(os.path.join(temp_extracted, '**/*.npz'), recursive=True)\n",
        "\n",
        "            if npz_files:\n",
        "                # Copy all .npz files to destination\n",
        "                for npz_file in npz_files:\n",
        "                    shutil.copy2(npz_file, dest_path)\n",
        "                print(f\"  ✓ Extracted and copied {len(npz_files)} .npz files\")\n",
        "\n",
        "                # Cleanup\n",
        "                shutil.rmtree(temp_extracted, ignore_errors=True)\n",
        "                os.remove(temp_zip)\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"  ⚠ No .npz files found in extracted zip\")\n",
        "                shutil.rmtree(temp_extracted, ignore_errors=True)\n",
        "                os.remove(temp_zip)\n",
        "                return False\n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ Error extracting {feature_name} zip: {e}\")\n",
        "            if os.path.exists(temp_extracted):\n",
        "                shutil.rmtree(temp_extracted, ignore_errors=True)\n",
        "            if os.path.exists(temp_zip):\n",
        "                os.remove(temp_zip)\n",
        "            return False\n",
        "    else:\n",
        "        # It's a directory\n",
        "        print(f\"  Detected directory, copying .npz files...\")\n",
        "        npz_files = glob.glob(os.path.join(source_path, '**/*.npz'), recursive=True)\n",
        "\n",
        "        if npz_files:\n",
        "            # Copy all .npz files to destination\n",
        "            for npz_file in npz_files:\n",
        "                shutil.copy2(npz_file, dest_path)\n",
        "            print(f\"  ✓ Copied {len(npz_files)} .npz files\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"  ⚠ No .npz files found in {source_path}\")\n",
        "            return False\n",
        "\n",
        "# Load Omnivore and SlowFast features\n",
        "if USE_GOOGLE_DRIVE:\n",
        "    load_features(OMNIVORE_DRIVE_PATH, 'data/video/omnivore', 'Omnivore')\n",
        "    load_features(SLOWFAST_DRIVE_PATH, 'data/video/slowfast', 'SlowFast')\n",
        "else:\n",
        "    print(\"⚠ Please upload features manually:\")\n",
        "    print(\"  1. Use the file browser to upload .npz files or zip files\")\n",
        "    print(\"  2. Extract/copy them to data/video/omnivore/ and data/video/slowfast/\")\n",
        "\n",
        "# Verify features\n",
        "omnivore_count = len([f for f in os.listdir('data/video/omnivore') if f.endswith('.npz')]) if os.path.exists('data/video/omnivore') else 0\n",
        "slowfast_count = len([f for f in os.listdir('data/video/slowfast') if f.endswith('.npz')]) if os.path.exists('data/video/slowfast') else 0\n",
        "print(f\"\\nFeature file counts:\")\n",
        "print(f\"  Omnivore: {omnivore_count} .npz files\")\n",
        "print(f\"  SlowFast: {slowfast_count} .npz files\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ACWl92Qevlgw",
        "outputId": "72bfd0ec-3bb4-44b9-c636-1be6f824b25b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoints from: /content/drive/MyDrive/AML_mistake_detection/error_recognition_best.zip\n",
            "Detected zip file, extracting...\n",
            "Copying from: /tmp/checkpoints_extracted/error_recognition_best\n",
            "✓ Checkpoints extracted\n",
            "\n",
            "✓ Found 54 checkpoint files\n",
            "\n",
            "Sample checkpoint files:\n",
            "  checkpoints/error_recognition_best/Transformer/imagebind/error_recognition_Transformer_imagebind_audio_recordings_epoch_31.pt\n",
            "  checkpoints/error_recognition_best/Transformer/imagebind/error_recognition_Transformer_imagebind_audio_step_epoch_44.pt\n",
            "  checkpoints/error_recognition_best/Transformer/imagebind/error_recognition_Transformer_imagebind_video_audio_step_epoch_9.pt\n"
          ]
        }
      ],
      "source": [
        "# Load checkpoints\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "\n",
        "checkpoint_path = CHECKPOINTS_DRIVE_PATH if USE_GOOGLE_DRIVE else None\n",
        "\n",
        "if checkpoint_path and os.path.exists(checkpoint_path):\n",
        "    print(f\"Loading checkpoints from: {checkpoint_path}\")\n",
        "\n",
        "    # Check if it's a zip file\n",
        "    is_zip = checkpoint_path.lower().endswith('.zip') or (os.path.isfile(checkpoint_path) and 'zip' in str(checkpoint_path))\n",
        "\n",
        "    if is_zip:\n",
        "        print(\"Detected zip file, extracting...\")\n",
        "        shutil.copy(checkpoint_path, '/tmp/checkpoints.zip')\n",
        "\n",
        "        try:\n",
        "            subprocess.run(['unzip', '-q', '/tmp/checkpoints.zip', '-d', '/tmp/checkpoints_extracted'], check=True)\n",
        "\n",
        "            # Find error_recognition_best directory\n",
        "            extracted_base = '/tmp/checkpoints_extracted'\n",
        "            extracted_path = None\n",
        "\n",
        "            # Check common locations\n",
        "            if os.path.exists(os.path.join(extracted_base, 'error_recognition_best')):\n",
        "                extracted_path = os.path.join(extracted_base, 'error_recognition_best')\n",
        "            elif os.path.exists(os.path.join(extracted_base, 'MLP')) or os.path.exists(os.path.join(extracted_base, 'Transformer')):\n",
        "                extracted_path = extracted_base\n",
        "            else:\n",
        "                # Search recursively\n",
        "                for root, dirs, files in os.walk(extracted_base):\n",
        "                    if 'error_recognition_best' in dirs:\n",
        "                        extracted_path = os.path.join(root, 'error_recognition_best')\n",
        "                        break\n",
        "                    if 'MLP' in dirs or 'Transformer' in dirs:\n",
        "                        extracted_path = root\n",
        "                        break\n",
        "\n",
        "                if extracted_path is None:\n",
        "                    extracted_path = extracted_base\n",
        "\n",
        "            print(f\"Copying from: {extracted_path}\")\n",
        "            shutil.copytree(extracted_path, 'checkpoints/error_recognition_best', dirs_exist_ok=True)\n",
        "\n",
        "            # Cleanup\n",
        "            shutil.rmtree('/tmp/checkpoints_extracted', ignore_errors=True)\n",
        "            os.remove('/tmp/checkpoints.zip')\n",
        "            print(\"✓ Checkpoints extracted\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Error extracting checkpoints: {e}\")\n",
        "    else:\n",
        "        # It's a directory\n",
        "        print(\"Detected directory, copying...\")\n",
        "        if os.path.basename(checkpoint_path) == 'error_recognition_best':\n",
        "            shutil.copytree(checkpoint_path, 'checkpoints/error_recognition_best', dirs_exist_ok=True)\n",
        "        else:\n",
        "            os.makedirs('checkpoints/error_recognition_best', exist_ok=True)\n",
        "            for item in os.listdir(checkpoint_path):\n",
        "                src = os.path.join(checkpoint_path, item)\n",
        "                dst = os.path.join('checkpoints/error_recognition_best', item)\n",
        "                if os.path.isdir(src):\n",
        "                    shutil.copytree(src, dst, dirs_exist_ok=True)\n",
        "                else:\n",
        "                    shutil.copy2(src, dst)\n",
        "        print(\"✓ Checkpoints copied\")\n",
        "else:\n",
        "    print(\"⚠ Checkpoints not found. Please upload manually:\")\n",
        "    print(\"  1. Download from: https://utdallas.app.box.com/s/uz3s1alrzucz03sleify8kazhuc1ksl3\")\n",
        "    print(\"  2. Extract error_recognition_best directory\")\n",
        "    print(\"  3. Upload to checkpoints/error_recognition_best/\")\n",
        "\n",
        "# Verify checkpoints\n",
        "if os.path.exists('checkpoints/error_recognition_best'):\n",
        "    pt_files = []\n",
        "    for root, dirs, files in os.walk('checkpoints/error_recognition_best'):\n",
        "        pt_files.extend([os.path.join(root, f) for f in files if f.endswith('.pt')])\n",
        "    print(f\"\\n✓ Found {len(pt_files)} checkpoint files\")\n",
        "    if pt_files:\n",
        "        print(\"\\nSample checkpoint files:\")\n",
        "        for f in pt_files[:3]:\n",
        "            print(f\"  {f}\")\n",
        "else:\n",
        "    print(\"\\n✗ Checkpoints directory not found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rWwn3onhvlgw",
        "outputId": "534d3532-0e1c-4b26-e02a-83f1adde0885",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading annotations from: /content/drive/MyDrive/AML_mistake_detection/annotations\n",
            "  ✓ Copied error_category_idx.json\n",
            "  ✓ Copied activity_idx_step_idx.json\n",
            "  ✓ Copied error_annotations.json\n",
            "  ✓ Copied step_annotations.json\n",
            "  ✓ Copied complete_step_annotations.json\n",
            "  ✓ Copied step_idx_description.json\n",
            "  ✓ Copied recording_id_step_idx.json\n",
            "  ✓ Copied environment_data_split_combined.json\n",
            "  ✓ Copied person_data_split_normal.json\n",
            "  ✓ Copied recordings_data_split_normal.json\n",
            "  ✓ Copied person_data_split_combined.json\n",
            "  ✓ Copied environment_data_split_normal.json\n",
            "  ✓ Copied recordings_data_split_combined.json\n",
            "  ✓ Copied recipes_data_split_normal.json\n",
            "  ✓ Copied recipes_data_split_combined.json\n",
            "\n",
            "Verifying annotation files...\n",
            "✓ Found: annotations/annotation_json/step_annotations.json\n",
            "✓ Found: annotations/annotation_json/error_annotations.json\n",
            "✓ Found: er_annotations/recordings_combined_splits.json\n",
            "\n",
            "✓ All required annotation files are present!\n"
          ]
        }
      ],
      "source": [
        "# Load annotations (if not already in repository)\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "if USE_GOOGLE_DRIVE and os.path.exists(ANNOTATIONS_DRIVE_PATH):\n",
        "    print(f\"Loading annotations from: {ANNOTATIONS_DRIVE_PATH}\")\n",
        "\n",
        "    # Copy annotation_json\n",
        "    annotation_json_src = os.path.join(ANNOTATIONS_DRIVE_PATH, 'annotation_json')\n",
        "    if os.path.exists(annotation_json_src):\n",
        "        for file in os.listdir(annotation_json_src):\n",
        "            src = os.path.join(annotation_json_src, file)\n",
        "            dst = os.path.join('annotations/annotation_json', file)\n",
        "            if os.path.isfile(src):\n",
        "                shutil.copy2(src, dst)\n",
        "                print(f\"  ✓ Copied {file}\")\n",
        "\n",
        "    # Copy data_splits\n",
        "    data_splits_src = os.path.join(ANNOTATIONS_DRIVE_PATH, 'data_splits')\n",
        "    if os.path.exists(data_splits_src):\n",
        "        for file in os.listdir(data_splits_src):\n",
        "            src = os.path.join(data_splits_src, file)\n",
        "            dst = os.path.join('annotations/data_splits', file)\n",
        "            if os.path.isfile(src):\n",
        "                shutil.copy2(src, dst)\n",
        "                print(f\"  ✓ Copied {file}\")\n",
        "\n",
        "    # Copy er_annotations\n",
        "    er_annotations_src = os.path.join(ANNOTATIONS_DRIVE_PATH, 'er_annotations')\n",
        "    if os.path.exists(er_annotations_src):\n",
        "        for file in os.listdir(er_annotations_src):\n",
        "            src = os.path.join(er_annotations_src, file)\n",
        "            dst = os.path.join('er_annotations', file)\n",
        "            if os.path.isfile(src):\n",
        "                shutil.copy2(src, dst)\n",
        "                print(f\"  ✓ Copied {file}\")\n",
        "else:\n",
        "    print(\"⚠ Annotations not found in Drive. Checking repository...\")\n",
        "\n",
        "# Verify required annotation files\n",
        "print(\"\\nVerifying annotation files...\")\n",
        "required_files = [\n",
        "    'annotations/annotation_json/step_annotations.json',\n",
        "    'annotations/annotation_json/error_annotations.json',\n",
        "    'er_annotations/recordings_combined_splits.json'\n",
        "]\n",
        "\n",
        "missing = []\n",
        "for file in required_files:\n",
        "    if os.path.exists(file):\n",
        "        print(f\"✓ Found: {file}\")\n",
        "    else:\n",
        "        print(f\"✗ Missing: {file}\")\n",
        "        missing.append(file)\n",
        "\n",
        "if missing:\n",
        "    print(f\"\\n⚠ Warning: {len(missing)} required annotation file(s) are missing!\")\n",
        "    print(\"Please ensure these files are available before running evaluations.\")\n",
        "else:\n",
        "    print(\"\\n✓ All required annotation files are present!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B7rlVgdvlgw"
      },
      "source": [
        "## 3. Run Error-Type Analysis Evaluations\n",
        "\n",
        "Run evaluations with error-type analysis. The results will include:\n",
        "- Standard metrics (sub-step and step level)\n",
        "- **Error-type analysis** showing performance per error category\n",
        "\n",
        "**Note**: Use threshold 0.6 for `step` split and 0.4 for `recordings` split.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "y4rozjjDvlgw",
        "outputId": "cde2c79b-4a3f-43ad-f8c0-e736a16568a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded annotations...... \n",
            "Loading recording ids from recordings_combined_splits.json\n",
            "test Progress: 42347/798: 100% 798/798 [00:10<00:00, 76.45it/s]\n",
            "----------------------------------------------------------------\n",
            "test Sub Step Level Metrics: {'precision': 0.4096162736939436, 'recall': 0.2989708115404083, 'f1': 0.3456549302643129, 'accuracy': 0.6831416629277163, 'auc': np.float64(0.6541560352028618), 'pr_auc': tensor(0.3187)}\n",
            "test Step Level Metrics: {'precision': 0.6607142857142857, 'recall': 0.14859437751004015, 'f1': 0.24262295081967214, 'accuracy': 0.7105263157894737, 'auc': np.float64(0.7573902166041213), 'pr_auc': tensor(0.3638)}\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "test Error Type Analysis:\n",
            "----------------------------------------------------------------\n",
            "Error Type                Count    Precision    Recall       F1           Accuracy     AUC         \n",
            "-----------------------------------------------------------------------------------------------\n",
            "Measurement Error         42       1.0000       0.0952       0.1739       0.0952       0.0000      \n",
            "No Error                  626      0.4062       0.1688       0.2385       0.8674       0.7691      \n",
            "Preparation Error         49       1.0000       0.1633       0.2807       0.1633       0.0000      \n",
            "Technique Error           62       1.0000       0.2258       0.3684       0.2258       0.0000      \n",
            "Temperature Error         8        0.0000       0.0000       0.0000       0.0000       0.0000      \n",
            "Timing Error              34       1.0000       0.0588       0.1111       0.0588       0.0000      \n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Example 1: Omnivore - MLP - Step split\n",
        "# This will show error-type analysis in the output\n",
        "!python -m core.evaluate \\\n",
        "    --split step \\\n",
        "    --backbone omnivore \\\n",
        "    --variant MLP \\\n",
        "    --ckpt checkpoints/error_recognition_best/MLP/omnivore/error_recognition_MLP_omnivore_step_epoch_43.pt \\\n",
        "    --threshold 0.6\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "q7ptVjxUvlgw",
        "outputId": "b61c61db-0e46-48c1-a011-711a40e2bdc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded annotations...... \n",
            "Loading recording ids from recordings_combined_splits.json\n",
            "test Progress: 42347/798: 100% 798/798 [00:05<00:00, 133.97it/s]\n",
            "----------------------------------------------------------------\n",
            "test Sub Step Level Metrics: {'precision': 0.4445452483556362, 'recall': 0.6613801248523705, 'f1': 0.5317056629365887, 'accuracy': 0.6738848088412402, 'auc': np.float64(0.7461755308526944), 'pr_auc': tensor(0.3888)}\n",
            "test Step Level Metrics: {'precision': 0.5155709342560554, 'recall': 0.5983935742971888, 'f1': 0.5539033457249071, 'accuracy': 0.6992481203007519, 'auc': np.float64(0.7561832027563805), 'pr_auc': tensor(0.4338)}\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "test Error Type Analysis:\n",
            "----------------------------------------------------------------\n",
            "Error Type                Count    Precision    Recall       F1           Accuracy     AUC         \n",
            "-----------------------------------------------------------------------------------------------\n",
            "Measurement Error         42       1.0000       0.6429       0.7826       0.6429       0.0000      \n",
            "No Error                  626      0.2632       0.6494       0.3745       0.7332       0.7805      \n",
            "Preparation Error         49       1.0000       0.6122       0.7595       0.6122       0.0000      \n",
            "Technique Error           62       1.0000       0.5161       0.6809       0.5161       0.0000      \n",
            "Temperature Error         8        1.0000       0.5000       0.6667       0.5000       0.0000      \n",
            "Timing Error              34       1.0000       0.6176       0.7636       0.6176       0.0000      \n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Example 2: Omnivore - Transformer - Step split\n",
        "# This will show error-type analysis in the output\n",
        "!python -m core.evaluate \\\n",
        "    --split step \\\n",
        "    --backbone omnivore \\\n",
        "    --variant Transformer \\\n",
        "    --ckpt checkpoints/error_recognition_best/Transformer/omnivore/error_recognition_Transformer_omnivore_step_epoch_9.pt \\\n",
        "    --threshold 0.6\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_ehoeNe8vlgw",
        "outputId": "89456406-1d75-4251-8218-75290ccc3206",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded annotations...... \n",
            "Loading recording ids from recordings_combined_splits.json\n",
            "test Progress: 38340/671: 100% 671/671 [00:04<00:00, 150.57it/s]\n",
            "----------------------------------------------------------------\n",
            "test Sub Step Level Metrics: {'precision': 0.3964945261528254, 'recall': 0.5688109780280797, 'f1': 0.46727266803505685, 'accuracy': 0.5735263432446531, 'auc': np.float64(0.5988330748775713), 'pr_auc': tensor(0.3673)}\n",
            "test Step Level Metrics: {'precision': 0.4090909090909091, 'recall': 0.8589211618257261, 'f1': 0.5542168674698795, 'accuracy': 0.503725782414307, 'auc': np.float64(0.6302808067162018), 'pr_auc': tensor(0.4020)}\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "test Error Type Analysis:\n",
            "----------------------------------------------------------------\n",
            "Error Type                Count    Precision    Recall       F1           Accuracy     AUC         \n",
            "-----------------------------------------------------------------------------------------------\n",
            "Measurement Error         50       1.0000       0.9200       0.9583       0.9200       0.0000      \n",
            "No Error                  501      0.1577       0.7887       0.2629       0.3733       0.5829      \n",
            "Preparation Error         49       1.0000       0.8571       0.9231       0.8571       0.0000      \n",
            "Technique Error           59       1.0000       0.8983       0.9464       0.8983       0.0000      \n",
            "Temperature Error         8        1.0000       1.0000       1.0000       1.0000       0.0000      \n",
            "Timing Error              26       1.0000       0.8846       0.9388       0.8846       0.0000      \n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Example 3: Omnivore - MLP - Recordings split\n",
        "# This will show error-type analysis in the output\n",
        "!python -m core.evaluate \\\n",
        "    --split recordings \\\n",
        "    --backbone omnivore \\\n",
        "    --variant MLP \\\n",
        "    --ckpt checkpoints/error_recognition_best/MLP/omnivore/error_recognition_MLP_omnivore_recordings_epoch_33.pt \\\n",
        "    --threshold 0.4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6TD23b14vlgw",
        "outputId": "63303524-b76c-49b7-e327-8c8fd8d1d862",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded annotations...... \n",
            "Loading recording ids from recordings_combined_splits.json\n",
            "test Progress: 42347/798: 100% 798/798 [00:05<00:00, 155.78it/s]\n",
            "----------------------------------------------------------------\n",
            "test Sub Step Level Metrics: {'precision': 0.3910761154855643, 'recall': 0.03770879028176143, 'f1': 0.06878510425482803, 'accuracy': 0.7141946300800529, 'auc': np.float64(0.5777348914133424), 'pr_auc': tensor(0.2841)}\n",
            "test Step Level Metrics: {'precision': 0.31917631917631917, 'recall': 0.9959839357429718, 'f1': 0.4834307992202729, 'accuracy': 0.3358395989974937, 'auc': np.float64(0.6309610024798646), 'pr_auc': tensor(0.3191)}\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "test Error Type Analysis:\n",
            "----------------------------------------------------------------\n",
            "Error Type                Count    Precision    Recall       F1           Accuracy     AUC         \n",
            "-----------------------------------------------------------------------------------------------\n",
            "Measurement Error         42       1.0000       1.0000       1.0000       1.0000       0.0000      \n",
            "No Error                  626      0.1271       1.0000       0.2255       0.1550       0.6301      \n",
            "Preparation Error         49       1.0000       1.0000       1.0000       1.0000       0.0000      \n",
            "Technique Error           62       1.0000       0.9839       0.9919       0.9839       0.0000      \n",
            "Temperature Error         8        1.0000       1.0000       1.0000       1.0000       0.0000      \n",
            "Timing Error              34       1.0000       1.0000       1.0000       1.0000       0.0000      \n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Example 4: SlowFast - MLP - Step split\n",
        "# This will show error-type analysis in the output\n",
        "!python -m core.evaluate \\\n",
        "    --split step \\\n",
        "    --backbone slowfast \\\n",
        "    --variant MLP \\\n",
        "    --ckpt checkpoints/error_recognition_best/MLP/slowfast/error_recognition_MLP_slowfast_step_epoch_15.pt \\\n",
        "    --threshold 0.6\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaTyFUQwvlgw"
      },
      "source": [
        "## 4. View and Download Results\n",
        "\n",
        "The error-type analysis results are saved to CSV files. Let's view and download them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jgAsRiElvlgw",
        "outputId": "28fa4f3d-465c-44c2-e640-6267ae6ab174",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 error-type analysis CSV file(s):\n",
            "\n",
            "  results/error_recognition/error_type_analysis/error_type_analysis_step_True_substep_True_threshold_0.4.csv\n",
            "  results/error_recognition/error_type_analysis/error_type_analysis_step_True_substep_True_threshold_0.6.csv\n"
          ]
        }
      ],
      "source": [
        "# List all error-type analysis results\n",
        "import os\n",
        "import glob\n",
        "\n",
        "results_dir = 'results/error_recognition/error_type_analysis'\n",
        "if os.path.exists(results_dir):\n",
        "    csv_files = glob.glob(os.path.join(results_dir, '*.csv'))\n",
        "    if csv_files:\n",
        "        print(f\"Found {len(csv_files)} error-type analysis CSV file(s):\\n\")\n",
        "        for csv_file in sorted(csv_files):\n",
        "            print(f\"  {csv_file}\")\n",
        "    else:\n",
        "        print(\"No error-type analysis CSV files found.\")\n",
        "        print(\"\\n⚠ Note: If you just ran an evaluation, make sure:\")\n",
        "        print(\"  1. The evaluation completed successfully\")\n",
        "        print(\"  2. Error-type analysis was computed (check the evaluation output)\")\n",
        "        print(\"  3. The dataset contains error category annotations\")\n",
        "else:\n",
        "    print(f\"⚠ Results directory not found: {results_dir}\")\n",
        "    print(\"\\nThis means no evaluations with error-type analysis have been run yet.\")\n",
        "    print(\"Please run the evaluation cells in Section 3 first.\")\n",
        "    print(\"\\nThe results will be saved to:\")\n",
        "    print(f\"  {results_dir}/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NdFsRGiJvlgx",
        "outputId": "b99f49aa-44ac-4bea-db98-8f18afae1723",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Viewing: results/error_recognition/error_type_analysis/error_type_analysis_step_True_substep_True_threshold_0.6.csv\n",
            "\n",
            "Split Backbone     Variant Modality        Error Type  Count  Precision  Recall     F1  Accuracy   AUC  PR AUC\n",
            " step omnivore         MLP    video Measurement Error     42     100.00    9.52  17.39      9.52  0.00  100.00\n",
            " step omnivore         MLP    video          No Error    626      40.62   16.88  23.85     86.74 76.91   17.08\n",
            " step omnivore         MLP    video Preparation Error     49     100.00   16.33  28.07     16.33  0.00  100.00\n",
            " step omnivore         MLP    video   Technique Error     62     100.00   22.58  36.84     22.58  0.00  100.00\n",
            " step omnivore         MLP    video Temperature Error      8       0.00    0.00   0.00      0.00  0.00  100.00\n",
            " step omnivore         MLP    video      Timing Error     34     100.00    5.88  11.11      5.88  0.00  100.00\n",
            " step omnivore Transformer    video Measurement Error     42     100.00   64.29  78.26     64.29  0.00  100.00\n",
            " step omnivore Transformer    video          No Error    626      26.32   64.94  37.45     73.32 78.05   21.40\n",
            " step omnivore Transformer    video Preparation Error     49     100.00   61.22  75.95     61.22  0.00  100.00\n",
            " step omnivore Transformer    video   Technique Error     62     100.00   51.61  68.09     51.61  0.00  100.00\n",
            " step omnivore Transformer    video Temperature Error      8     100.00   50.00  66.67     50.00  0.00  100.00\n",
            " step omnivore Transformer    video      Timing Error     34     100.00   61.76  76.36     61.76  0.00  100.00\n",
            " step slowfast         MLP    video Measurement Error     42     100.00  100.00 100.00    100.00  0.00  100.00\n",
            " step slowfast         MLP    video          No Error    626      12.71  100.00  22.55     15.50 63.01   12.71\n",
            " step slowfast         MLP    video Preparation Error     49     100.00  100.00 100.00    100.00  0.00  100.00\n",
            " step slowfast         MLP    video   Technique Error     62     100.00   98.39  99.19     98.39  0.00  100.00\n",
            " step slowfast         MLP    video Temperature Error      8     100.00  100.00 100.00    100.00  0.00  100.00\n",
            " step slowfast         MLP    video      Timing Error     34     100.00  100.00 100.00    100.00  0.00  100.00\n"
          ]
        }
      ],
      "source": [
        "# View the latest error-type analysis results\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "\n",
        "results_dir = 'results/error_recognition/error_type_analysis'\n",
        "if os.path.exists(results_dir):\n",
        "    csv_files = glob.glob(os.path.join(results_dir, '*.csv'))\n",
        "    if csv_files:\n",
        "        # Get the most recent file\n",
        "        latest_file = max(csv_files, key=os.path.getmtime)\n",
        "        print(f\"Viewing: {latest_file}\\n\")\n",
        "\n",
        "        # Read and display\n",
        "        df = pd.read_csv(latest_file)\n",
        "        print(df.to_string(index=False))\n",
        "    else:\n",
        "        print(\"No CSV files found.\")\n",
        "        print(\"\\n⚠ Note: If you just ran an evaluation, make sure:\")\n",
        "        print(\"  1. The evaluation completed successfully\")\n",
        "        print(\"  2. Error-type analysis was computed (check the evaluation output)\")\n",
        "        print(\"  3. The dataset contains error category annotations\")\n",
        "else:\n",
        "    print(f\"⚠ Results directory not found: {results_dir}\")\n",
        "    print(\"\\nThis means no evaluations with error-type analysis have been run yet.\")\n",
        "    print(\"Please run the evaluation cells in Section 3 first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Vb3cFfcWvlgx",
        "outputId": "cf037122-7ff0-449c-f58c-8bc3bdd38b99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 CSV file(s) to download.\n",
            "Downloading error-type analysis results...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9958b7ee-01fc-421b-b9cf-63c543759863\", \"error_type_analysis_results.zip\", 1123)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Download complete!\n"
          ]
        }
      ],
      "source": [
        "# Download results to your local machine\n",
        "from google.colab import files\n",
        "import shutil\n",
        "import os\n",
        "import glob\n",
        "\n",
        "results_dir = 'results/error_recognition/error_type_analysis'\n",
        "if os.path.exists(results_dir):\n",
        "    csv_files = glob.glob(os.path.join(results_dir, '*.csv'))\n",
        "    if csv_files:\n",
        "        # Create a zip file of all results\n",
        "        zip_path = '/tmp/error_type_analysis_results.zip'\n",
        "        shutil.make_archive(zip_path.replace('.zip', ''), 'zip', results_dir)\n",
        "\n",
        "        print(f\"Found {len(csv_files)} CSV file(s) to download.\")\n",
        "        print(\"Downloading error-type analysis results...\")\n",
        "        files.download(zip_path)\n",
        "        print(\"✓ Download complete!\")\n",
        "    else:\n",
        "        print(\"No CSV files found to download.\")\n",
        "        print(\"\\n⚠ Note: If you just ran an evaluation, make sure:\")\n",
        "        print(\"  1. The evaluation completed successfully\")\n",
        "        print(\"  2. Error-type analysis was computed (check the evaluation output)\")\n",
        "        print(\"  3. The dataset contains error category annotations\")\n",
        "else:\n",
        "    print(f\"⚠ Results directory not found: {results_dir}\")\n",
        "    print(\"\\nThis means no evaluations with error-type analysis have been run yet.\")\n",
        "    print(\"Please run the evaluation cells in Section 3 first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F36jis2hvlgx"
      },
      "source": [
        "## 5. List Available Checkpoints\n",
        "\n",
        "Use this cell to find the correct checkpoint paths for different configurations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xu78f50wvlgx",
        "outputId": "62694f21-d784-4ae6-9b35-a08d75fd5834",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available checkpoints:\n",
            "  checkpoints/error_recognition_best/MLP/3dresnet/error_recognition_MLP_3dresnet_environment_epoch_11.pt\n",
            "  checkpoints/error_recognition_best/MLP/3dresnet/error_recognition_MLP_3dresnet_person_epoch_39.pt\n",
            "  checkpoints/error_recognition_best/MLP/3dresnet/error_recognition_MLP_3dresnet_recordings_epoch_45.pt\n",
            "  checkpoints/error_recognition_best/MLP/3dresnet/error_recognition_MLP_3dresnet_step_epoch_41.pt\n",
            "  checkpoints/error_recognition_best/MLP/imagebind/error_recognition_MLP_imagebind_audio_environment_epoch_50.pt\n",
            "  checkpoints/error_recognition_best/MLP/imagebind/error_recognition_MLP_imagebind_audio_person_epoch_8.pt\n",
            "  checkpoints/error_recognition_best/MLP/imagebind/error_recognition_MLP_imagebind_audio_recordings_epoch_2.pt\n",
            "  checkpoints/error_recognition_best/MLP/imagebind/error_recognition_MLP_imagebind_audio_step_epoch_28.pt\n",
            "  checkpoints/error_recognition_best/MLP/imagebind/error_recognition_MLP_imagebind_video_audio_environment_epoch_31.pt\n",
            "  checkpoints/error_recognition_best/MLP/imagebind/error_recognition_MLP_imagebind_video_audio_person_epoch_21.pt\n",
            "  checkpoints/error_recognition_best/MLP/imagebind/error_recognition_MLP_imagebind_video_audio_recordings_epoch_41.pt\n",
            "  checkpoints/error_recognition_best/MLP/imagebind/error_recognition_MLP_imagebind_video_audio_step_epoch_38.pt\n",
            "  checkpoints/error_recognition_best/MLP/imagebind/error_recognition_MLP_imagebind_video_environment_epoch_11.pt\n",
            "  checkpoints/error_recognition_best/MLP/imagebind/error_recognition_MLP_imagebind_video_person_epoch_48.pt\n",
            "  checkpoints/error_recognition_best/MLP/imagebind/error_recognition_MLP_imagebind_video_recordings_epoch_42.pt\n",
            "  checkpoints/error_recognition_best/MLP/imagebind/error_recognition_MLP_imagebind_video_step_epoch_37.pt\n",
            "  checkpoints/error_recognition_best/MLP/omnivore/error_recognition_MLP_omnivore_environment_epoch_5.pt\n",
            "  checkpoints/error_recognition_best/MLP/omnivore/error_recognition_MLP_omnivore_person_epoch_28.pt\n",
            "  checkpoints/error_recognition_best/MLP/omnivore/error_recognition_MLP_omnivore_recordings_epoch_33.pt\n",
            "  checkpoints/error_recognition_best/MLP/omnivore/error_recognition_MLP_omnivore_step_epoch_43.pt\n",
            "  checkpoints/error_recognition_best/MLP/slowfast/error_recognition_MLP_slowfast_environment_epoch_7.pt\n",
            "  checkpoints/error_recognition_best/MLP/slowfast/error_recognition_MLP_slowfast_person_epoch_3.pt\n",
            "  checkpoints/error_recognition_best/MLP/slowfast/error_recognition_MLP_slowfast_recordings_epoch_31.pt\n",
            "  checkpoints/error_recognition_best/MLP/slowfast/error_recognition_MLP_slowfast_step_epoch_15.pt\n",
            "  checkpoints/error_recognition_best/MLP/x3d/error_recognition_MLP_x3d_environment_epoch_2.pt\n",
            "  checkpoints/error_recognition_best/MLP/x3d/error_recognition_MLP_x3d_person_epoch_13.pt\n",
            "  checkpoints/error_recognition_best/MLP/x3d/error_recognition_MLP_x3d_recordings_epoch_5.pt\n",
            "  checkpoints/error_recognition_best/MLP/x3d/error_recognition_MLP_x3d_step_epoch_15.pt\n",
            "  checkpoints/error_recognition_best/Transformer/3dresnet/error_recognition_Transformer_3dresnet_environment_epoch_39.pt\n",
            "  checkpoints/error_recognition_best/Transformer/3dresnet/error_recognition_Transformer_3dresnet_person_epoch_50.pt\n",
            "  checkpoints/error_recognition_best/Transformer/3dresnet/error_recognition_Transformer_3dresnet_recordings_epoch_25.pt\n",
            "  checkpoints/error_recognition_best/Transformer/3dresnet/error_recognition_Transformer_3dresnet_step_epoch_3.pt\n",
            "  checkpoints/error_recognition_best/Transformer/imagebind/error_recognition_Transformer_imagebind_audio_environment_epoch_4.pt\n",
            "  checkpoints/error_recognition_best/Transformer/imagebind/error_recognition_Transformer_imagebind_audio_person_epoch_19.pt\n",
            "  checkpoints/error_recognition_best/Transformer/imagebind/error_recognition_Transformer_imagebind_audio_recordings_epoch_31.pt\n",
            "  checkpoints/error_recognition_best/Transformer/imagebind/error_recognition_Transformer_imagebind_audio_step_epoch_44.pt\n",
            "  checkpoints/error_recognition_best/Transformer/imagebind/error_recognition_Transformer_imagebind_video_audio_environment_epoch_8.pt\n",
            "  checkpoints/error_recognition_best/Transformer/imagebind/error_recognition_Transformer_imagebind_video_audio_person_epoch_17.pt\n",
            "  checkpoints/error_recognition_best/Transformer/imagebind/error_recognition_Transformer_imagebind_video_audio_recordings_epoch_5.pt\n",
            "  checkpoints/error_recognition_best/Transformer/imagebind/error_recognition_Transformer_imagebind_video_audio_step_epoch_9.pt\n",
            "  checkpoints/error_recognition_best/Transformer/imagebind/error_recognition_Transformer_imagebind_video_recordings_epoch_27.pt\n",
            "  checkpoints/error_recognition_best/Transformer/imagebind/error_recognition_Transformer_imagebind_video_step_epoch_20.pt\n",
            "  checkpoints/error_recognition_best/Transformer/omnivore/error_recognition_Transformer_omnivore_environment_epoch_6.pt\n",
            "  checkpoints/error_recognition_best/Transformer/omnivore/error_recognition_Transformer_omnivore_person_epoch_4.pt\n",
            "  checkpoints/error_recognition_best/Transformer/omnivore/error_recognition_Transformer_omnivore_recordings_epoch_31.pt\n",
            "  checkpoints/error_recognition_best/Transformer/omnivore/error_recognition_Transformer_omnivore_step_epoch_9.pt\n",
            "  checkpoints/error_recognition_best/Transformer/slowfast/error_recognition_Transformer_slowfast_environment_epoch_23.pt\n",
            "  checkpoints/error_recognition_best/Transformer/slowfast/error_recognition_Transformer_slowfast_person_epoch_20.pt\n",
            "  checkpoints/error_recognition_best/Transformer/slowfast/error_recognition_Transformer_slowfast_recordings_epoch_49.pt\n",
            "  checkpoints/error_recognition_best/Transformer/slowfast/error_recognition_Transformer_slowfast_step_epoch_25.pt\n",
            "  checkpoints/error_recognition_best/Transformer/x3d/error_recognition_Transformer_x3d_environment_epoch_49.pt\n",
            "  checkpoints/error_recognition_best/Transformer/x3d/error_recognition_Transformer_x3d_person_epoch_6.pt\n",
            "  checkpoints/error_recognition_best/Transformer/x3d/error_recognition_Transformer_x3d_recordings_epoch_43.pt\n",
            "  checkpoints/error_recognition_best/Transformer/x3d/error_recognition_Transformer_x3d_step_epoch_40.pt\n"
          ]
        }
      ],
      "source": [
        "# List available checkpoints to find correct epoch numbers\n",
        "import os\n",
        "import glob\n",
        "\n",
        "checkpoint_base = 'checkpoints/error_recognition_best'\n",
        "if os.path.exists(checkpoint_base):\n",
        "    print(\"Available checkpoints:\")\n",
        "    for ckpt_file in sorted(glob.glob(os.path.join(checkpoint_base, '**/*.pt'), recursive=True)):\n",
        "        print(f\"  {ckpt_file}\")\n",
        "else:\n",
        "    print(\"Checkpoints directory not found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RVNQ5r1vlgx"
      },
      "source": [
        "## Troubleshooting\n",
        "\n",
        "### Common Issues:\n",
        "\n",
        "1. **Missing features**: Make sure you have uploaded both Omnivore and SlowFast features\n",
        "2. **Missing checkpoints**: Download from the official Box link and extract properly\n",
        "3. **Missing annotations**: Ensure annotation files are in the correct directories\n",
        "4. **Wrong checkpoint path**: Use the cell above to list available checkpoints and update paths\n",
        "5. **CUDA errors**: The code will automatically fall back to CPU if CUDA is not available\n",
        "6. **No error-type analysis output**: Make sure you're using the latest code with error-type analysis support\n",
        "\n",
        "### Expected Output:\n",
        "\n",
        "When running evaluations, you should see:\n",
        "1. Standard metrics (sub-step and step level)\n",
        "2. **Error Type Analysis** table showing:\n",
        "   - Error Type (Technique Error, Preparation Error, etc.)\n",
        "   - Count (number of steps with that error type)\n",
        "   - Precision, Recall, F1, Accuracy, AUC for each error type\n",
        "\n",
        "### Getting Help:\n",
        "- Check the README.md in the repository\n",
        "- Verify all file paths are correct\n",
        "- Ensure all dependencies are installed\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}