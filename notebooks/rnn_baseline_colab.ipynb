{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aexomir/AML_mistake_detection/blob/feat%2Frnn/notebooks/rnn_baseline_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTvg0mxC__sk"
      },
      "source": [
        "# RNN Baseline Training for CaptainCook4D SupervisedER\n",
        "\n",
        "This notebook trains the V_RNN (RNN/LSTM) baseline for mistake detection and compares it against V1 (MLP) and V2 (Transformer) baselines.\n",
        "\n",
        "## What this notebook does:\n",
        "1. **Setup**: Clone repository and install dependencies\n",
        "2. **Load Data**: Load features, annotations, and optionally checkpoints from Google Drive\n",
        "3. **Train**: Train the RNN baseline model\n",
        "4. **Evaluate**: Evaluate the trained model\n",
        "5. **Compare**: Compare results against V1 (MLP) and V2 (Transformer) baselines\n",
        "\n",
        "## Prerequisites:\n",
        "You need to have:\n",
        "- Pre-extracted features (Omnivore and SlowFast) in `.npz` format or zip files\n",
        "- Annotation files (should be in the repository or uploaded separately)\n",
        "- (Optional) Pre-trained checkpoints for comparison\n",
        "\n",
        "## Quick Start:\n",
        "1. Configure paths in Section 1\n",
        "2. Run all cells sequentially\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF5PTHDr__sl",
        "outputId": "d20554fc-5361-4541-c5d1-0d074ec2f4ee"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CONFIGURE YOUR REPOSITORY\n",
        "# ============================================\n",
        "# Option 1: Clone from GitHub (recommended)\n",
        "REPO_URL = \"https://github.com/aexomir/AML_mistake_detection.git\"\n",
        "REPO_BRANCH = \"feat/rnn\"  # Leave empty for default branch, or specify branch name\n",
        "\n",
        "# Option 2: Manual upload - set REPO_URL to empty string and upload files manually\n",
        "# REPO_URL = \"\"\n",
        "\n",
        "REPO_DIR = \"code\"\n",
        "\n",
        "print(f\"Repository URL: {REPO_URL if REPO_URL else 'Manual upload mode'}\")\n",
        "print(f\"Repository branch: {REPO_BRANCH if REPO_BRANCH else 'default'}\")\n",
        "print(f\"Repository directory: {REPO_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTGummcG__sm",
        "outputId": "a3c574a2-d6ee-4dda-e123-2a29507355f7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Remove existing directory if it exists\n",
        "if os.path.exists(REPO_DIR):\n",
        "    print(f\"Removing existing {REPO_DIR} directory...\")\n",
        "    shutil.rmtree(REPO_DIR)\n",
        "\n",
        "# Clone repository\n",
        "if REPO_URL:\n",
        "    print(f\"Cloning repository from {REPO_URL}...\")\n",
        "    clone_cmd = f\"git clone {REPO_URL} {REPO_DIR}\"\n",
        "    result = os.system(clone_cmd)\n",
        "\n",
        "    if result != 0:\n",
        "        print(f\"⚠ Clone failed. Please check the URL or upload files manually.\")\n",
        "        os.makedirs(REPO_DIR, exist_ok=True)\n",
        "    else:\n",
        "        print(\"✓ Repository cloned successfully\")\n",
        "\n",
        "        # Checkout specific branch if specified\n",
        "        if REPO_BRANCH:\n",
        "            print(f\"Checking out branch: {REPO_BRANCH}\")\n",
        "            os.chdir(REPO_DIR)\n",
        "            os.system(f\"git checkout {REPO_BRANCH}\")\n",
        "            os.chdir('..')\n",
        "            print(f\"✓ Switched to branch: {REPO_BRANCH}\")\n",
        "else:\n",
        "    print(\"Manual upload mode: Creating directory...\")\n",
        "    os.makedirs(REPO_DIR, exist_ok=True)\n",
        "\n",
        "# Change to repository directory\n",
        "if os.path.exists(REPO_DIR):\n",
        "    os.chdir(REPO_DIR)\n",
        "    print(f\"\\n✓ Changed to directory: {os.getcwd()}\")\n",
        "    print(f\"\\nRepository contents:\")\n",
        "    !ls -la\n",
        "else:\n",
        "    print(f\"✗ Error: {REPO_DIR} directory not found!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hXJexbo__sm",
        "outputId": "a2631425-4e09-4720-9a56-85e4695c93b6"
      },
      "outputs": [],
      "source": [
        "# Verify repository structure\n",
        "import os\n",
        "\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "print(f\"\\nChecking repository structure...\")\n",
        "\n",
        "required_items = [\n",
        "    'scripts/train_rnn_baseline.py',\n",
        "    'core/evaluate.py',\n",
        "    'dataloader',\n",
        "    'base.py',\n",
        "    'constants.py'\n",
        "]\n",
        "\n",
        "missing = []\n",
        "for item in required_items:\n",
        "    if os.path.exists(item):\n",
        "        print(f\"✓ Found: {item}\")\n",
        "    else:\n",
        "        print(f\"✗ Missing: {item}\")\n",
        "        missing.append(item)\n",
        "\n",
        "if missing:\n",
        "    print(f\"\\n⚠ Warning: Some required files/directories are missing!\")\n",
        "    print(f\"Please ensure all files are present before proceeding.\")\n",
        "else:\n",
        "    print(f\"\\n✓ Repository structure looks good!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HX_mVWn__sn",
        "outputId": "10902de3-fc33-4dc5-c502-d365a6bfbcde"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "# Colab comes with PyTorch pre-installed, so we'll work with that\n",
        "# Remove PyTorch version constraints to avoid conflicts\n",
        "if os.path.exists('requirements.txt'):\n",
        "    !sed -i '/^torch==/d' requirements.txt 2>/dev/null || true\n",
        "    !sed -i '/^torchvision==/d' requirements.txt 2>/dev/null || true\n",
        "\n",
        "# Install torcheval (required for evaluation metrics)\n",
        "!pip install -q torcheval\n",
        "\n",
        "# Install all remaining dependencies from requirements.txt\n",
        "if os.path.exists('requirements.txt'):\n",
        "    !pip install -q -r requirements.txt\n",
        "elif os.path.exists('requirements-cpu.txt'):\n",
        "    !pip install -q -r requirements-cpu.txt\n",
        "\n",
        "# Install additional dependencies for RNN baseline\n",
        "!pip install -q wandb loguru\n",
        "\n",
        "print(\"✓ All dependencies installed successfully\")\n",
        "\n",
        "# Verify PyTorch installation\n",
        "import torch\n",
        "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKPcoP7W__sn",
        "outputId": "a822637c-24e6-4df6-8b30-cd237c915346"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CONFIGURE DATA PATHS\n",
        "# ============================================\n",
        "# Option 1: From Google Drive (recommended for large files)\n",
        "USE_GOOGLE_DRIVE = True  # Set to False if uploading directly\n",
        "\n",
        "# Paths on Google Drive (update these to match your Drive structure)\n",
        "OMNIVORE_DRIVE_PATH = \"/content/drive/MyDrive/AML_mistake_detection/omnivore.zip\"  # Can be .zip or directory\n",
        "SLOWFAST_DRIVE_PATH = \"/content/drive/MyDrive/AML_mistake_detection/slowfast.zip\"  # Can be .zip or directory\n",
        "CHECKPOINTS_DRIVE_PATH = \"/content/drive/MyDrive/AML_mistake_detection/error_recognition_best.zip\"  # Can be .zip or directory\n",
        "ANNOTATIONS_DRIVE_PATH = \"/content/drive/MyDrive/AML_mistake_detection/annotations\"  # Optional if in repo\n",
        "\n",
        "# Option 2: Direct upload - set USE_GOOGLE_DRIVE = False and upload files in next cell\n",
        "\n",
        "print(\"Data paths configured:\")\n",
        "print(f\"  Use Google Drive: {USE_GOOGLE_DRIVE}\")\n",
        "print(f\"  Omnivore: {OMNIVORE_DRIVE_PATH}\")\n",
        "print(f\"  SlowFast: {SLOWFAST_DRIVE_PATH}\")\n",
        "print(f\"  Checkpoints: {CHECKPOINTS_DRIVE_PATH}\")\n",
        "print(f\"  Annotations: {ANNOTATIONS_DRIVE_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX45xwkr__sn"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Cffl82S__sn"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4Va6eSf__sn",
        "outputId": "ca4ff0d2-648c-428a-cbab-5e591f6f84dd"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive if using it\n",
        "if USE_GOOGLE_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"✓ Google Drive mounted\")\n",
        "else:\n",
        "    print(\"⚠ Google Drive not mounted. Please upload files directly using the file browser.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBc3c5pX__so",
        "outputId": "15276510-4980-4660-defa-85be0a81f8b2"
      },
      "outputs": [],
      "source": [
        "# Create data directory structure\n",
        "import os\n",
        "os.makedirs('data/video/omnivore', exist_ok=True)\n",
        "os.makedirs('data/video/slowfast', exist_ok=True)\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "os.makedirs('annotations/annotation_json', exist_ok=True)\n",
        "os.makedirs('annotations/data_splits', exist_ok=True)\n",
        "os.makedirs('er_annotations', exist_ok=True)\n",
        "\n",
        "print(\"✓ Directory structure created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDnhdR6d__so",
        "outputId": "4eb32606-5eae-4a9b-a267-0c428203780c"
      },
      "outputs": [],
      "source": [
        "# Load features from Google Drive or direct upload\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import glob\n",
        "\n",
        "def load_features(source_path, dest_path, feature_name):\n",
        "    \"\"\"Load features from source (zip file or directory) to destination.\"\"\"\n",
        "    if not os.path.exists(source_path):\n",
        "        print(f\"⚠ {feature_name}: Source path not found: {source_path}\")\n",
        "        return False\n",
        "\n",
        "    print(f\"Loading {feature_name} features from: {source_path}\")\n",
        "\n",
        "    # Check if it's a zip file\n",
        "    is_zip = source_path.lower().endswith('.zip') or (os.path.isfile(source_path) and 'zip' in str(source_path))\n",
        "\n",
        "    if is_zip:\n",
        "        print(f\"  Detected zip file, extracting...\")\n",
        "        temp_zip = f'/tmp/{feature_name.lower()}.zip'\n",
        "        temp_extracted = f'/tmp/{feature_name.lower()}_extracted'\n",
        "\n",
        "        try:\n",
        "            shutil.copy(source_path, temp_zip)\n",
        "            subprocess.run(['unzip', '-q', temp_zip, '-d', temp_extracted], check=True)\n",
        "\n",
        "            # Find .npz files in extracted directory\n",
        "            npz_files = glob.glob(os.path.join(temp_extracted, '**/*.npz'), recursive=True)\n",
        "\n",
        "            if npz_files:\n",
        "                # Copy all .npz files to destination\n",
        "                for npz_file in npz_files:\n",
        "                    shutil.copy2(npz_file, dest_path)\n",
        "                print(f\"  ✓ Extracted and copied {len(npz_files)} .npz files\")\n",
        "\n",
        "                # Cleanup\n",
        "                shutil.rmtree(temp_extracted, ignore_errors=True)\n",
        "                os.remove(temp_zip)\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"  ⚠ No .npz files found in extracted zip\")\n",
        "                shutil.rmtree(temp_extracted, ignore_errors=True)\n",
        "                os.remove(temp_zip)\n",
        "                return False\n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ Error extracting {feature_name} zip: {e}\")\n",
        "            if os.path.exists(temp_extracted):\n",
        "                shutil.rmtree(temp_extracted, ignore_errors=True)\n",
        "            if os.path.exists(temp_zip):\n",
        "                os.remove(temp_zip)\n",
        "            return False\n",
        "    else:\n",
        "        # It's a directory\n",
        "        print(f\"  Detected directory, copying .npz files...\")\n",
        "        npz_files = glob.glob(os.path.join(source_path, '**/*.npz'), recursive=True)\n",
        "\n",
        "        if npz_files:\n",
        "            # Copy all .npz files to destination\n",
        "            for npz_file in npz_files:\n",
        "                shutil.copy2(npz_file, dest_path)\n",
        "            print(f\"  ✓ Copied {len(npz_files)} .npz files\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"  ⚠ No .npz files found in {source_path}\")\n",
        "            return False\n",
        "\n",
        "# Load Omnivore and SlowFast features\n",
        "if USE_GOOGLE_DRIVE:\n",
        "    load_features(OMNIVORE_DRIVE_PATH, 'data/video/omnivore', 'Omnivore')\n",
        "    load_features(SLOWFAST_DRIVE_PATH, 'data/video/slowfast', 'SlowFast')\n",
        "else:\n",
        "    print(\"⚠ Please upload features manually:\")\n",
        "    print(\"  1. Use the file browser to upload .npz files or zip files\")\n",
        "    print(\"  2. Extract/copy them to data/video/omnivore/ and data/video/slowfast/\")\n",
        "\n",
        "# Verify features\n",
        "omnivore_count = len([f for f in os.listdir('data/video/omnivore') if f.endswith('.npz')]) if os.path.exists('data/video/omnivore') else 0\n",
        "slowfast_count = len([f for f in os.listdir('data/video/slowfast') if f.endswith('.npz')]) if os.path.exists('data/video/slowfast') else 0\n",
        "print(f\"\\nFeature file counts:\")\n",
        "print(f\"  Omnivore: {omnivore_count} .npz files\")\n",
        "print(f\"  SlowFast: {slowfast_count} .npz files\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2IpUDQ3__so",
        "outputId": "ce63c428-e458-49ee-cf44-2123d8f9aff4"
      },
      "outputs": [],
      "source": [
        "# Load annotations (if not already in repository)\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "if USE_GOOGLE_DRIVE and os.path.exists(ANNOTATIONS_DRIVE_PATH):\n",
        "    print(f\"Loading annotations from: {ANNOTATIONS_DRIVE_PATH}\")\n",
        "\n",
        "    # Copy annotation_json\n",
        "    annotation_json_src = os.path.join(ANNOTATIONS_DRIVE_PATH, 'annotation_json')\n",
        "    if os.path.exists(annotation_json_src):\n",
        "        for file in os.listdir(annotation_json_src):\n",
        "            src = os.path.join(annotation_json_src, file)\n",
        "            dst = os.path.join('annotations/annotation_json', file)\n",
        "            if os.path.isfile(src):\n",
        "                shutil.copy2(src, dst)\n",
        "                print(f\"  ✓ Copied {file}\")\n",
        "\n",
        "    # Copy data_splits\n",
        "    data_splits_src = os.path.join(ANNOTATIONS_DRIVE_PATH, 'data_splits')\n",
        "    if os.path.exists(data_splits_src):\n",
        "        for file in os.listdir(data_splits_src):\n",
        "            src = os.path.join(data_splits_src, file)\n",
        "            dst = os.path.join('annotations/data_splits', file)\n",
        "            if os.path.isfile(src):\n",
        "                shutil.copy2(src, dst)\n",
        "                print(f\"  ✓ Copied {file}\")\n",
        "\n",
        "    # Copy er_annotations\n",
        "    er_annotations_src = os.path.join(ANNOTATIONS_DRIVE_PATH, 'er_annotations')\n",
        "    if os.path.exists(er_annotations_src):\n",
        "        for file in os.listdir(er_annotations_src):\n",
        "            src = os.path.join(er_annotations_src, file)\n",
        "            dst = os.path.join('er_annotations', file)\n",
        "            if os.path.isfile(src):\n",
        "                shutil.copy2(src, dst)\n",
        "                print(f\"  ✓ Copied {file}\")\n",
        "else:\n",
        "    print(\"⚠ Annotations not found in Drive. Checking repository...\")\n",
        "\n",
        "# Verify required annotation files\n",
        "print(\"\\nVerifying annotation files...\")\n",
        "required_files = [\n",
        "    'annotations/annotation_json/step_annotations.json',\n",
        "    'annotations/annotation_json/error_annotations.json',\n",
        "    'er_annotations/recordings_combined_splits.json'\n",
        "]\n",
        "\n",
        "missing = []\n",
        "for file in required_files:\n",
        "    if os.path.exists(file):\n",
        "        print(f\"✓ Found: {file}\")\n",
        "    else:\n",
        "        print(f\"✗ Missing: {file}\")\n",
        "        missing.append(file)\n",
        "\n",
        "if missing:\n",
        "    print(f\"\\n⚠ Warning: {len(missing)} required annotation file(s) are missing!\")\n",
        "    print(\"Please ensure these files are available before running training.\")\n",
        "else:\n",
        "    print(\"\\n✓ All required annotation files are present!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lQqJnhT__so",
        "outputId": "e852ad69-5ecb-4475-fcdd-ac2142026d89"
      },
      "outputs": [],
      "source": [
        "# Load checkpoints (optional - for comparison with existing baselines)\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "\n",
        "checkpoint_path = CHECKPOINTS_DRIVE_PATH if USE_GOOGLE_DRIVE else None\n",
        "\n",
        "if checkpoint_path and os.path.exists(checkpoint_path):\n",
        "    print(f\"Loading checkpoints from: {checkpoint_path}\")\n",
        "\n",
        "    # Check if it's a zip file\n",
        "    is_zip = checkpoint_path.lower().endswith('.zip') or (os.path.isfile(checkpoint_path) and 'zip' in str(checkpoint_path))\n",
        "\n",
        "    if is_zip:\n",
        "        print(\"Detected zip file, extracting...\")\n",
        "        shutil.copy(checkpoint_path, '/tmp/checkpoints.zip')\n",
        "\n",
        "        try:\n",
        "            subprocess.run(['unzip', '-q', '/tmp/checkpoints.zip', '-d', '/tmp/checkpoints_extracted'], check=True)\n",
        "\n",
        "            # Find error_recognition_best directory\n",
        "            extracted_base = '/tmp/checkpoints_extracted'\n",
        "            extracted_path = None\n",
        "\n",
        "            # Check common locations\n",
        "            if os.path.exists(os.path.join(extracted_base, 'error_recognition_best')):\n",
        "                extracted_path = os.path.join(extracted_base, 'error_recognition_best')\n",
        "            elif os.path.exists(os.path.join(extracted_base, 'MLP')) or os.path.exists(os.path.join(extracted_base, 'Transformer')):\n",
        "                extracted_path = extracted_base\n",
        "            else:\n",
        "                # Search recursively\n",
        "                for root, dirs, files in os.walk(extracted_base):\n",
        "                    if 'error_recognition_best' in dirs:\n",
        "                        extracted_path = os.path.join(root, 'error_recognition_best')\n",
        "                        break\n",
        "                    if 'MLP' in dirs or 'Transformer' in dirs:\n",
        "                        extracted_path = root\n",
        "                        break\n",
        "\n",
        "                if extracted_path is None:\n",
        "                    extracted_path = extracted_base\n",
        "\n",
        "            print(f\"Copying from: {extracted_path}\")\n",
        "            shutil.copytree(extracted_path, 'checkpoints/error_recognition_best', dirs_exist_ok=True)\n",
        "\n",
        "            # Cleanup\n",
        "            shutil.rmtree('/tmp/checkpoints_extracted', ignore_errors=True)\n",
        "            os.remove('/tmp/checkpoints.zip')\n",
        "            print(\"✓ Checkpoints extracted\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Error extracting checkpoints: {e}\")\n",
        "    else:\n",
        "        # It's a directory\n",
        "        print(\"Detected directory, copying...\")\n",
        "        if os.path.basename(checkpoint_path) == 'error_recognition_best':\n",
        "            shutil.copytree(checkpoint_path, 'checkpoints/error_recognition_best', dirs_exist_ok=True)\n",
        "        else:\n",
        "            os.makedirs('checkpoints/error_recognition_best', exist_ok=True)\n",
        "            for item in os.listdir(checkpoint_path):\n",
        "                src = os.path.join(checkpoint_path, item)\n",
        "                dst = os.path.join('checkpoints/error_recognition_best', item)\n",
        "                if os.path.isdir(src):\n",
        "                    shutil.copytree(src, dst, dirs_exist_ok=True)\n",
        "                else:\n",
        "                    shutil.copy2(src, dst)\n",
        "        print(\"✓ Checkpoints copied\")\n",
        "else:\n",
        "    print(\"⚠ Checkpoints not found. This is optional - you can still train the RNN baseline.\")\n",
        "    print(\"   If you want to compare with existing baselines, download checkpoints from:\")\n",
        "    print(\"   https://utdallas.app.box.com/s/uz3s1alrzucz03sleify8kazhuc1ksl3\")\n",
        "\n",
        "# Verify checkpoints\n",
        "if os.path.exists('checkpoints/error_recognition_best'):\n",
        "    pt_files = []\n",
        "    for root, dirs, files in os.walk('checkpoints/error_recognition_best'):\n",
        "        pt_files.extend([os.path.join(root, f) for f in files if f.endswith('.pt')])\n",
        "    print(f\"\\n✓ Found {len(pt_files)} checkpoint files\")\n",
        "else:\n",
        "    print(\"\\n⚠ Checkpoints directory not found (this is optional)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.5. Configure Checkpoint Saving to Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CONFIGURE CHECKPOINT SAVING TO GOOGLE DRIVE\n",
        "# ============================================\n",
        "# Checkpoints will be saved to Google Drive during and after training\n",
        "# This ensures your trained models are preserved even if the Colab session ends\n",
        "\n",
        "GDRIVE_CHECKPOINT_PATH = \"/content/drive/MyDrive/AML_mistake_detection/error_recognition_best/RNN\"\n",
        "ENABLE_CHECKPOINT_SYNC = True  # Set to False to disable syncing to Google Drive\n",
        "\n",
        "print(\"Checkpoint Configuration:\")\n",
        "print(f\"  Enable Google Drive Sync: {ENABLE_CHECKPOINT_SYNC}\")\n",
        "print(f\"  Google Drive Path: {GDRIVE_CHECKPOINT_PATH}\")\n",
        "print(f\"  Local Checkpoint Path: checkpoints/error_recognition/RNN/\")\n",
        "print(\"\\nCheckpoints will be synced after training completes.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function to sync checkpoints to Google Drive\n",
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "def sync_checkpoints_to_gdrive(local_path, gdrive_path, backbone_name):\n",
        "    \"\"\"Sync checkpoints from local storage to Google Drive.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Starting Checkpoint Sync to Google Drive\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    if not ENABLE_CHECKPOINT_SYNC:\n",
        "        print(\"⚠ Checkpoint sync is DISABLED\")\n",
        "        print(\"  Set ENABLE_CHECKPOINT_SYNC = True to enable\")\n",
        "        return False\n",
        "    \n",
        "    # Check Google Drive is mounted\n",
        "    if not os.path.exists(\"/content/drive/MyDrive\"):\n",
        "        print(\"✗ ERROR: Google Drive not mounted!\")\n",
        "        print(\"  Please run the Google Drive mount cell first\")\n",
        "        return False\n",
        "    \n",
        "    # Check local path exists\n",
        "    if not os.path.exists(local_path):\n",
        "        print(f\"✗ ERROR: Local checkpoint path not found\")\n",
        "        print(f\"  Path: {local_path}\")\n",
        "        print(f\"  This usually means training failed or checkpoints weren't saved\")\n",
        "        return False\n",
        "    \n",
        "    # List files in local path for debugging\n",
        "    all_files = os.listdir(local_path)\n",
        "    checkpoint_files = [f for f in all_files if f.endswith('.pt')]\n",
        "    \n",
        "    print(f\"\\nLocal checkpoint directory: {local_path}\")\n",
        "    print(f\"  Total files: {len(all_files)}\")\n",
        "    print(f\"  Checkpoint files (.pt): {len(checkpoint_files)}\")\n",
        "    \n",
        "    if not checkpoint_files:\n",
        "        print(f\"✗ ERROR: No checkpoint files (.pt) found in {local_path}\")\n",
        "        print(f\"  Files present: {all_files[:5]}\")  # Show first 5 files\n",
        "        return False\n",
        "    \n",
        "    # Create Google Drive directory\n",
        "    backbone_gdrive_path = os.path.join(gdrive_path, backbone_name)\n",
        "    try:\n",
        "        os.makedirs(backbone_gdrive_path, exist_ok=True)\n",
        "        print(f\"\\nGoogle Drive path: {backbone_gdrive_path}\")\n",
        "        \n",
        "        # Verify directory was created\n",
        "        if not os.path.exists(backbone_gdrive_path):\n",
        "            print(f\"✗ ERROR: Failed to create Google Drive directory\")\n",
        "            return False\n",
        "        print(f\"  ✓ Directory verified\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ ERROR: Cannot create Google Drive directory\")\n",
        "        print(f\"  Error: {e}\")\n",
        "        return False\n",
        "    \n",
        "    # Copy each checkpoint file\n",
        "    print(f\"\\nSyncing {len(checkpoint_files)} file(s)...\")\n",
        "    synced_files = []\n",
        "    failed_files = []\n",
        "    \n",
        "    for ckpt_file in tqdm(checkpoint_files, desc=\"Syncing\"):\n",
        "        try:\n",
        "            src = os.path.join(local_path, ckpt_file)\n",
        "            dst = os.path.join(backbone_gdrive_path, ckpt_file)\n",
        "            \n",
        "            # Copy file\n",
        "            shutil.copy2(src, dst)\n",
        "            \n",
        "            # Verify file was copied\n",
        "            if os.path.exists(dst):\n",
        "                src_size = os.path.getsize(src)\n",
        "                dst_size = os.path.getsize(dst)\n",
        "                if src_size == dst_size:\n",
        "                    synced_files.append(ckpt_file)\n",
        "                else:\n",
        "                    print(f\"\\n⚠ Size mismatch for {ckpt_file}: {src_size} vs {dst_size}\")\n",
        "                    failed_files.append(ckpt_file)\n",
        "            else:\n",
        "                print(f\"\\n⚠ File not found after copy: {ckpt_file}\")\n",
        "                failed_files.append(ckpt_file)\n",
        "        except Exception as e:\n",
        "            print(f\"\\n✗ Error syncing {ckpt_file}: {e}\")\n",
        "            failed_files.append(ckpt_file)\n",
        "    \n",
        "    # Summary\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Sync Complete!\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"✓ Successfully synced: {len(synced_files)} file(s)\")\n",
        "    if failed_files:\n",
        "        print(f\"✗ Failed to sync: {len(failed_files)} file(s)\")\n",
        "        print(f\"  Failed files: {failed_files}\")\n",
        "    print(f\"\\nLocation: {backbone_gdrive_path}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    return len(synced_files) > 0\n",
        "\n",
        "print(\"✓ Checkpoint sync helper function loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.6. Verify Google Drive and Create Checkpoint Directories\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify Google Drive is mounted and create checkpoint directories\n",
        "import os\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Pre-Training Verification\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check if Google Drive is mounted\n",
        "gdrive_root = \"/content/drive/MyDrive\"\n",
        "if not os.path.exists(gdrive_root):\n",
        "    print(\"✗ ERROR: Google Drive is not mounted!\")\n",
        "    print(\"  Please run the 'Mount Google Drive' cell (Section 2) first.\")\n",
        "    raise RuntimeError(\"Google Drive not mounted\")\n",
        "else:\n",
        "    print(f\"✓ Google Drive is mounted at {gdrive_root}\")\n",
        "\n",
        "# Create checkpoint directories on Google Drive\n",
        "if ENABLE_CHECKPOINT_SYNC:\n",
        "    print(f\"\\n✓ Checkpoint sync is enabled\")\n",
        "    print(f\"Creating directories on Google Drive...\")\n",
        "    \n",
        "    for backbone in [\"omnivore\", \"slowfast\"]:\n",
        "        backbone_path = os.path.join(GDRIVE_CHECKPOINT_PATH, backbone)\n",
        "        os.makedirs(backbone_path, exist_ok=True)\n",
        "        \n",
        "        # Verify directory was created\n",
        "        if os.path.exists(backbone_path):\n",
        "            print(f\"  ✓ Created/verified: {backbone_path}\")\n",
        "        else:\n",
        "            print(f\"  ✗ Failed to create: {backbone_path}\")\n",
        "            raise RuntimeError(f\"Cannot create Google Drive directory: {backbone_path}\")\n",
        "    \n",
        "    # Test write permissions\n",
        "    test_file = os.path.join(GDRIVE_CHECKPOINT_PATH, \"test_write.tmp\")\n",
        "    try:\n",
        "        with open(test_file, 'w') as f:\n",
        "            f.write(\"test\")\n",
        "        os.remove(test_file)\n",
        "        print(f\"\\n✓ Write permissions verified for Google Drive path\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n✗ ERROR: Cannot write to Google Drive path!\")\n",
        "        print(f\"  Error: {e}\")\n",
        "        raise RuntimeError(f\"No write permissions to {GDRIVE_CHECKPOINT_PATH}\")\n",
        "else:\n",
        "    print(\"\\n⚠ Checkpoint sync is disabled\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✓ All pre-training checks passed!\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTKZ78NY__sp"
      },
      "source": [
        "## 3. Train RNN Baseline with Omnivore Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-oMOMxMBgr0",
        "outputId": "48c7360d-5be3-428f-cc8f-be2833edb05e"
      },
      "outputs": [],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zppt2TWr__sp",
        "outputId": "2f224de6-ae2c-4847-8d98-6e830f15b355"
      },
      "outputs": [],
      "source": [
        "# Train RNN baseline with Omnivore features\n",
        "# Default hyperparameters: hidden_size=256, num_layers=2, bidirectional=True, rnn_type=LSTM\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Ensure we're in the repository root directory\n",
        "repo_root = os.getcwd()\n",
        "if not os.path.exists(\"scripts/train_rnn_baseline.py\"):\n",
        "    print(f\"⚠ Error: scripts/train_rnn_baseline.py not found in {repo_root}\")\n",
        "    print(\"Please make sure you're in the repository root directory.\")\n",
        "else:\n",
        "    print(f\"Running from directory: {repo_root}\")\n",
        "\n",
        "    cmd = [\n",
        "        sys.executable, \"scripts/train_rnn_baseline.py\",\n",
        "        \"--variant\", \"RNN\",\n",
        "        \"--backbone\", \"omnivore\",\n",
        "        \"--split\", \"recordings\",\n",
        "        \"--batch_size\", \"4\",\n",
        "        \"--num_epochs\", \"20\",\n",
        "        \"--lr\", \"1e-3\",\n",
        "        \"--weight_decay\", \"1e-3\",\n",
        "        \"--rnn_hidden_size\", \"256\",\n",
        "        \"--rnn_num_layers\", \"2\",\n",
        "        \"--rnn_dropout\", \"0.2\",\n",
        "        \"--rnn_bidirectional\", \"True\",\n",
        "        \"--rnn_type\", \"LSTM\",\n",
        "        # \"--segment_features_directory\", \"data/\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\nRunning command:\")\n",
        "    print(\" \".join(cmd))\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "    # Set PYTHONPATH to include repo root for imports\n",
        "    env = os.environ.copy()\n",
        "    env['PYTHONPATH'] = repo_root + (os.pathsep + env.get('PYTHONPATH', ''))\n",
        "\n",
        "    # Run from the repository root directory and capture output\n",
        "    result = subprocess.run(\n",
        "        cmd,\n",
        "        cwd=repo_root,\n",
        "        env=env,\n",
        "        check=False,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True,\n",
        "        bufsize=1\n",
        "    )\n",
        "\n",
        "    # Print output in real-time (already captured, but print it)\n",
        "    if result.stdout:\n",
        "        print(result.stdout)\n",
        "\n",
        "    if result.returncode != 0:\n",
        "        print(f\"\\n⚠ Training failed with exit code {result.returncode}\")\n",
        "        if result.stderr:\n",
        "            print(\"Error output:\")\n",
        "            print(result.stderr)\n",
        "    else:\n",
        "        print(\"\\n✓ Training completed successfully!\")\n",
        "        \n",
        "        # Sync checkpoints to Google Drive\n",
        "        local_ckpt_path = \"checkpoints/error_recognition/RNN/omnivore\"\n",
        "        sync_checkpoints_to_gdrive(local_ckpt_path, GDRIVE_CHECKPOINT_PATH, \"omnivore\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f6e782a"
      },
      "source": [
        "### Patching `core/models/blocks.py` to fix `pack_padded_sequence` error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecea4870",
        "outputId": "cba491ba-66a0-46f1-ebda-3dc8e268f3fd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "file_path = \"core/models/blocks.py\"\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    original_line = \"x = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\"\n",
        "    replacement_line = \"x = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\"\n",
        "\n",
        "    if original_line in content:\n",
        "        new_content = content.replace(original_line, replacement_line)\n",
        "        with open(file_path, 'w') as f:\n",
        "            f.write(new_content)\n",
        "        print(f\"Patched {file_path}: 'lengths' argument for pack_padded_sequence moved to CPU.\")\n",
        "    else:\n",
        "        print(f\"Original line not found in {file_path}. Patching failed or already applied.\")\n",
        "else:\n",
        "    print(f\"Error: {file_path} not found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8icm8vE__sp"
      },
      "source": [
        "## 4. Train RNN Baseline with SlowFast Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A041dH-M__sp",
        "outputId": "3653d75b-7909-464f-c075-7d85cde5575a"
      },
      "outputs": [],
      "source": [
        "# Train RNN baseline with SlowFast features\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Ensure we're in the repository root directory\n",
        "repo_root = os.getcwd()\n",
        "if not os.path.exists(\"scripts/train_rnn_baseline.py\"):\n",
        "    print(f\"⚠ Error: scripts/train_rnn_baseline.py not found in {repo_root}\")\n",
        "    print(\"Please make sure you're in the repository root directory.\")\n",
        "else:\n",
        "    print(f\"Running from directory: {repo_root}\")\n",
        "\n",
        "    cmd = [\n",
        "        sys.executable, \"scripts/train_rnn_baseline.py\",\n",
        "        \"--variant\", \"RNN\",\n",
        "        \"--backbone\", \"slowfast\",\n",
        "        \"--split\", \"recordings\",\n",
        "        \"--batch_size\", \"4\",\n",
        "        \"--num_epochs\", \"20\",\n",
        "        \"--lr\", \"1e-3\",\n",
        "        \"--weight_decay\", \"1e-3\",\n",
        "        \"--rnn_hidden_size\", \"256\",\n",
        "        \"--rnn_num_layers\", \"2\",\n",
        "        \"--rnn_dropout\", \"0.2\",\n",
        "        \"--rnn_bidirectional\", \"True\",\n",
        "        \"--rnn_type\", \"LSTM\",\n",
        "        # \"--segment_features_directory\", \"data/\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\nRunning command:\")\n",
        "    print(\" \".join(cmd))\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "    # Set PYTHONPATH to include repo root for imports\n",
        "    env = os.environ.copy()\n",
        "    env['PYTHONPATH'] = repo_root + (os.pathsep + env.get('PYTHONPATH', ''))\n",
        "\n",
        "    # Run from the repository root directory and capture output\n",
        "    result = subprocess.run(\n",
        "        cmd,\n",
        "        cwd=repo_root,\n",
        "        env=env,\n",
        "        check=False,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True,\n",
        "        bufsize=1\n",
        "    )\n",
        "\n",
        "    # Print output\n",
        "    if result.stdout:\n",
        "        print(result.stdout)\n",
        "\n",
        "    if result.returncode != 0:\n",
        "        print(f\"\\n⚠ Training failed with exit code {result.returncode}\")\n",
        "        if result.stderr:\n",
        "            print(\"Error output:\")\n",
        "            print(result.stderr)\n",
        "    else:\n",
        "        print(\"\\n✓ Training completed successfully!\")\n",
        "        \n",
        "        # Sync checkpoints to Google Drive\n",
        "        local_ckpt_path = \"checkpoints/error_recognition/RNN/slowfast\"\n",
        "        sync_checkpoints_to_gdrive(local_ckpt_path, GDRIVE_CHECKPOINT_PATH, \"slowfast\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJDkEXNg__sq"
      },
      "source": [
        "## 5. Evaluate Trained Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZbzfA9f__sq",
        "outputId": "3a1d0f19-01d5-49ca-af69-6d56878e00c1"
      },
      "outputs": [],
      "source": [
        "# Evaluate both trained models (Omnivore and SlowFast)\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "def find_best_checkpoint(backbone):\n",
        "    \"\"\"Find the best checkpoint for a given backbone, checking both local and Google Drive.\"\"\"\n",
        "    # Check Google Drive first\n",
        "    gdrive_dir = os.path.join(GDRIVE_CHECKPOINT_PATH, backbone)\n",
        "    local_dir = f\"checkpoints/error_recognition/RNN/{backbone}\"\n",
        "    \n",
        "    # Try Google Drive first\n",
        "    checkpoint_dir = gdrive_dir if os.path.exists(gdrive_dir) else local_dir\n",
        "    \n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        print(f\"⚠ No checkpoint directory found for {backbone}\")\n",
        "        return None\n",
        "    \n",
        "    # Look for the best model\n",
        "    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.endswith('_best.pt')]\n",
        "    if checkpoint_files:\n",
        "        return os.path.join(checkpoint_dir, checkpoint_files[0])\n",
        "    \n",
        "    # Fallback to any .pt file\n",
        "    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pt')]\n",
        "    if checkpoint_files:\n",
        "        return os.path.join(checkpoint_dir, sorted(checkpoint_files)[-1])  # Get latest\n",
        "    \n",
        "    return None\n",
        "\n",
        "def evaluate_model(backbone, threshold=0.6):\n",
        "    \"\"\"Evaluate a trained RNN model.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Evaluating RNN + {backbone.upper()}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    repo_root = os.getcwd()\n",
        "    checkpoint_path = find_best_checkpoint(backbone)\n",
        "    \n",
        "    if checkpoint_path is None:\n",
        "        print(f\"⚠ No checkpoint found for {backbone}. Skipping evaluation.\")\n",
        "        return False\n",
        "    \n",
        "    print(f\"Using checkpoint: {checkpoint_path}\")\n",
        "    \n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        print(f\"⚠ Checkpoint file not found: {checkpoint_path}\")\n",
        "        return False\n",
        "    \n",
        "    cmd = [\n",
        "        sys.executable, \"-m\", \"core.evaluate\",\n",
        "        \"--variant\", \"RNN\",\n",
        "        \"--backbone\", backbone,\n",
        "        \"--split\", \"recordings\",\n",
        "        \"--ckpt\", checkpoint_path,\n",
        "        \"--threshold\", str(threshold)\n",
        "    ]\n",
        "    \n",
        "    print(\"\\nRunning command:\")\n",
        "    print(\" \".join(cmd))\n",
        "    print()\n",
        "    \n",
        "    # Set PYTHONPATH to include repo root for imports\n",
        "    env = os.environ.copy()\n",
        "    env['PYTHONPATH'] = repo_root + (os.pathsep + env.get('PYTHONPATH', ''))\n",
        "    \n",
        "    # Run evaluation\n",
        "    result = subprocess.run(cmd, cwd=repo_root, env=env, check=False)\n",
        "    \n",
        "    if result.returncode != 0:\n",
        "        print(f\"\\n⚠ Evaluation failed for {backbone} with exit code {result.returncode}\")\n",
        "        return False\n",
        "    else:\n",
        "        print(f\"\\n✓ Evaluation completed successfully for {backbone}!\")\n",
        "        return True\n",
        "\n",
        "# Evaluate both backbones\n",
        "print(\"Starting evaluation of trained RNN models...\")\n",
        "print(f\"Threshold: 0.6\")\n",
        "print()\n",
        "\n",
        "omnivore_success = evaluate_model(\"omnivore\", threshold=0.6)\n",
        "slowfast_success = evaluate_model(\"slowfast\", threshold=0.6)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Evaluation Summary\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"RNN + Omnivore: {'✓ Success' if omnivore_success else '✗ Failed'}\")\n",
        "print(f\"RNN + SlowFast: {'✓ Success' if slowfast_success else '✗ Failed'}\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "if omnivore_success or slowfast_success:\n",
        "    print(\"\\nResults saved to: results/error_recognition/combined_results/\")\n",
        "    print(\"Proceed to the next section for comparison with other baselines.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFMYih76__sq"
      },
      "source": [
        "## 6. Compare Results: RNN vs MLP vs Transformer\n",
        "\n",
        "This section compares the trained RNN baseline against existing MLP (V1) and Transformer (V2) baselines across all backbone combinations (Omnivore and SlowFast).\n",
        "\n",
        "**What this section does:**\n",
        "- Loads results from `results/error_recognition/combined_results/`\n",
        "- Displays comparison table with key metrics (Precision, Recall, F1, Accuracy, AUC)\n",
        "- Generates visualizations comparing all model combinations\n",
        "- Identifies the best performing model configuration\n",
        "\n",
        "**Note:** Ensure evaluation has been run for all models before running this comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_N6hxVUr__sq"
      },
      "outputs": [],
      "source": [
        "# Comprehensive results comparison: RNN vs MLP vs Transformer\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Configuration\n",
        "threshold = 0.6\n",
        "results_file = f\"results/error_recognition/combined_results/step_True_substep_True_threshold_{threshold}.csv\"\n",
        "\n",
        "print(f\"{'='*80}\")\n",
        "print(\"Comprehensive Model Comparison\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"Results file: {results_file}\")\n",
        "print()\n",
        "\n",
        "if not os.path.exists(results_file):\n",
        "    print(f\"⚠ Results file not found: {results_file}\")\n",
        "    print(\"Please run evaluation first.\")\n",
        "else:\n",
        "    # Load results\n",
        "    df = pd.read_csv(results_file)\n",
        "    \n",
        "    # Filter for recordings split\n",
        "    df_filtered = df[df['Split'] == 'recordings'].copy()\n",
        "    \n",
        "    if len(df_filtered) == 0:\n",
        "        print(\"⚠ No results found for 'recordings' split\")\n",
        "    else:\n",
        "        # Select relevant columns\n",
        "        columns_to_show = ['Variant', 'Backbone', 'Step Precision', 'Step Recall', \n",
        "                          'Step F1', 'Step Accuracy', 'Step AUC']\n",
        "        \n",
        "        # Check which columns exist\n",
        "        existing_columns = [col for col in columns_to_show if col in df_filtered.columns]\n",
        "        \n",
        "        print(\"=\"*80)\n",
        "        print(\"Model Performance Comparison (Recordings Split, Threshold=0.6)\")\n",
        "        print(\"=\"*80)\n",
        "        print()\n",
        "        \n",
        "        # Display full comparison table\n",
        "        comparison_df = df_filtered[existing_columns].copy()\n",
        "        \n",
        "        # Sort by F1 score (descending)\n",
        "        if 'Step F1' in comparison_df.columns:\n",
        "            comparison_df = comparison_df.sort_values('Step F1', ascending=False)\n",
        "        \n",
        "        # Format numeric columns\n",
        "        numeric_columns = comparison_df.select_dtypes(include=[np.number]).columns\n",
        "        for col in numeric_columns:\n",
        "            comparison_df[col] = comparison_df[col].round(4)\n",
        "        \n",
        "        print(comparison_df.to_string(index=False))\n",
        "        print()\n",
        "        \n",
        "        # Highlight best model\n",
        "        if 'Step F1' in comparison_df.columns:\n",
        "            best_idx = comparison_df['Step F1'].idxmax()\n",
        "            best_model = comparison_df.loc[best_idx]\n",
        "            print(\"=\"*80)\n",
        "            print(\"Best Model (by F1 Score):\")\n",
        "            print(\"=\"*80)\n",
        "            print(f\"Variant: {best_model['Variant']}\")\n",
        "            print(f\"Backbone: {best_model['Backbone']}\")\n",
        "            print(f\"F1 Score: {best_model['Step F1']:.4f}\")\n",
        "            print()\n",
        "        \n",
        "        # Create visualizations\n",
        "        print(\"=\"*80)\n",
        "        print(\"Generating Visualizations...\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        # Prepare data for plotting\n",
        "        variants = comparison_df['Variant'].tolist()\n",
        "        backbones = comparison_df['Backbone'].tolist()\n",
        "        labels = [f\"{v}+{b}\" for v, b in zip(variants, backbones)]\n",
        "        \n",
        "        metrics = ['Step Precision', 'Step Recall', 'Step F1', 'Step AUC']\n",
        "        metrics = [m for m in metrics if m in comparison_df.columns]\n",
        "        \n",
        "        if len(metrics) > 0:\n",
        "            # Create subplot for each metric\n",
        "            fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "            fig.suptitle('Model Comparison: RNN vs MLP vs Transformer', fontsize=16, fontweight='bold')\n",
        "            \n",
        "            for idx, metric in enumerate(metrics):\n",
        "                row = idx // 2\n",
        "                col = idx % 2\n",
        "                ax = axes[row, col]\n",
        "                \n",
        "                values = comparison_df[metric].tolist()\n",
        "                colors = ['#1f77b4' if 'RNN' in label else '#ff7f0e' if 'MLP' in label else '#2ca02c' \n",
        "                         for label in labels]\n",
        "                \n",
        "                bars = ax.bar(range(len(labels)), values, color=colors, alpha=0.8, edgecolor='black')\n",
        "                ax.set_xticks(range(len(labels)))\n",
        "                ax.set_xticklabels(labels, rotation=45, ha='right')\n",
        "                ax.set_ylabel(metric, fontsize=12)\n",
        "                ax.set_title(metric, fontsize=13, fontweight='bold')\n",
        "                ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "                ax.set_ylim([0, 1.0])\n",
        "                \n",
        "                # Add value labels on bars\n",
        "                for bar, val in zip(bars, values):\n",
        "                    height = bar.get_height()\n",
        "                    ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                           f'{val:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')\n",
        "            print(\"✓ Saved comparison plot to: model_comparison.png\")\n",
        "            plt.show()\n",
        "            \n",
        "            # Create grouped bar chart by backbone\n",
        "            fig, ax = plt.subplots(figsize=(14, 8))\n",
        "            \n",
        "            backbones_unique = sorted(comparison_df['Backbone'].unique())\n",
        "            variants_unique = sorted(comparison_df['Variant'].unique())\n",
        "            \n",
        "            x = np.arange(len(backbones_unique))\n",
        "            width = 0.25\n",
        "            \n",
        "            for i, variant in enumerate(variants_unique):\n",
        "                variant_data = []\n",
        "                for backbone in backbones_unique:\n",
        "                    matching = comparison_df[(comparison_df['Variant'] == variant) & \n",
        "                                            (comparison_df['Backbone'] == backbone)]\n",
        "                    if len(matching) > 0 and 'Step F1' in matching.columns:\n",
        "                        variant_data.append(matching['Step F1'].iloc[0])\n",
        "                    else:\n",
        "                        variant_data.append(0)\n",
        "                \n",
        "                offset = width * (i - len(variants_unique) / 2 + 0.5)\n",
        "                bars = ax.bar(x + offset, variant_data, width, label=variant, alpha=0.8, edgecolor='black')\n",
        "                \n",
        "                # Add value labels\n",
        "                for bar in bars:\n",
        "                    height = bar.get_height()\n",
        "                    if height > 0:\n",
        "                        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                               f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "            \n",
        "            ax.set_xlabel('Backbone', fontsize=13, fontweight='bold')\n",
        "            ax.set_ylabel('Step F1 Score', fontsize=13, fontweight='bold')\n",
        "            ax.set_title('F1 Score Comparison by Backbone and Variant', fontsize=14, fontweight='bold')\n",
        "            ax.set_xticks(x)\n",
        "            ax.set_xticklabels(backbones_unique)\n",
        "            ax.legend(title='Variant', fontsize=11)\n",
        "            ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "            ax.set_ylim([0, 1.0])\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.savefig('f1_comparison_by_backbone.png', dpi=150, bbox_inches='tight')\n",
        "            print(\"✓ Saved F1 comparison plot to: f1_comparison_by_backbone.png\")\n",
        "            plt.show()\n",
        "            \n",
        "            print()\n",
        "            print(\"=\"*80)\n",
        "            print(\"Analysis Complete!\")\n",
        "            print(\"=\"*80)\n",
        "        else:\n",
        "            print(\"⚠ No metrics found for visualization\")\n",
        "            \n",
        "print()\n",
        "print(\"Note: Make sure you've run training and evaluation for all models before comparison.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
