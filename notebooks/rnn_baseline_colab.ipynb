{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aexomir/AML_mistake_detection/blob/feat%2Frnn/notebooks/rnn_baseline_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTvg0mxC__sk"
      },
      "source": [
        "# RNN Baseline Training for CaptainCook4D SupervisedER\n",
        "\n",
        "Train RNN/LSTM baseline for mistake detection and compare against MLP and Transformer baselines.\n",
        "\n",
        "## Workflow:\n",
        "1. Setup repository and dependencies\n",
        "2. Load features and annotations from HuggingFace\n",
        "3. Train RNN models (Omnivore + SlowFast)\n",
        "4. Evaluate and compare results\n",
        "\n",
        "## Prerequisites:\n",
        "- Set environment variables: `WANDB_API_KEY`, `HF_TOKEN`, `HF_USERNAME`\n",
        "- HuggingFace datasets: features and annotations\n",
        "- Run all cells sequentially\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF5PTHDr__sl",
        "outputId": "d20554fc-5361-4541-c5d1-0d074ec2f4ee"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CONFIGURE YOUR REPOSITORY\n",
        "# ============================================\n",
        "# Option 1: Clone from GitHub (recommended)\n",
        "REPO_URL = \"https://github.com/aexomir/AML_mistake_detection.git\"\n",
        "REPO_BRANCH = \"feat/rnn\"  # Leave empty for default branch, or specify branch name\n",
        "\n",
        "# Option 2: Manual upload - set REPO_URL to empty string and upload files manually\n",
        "# REPO_URL = \"\"\n",
        "\n",
        "REPO_DIR = \"code\"\n",
        "\n",
        "print(f\"Repository URL: {REPO_URL if REPO_URL else 'Manual upload mode'}\")\n",
        "print(f\"Repository branch: {REPO_BRANCH if REPO_BRANCH else 'default'}\")\n",
        "print(f\"Repository directory: {REPO_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTGummcG__sm",
        "outputId": "a3c574a2-d6ee-4dda-e123-2a29507355f7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Remove existing directory if it exists\n",
        "if os.path.exists(REPO_DIR):\n",
        "    print(f\"Removing existing {REPO_DIR} directory...\")\n",
        "    shutil.rmtree(REPO_DIR)\n",
        "\n",
        "# Clone repository\n",
        "if REPO_URL:\n",
        "    print(f\"Cloning repository from {REPO_URL}...\")\n",
        "    clone_cmd = f\"git clone {REPO_URL} {REPO_DIR}\"\n",
        "    result = os.system(clone_cmd)\n",
        "\n",
        "    if result != 0:\n",
        "        print(f\"⚠ Clone failed. Please check the URL or upload files manually.\")\n",
        "        os.makedirs(REPO_DIR, exist_ok=True)\n",
        "    else:\n",
        "        print(\"✓ Repository cloned successfully\")\n",
        "\n",
        "        # Checkout specific branch if specified\n",
        "        if REPO_BRANCH:\n",
        "            print(f\"Checking out branch: {REPO_BRANCH}\")\n",
        "            os.chdir(REPO_DIR)\n",
        "            os.system(f\"git checkout {REPO_BRANCH}\")\n",
        "            os.chdir('..')\n",
        "            print(f\"✓ Switched to branch: {REPO_BRANCH}\")\n",
        "else:\n",
        "    print(\"Manual upload mode: Creating directory...\")\n",
        "    os.makedirs(REPO_DIR, exist_ok=True)\n",
        "\n",
        "# Change to repository directory\n",
        "if os.path.exists(REPO_DIR):\n",
        "    os.chdir(REPO_DIR)\n",
        "    print(f\"\\n✓ Changed to directory: {os.getcwd()}\")\n",
        "    print(f\"\\nRepository contents:\")\n",
        "    !ls -la\n",
        "else:\n",
        "    print(f\"✗ Error: {REPO_DIR} directory not found!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hXJexbo__sm",
        "outputId": "a2631425-4e09-4720-9a56-85e4695c93b6"
      },
      "outputs": [],
      "source": [
        "# Verify repository structure\n",
        "import os\n",
        "\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "print(f\"\\nChecking repository structure...\")\n",
        "\n",
        "required_items = [\n",
        "    'scripts/train_rnn_baseline.py',\n",
        "    'core/evaluate.py',\n",
        "    'dataloader',\n",
        "    'base.py',\n",
        "    'constants.py'\n",
        "]\n",
        "\n",
        "missing = []\n",
        "for item in required_items:\n",
        "    if os.path.exists(item):\n",
        "        print(f\"✓ Found: {item}\")\n",
        "    else:\n",
        "        print(f\"✗ Missing: {item}\")\n",
        "        missing.append(item)\n",
        "\n",
        "if missing:\n",
        "    print(f\"\\n⚠ Warning: Some required files/directories are missing!\")\n",
        "    print(f\"Please ensure all files are present before proceeding.\")\n",
        "else:\n",
        "    print(f\"\\n✓ Repository structure looks good!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HX_mVWn__sn",
        "outputId": "10902de3-fc33-4dc5-c502-d365a6bfbcde"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "# Colab comes with PyTorch pre-installed, so we'll work with that\n",
        "# Remove PyTorch version constraints to avoid conflicts\n",
        "if os.path.exists('requirements.txt'):\n",
        "    !sed -i '/^torch==/d' requirements.txt 2>/dev/null || true\n",
        "    !sed -i '/^torchvision==/d' requirements.txt 2>/dev/null || true\n",
        "\n",
        "# Install torcheval (required for evaluation metrics)\n",
        "!pip install -q torcheval\n",
        "\n",
        "# Install all remaining dependencies from requirements.txt\n",
        "if os.path.exists('requirements.txt'):\n",
        "    !pip install -q -r requirements.txt\n",
        "elif os.path.exists('requirements-cpu.txt'):\n",
        "    !pip install -q -r requirements-cpu.txt\n",
        "\n",
        "# Install additional dependencies for RNN baseline\n",
        "!pip install -q wandb loguru\n",
        "\n",
        "print(\"✓ All dependencies installed successfully\")\n",
        "\n",
        "# Verify PyTorch installation\n",
        "import torch\n",
        "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKPcoP7W__sn",
        "outputId": "a822637c-24e6-4df6-8b30-cd237c915346"
      },
      "outputs": [],
      "source": [
        "# HuggingFace Configuration\n",
        "import os\n",
        "\n",
        "HF_USERNAME = os.getenv('HF_USERNAME', 'your_username')\n",
        "HF_DATASET_REPO = f\"{HF_USERNAME}/captaincook4d-features\"\n",
        "HF_BASELINES_REPO = f\"{HF_USERNAME}/captaincook4d-baselines\"\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"HuggingFace Configuration\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Dataset Repository: {HF_DATASET_REPO}\")\n",
        "print(f\"Baselines Repository: {HF_BASELINES_REPO}\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX45xwkr__sn"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Cffl82S__sn"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4Va6eSf__sn",
        "outputId": "ca4ff0d2-648c-428a-cbab-5e591f6f84dd"
      },
      "outputs": [],
      "source": [
        "# Create data directory structure\n",
        "import os\n",
        "\n",
        "os.makedirs('data/video/omnivore', exist_ok=True)\n",
        "os.makedirs('data/video/slowfast', exist_ok=True)\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "os.makedirs('annotations/annotation_json', exist_ok=True)\n",
        "os.makedirs('annotations/data_splits', exist_ok=True)\n",
        "os.makedirs('er_annotations', exist_ok=True)\n",
        "\n",
        "print(\"✓ Directory structure created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBc3c5pX__so",
        "outputId": "15276510-4980-4660-defa-85be0a81f8b2"
      },
      "outputs": [],
      "source": [
        "# Load features from HuggingFace\n",
        "from huggingface_hub import hf_hub_download\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Loading Features from HuggingFace\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Download and extract omnivore features\n",
        "print(\"Downloading omnivore features...\")\n",
        "omnivore_zip = hf_hub_download(\n",
        "    repo_id=HF_DATASET_REPO,\n",
        "    filename=\"omnivore.zip\",\n",
        "    repo_type=\"dataset\",\n",
        "    token=HF_TOKEN\n",
        ")\n",
        "\n",
        "with zipfile.ZipFile(omnivore_zip, 'r') as z:\n",
        "    z.extractall('data/video/omnivore')\n",
        "\n",
        "omnivore_count = len([f for f in os.listdir('data/video/omnivore') if f.endswith('.npz')])\n",
        "print(f\"✓ Extracted {omnivore_count} omnivore features\")\n",
        "\n",
        "# Download and extract slowfast features\n",
        "print(\"Downloading slowfast features...\")\n",
        "slowfast_zip = hf_hub_download(\n",
        "    repo_id=HF_DATASET_REPO,\n",
        "    filename=\"slowfast.zip\",\n",
        "    repo_type=\"dataset\",\n",
        "    token=HF_TOKEN\n",
        ")\n",
        "\n",
        "with zipfile.ZipFile(slowfast_zip, 'r') as z:\n",
        "    z.extractall('data/video/slowfast')\n",
        "\n",
        "slowfast_count = len([f for f in os.listdir('data/video/slowfast') if f.endswith('.npz')])\n",
        "print(f\"✓ Extracted {slowfast_count} slowfast features\")\n",
        "\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDnhdR6d__so",
        "outputId": "4eb32606-5eae-4a9b-a267-0c428203780c"
      },
      "outputs": [],
      "source": [
        "# Load annotations (HuggingFace or Git repository)\n",
        "import os\n",
        "import zipfile\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Loading Annotations\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    print(\"Attempting to load from HuggingFace...\")\n",
        "    annotations_zip = hf_hub_download(\n",
        "        repo_id=HF_DATASET_REPO,\n",
        "        filename=\"annotations.zip\",\n",
        "        repo_type=\"dataset\",\n",
        "        token=HF_TOKEN\n",
        "    )\n",
        "    \n",
        "    with zipfile.ZipFile(annotations_zip, 'r') as z:\n",
        "        z.extractall('.')\n",
        "    \n",
        "    print(\"✓ Loaded annotations from HuggingFace\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: {e}\")\n",
        "    print(\"Using annotations from Git repository\")\n",
        "\n",
        "# Verify required annotation files\n",
        "required_files = [\n",
        "    'annotations/annotation_json/step_annotations.json',\n",
        "    'annotations/annotation_json/error_annotations.json',\n",
        "    'er_annotations/recordings_combined_splits.json'\n",
        "]\n",
        "\n",
        "missing = []\n",
        "for file in required_files:\n",
        "    if os.path.exists(file):\n",
        "        print(f\"✓ {file}\")\n",
        "    else:\n",
        "        print(f\"✗ Missing: {file}\")\n",
        "        missing.append(file)\n",
        "\n",
        "if missing:\n",
        "    print(f\"\\n⚠ Warning: {len(missing)} file(s) missing!\")\n",
        "else:\n",
        "    print(\"\\n✓ All annotation files present\")\n",
        "\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2IpUDQ3__so",
        "outputId": "ce63c428-e458-49ee-cf44-2123d8f9aff4"
      },
      "outputs": [],
      "source": [
        "## 2.5. Authentication & HuggingFace Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lQqJnhT__so",
        "outputId": "e852ad69-5ecb-4475-fcdd-ac2142026d89"
      },
      "outputs": [],
      "source": [
        "# Load checkpoints (optional - for comparison with existing baselines)\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "\n",
        "# Authentication Configuration\n",
        "import os\n",
        "\n",
        "WANDB_API_KEY = os.getenv('WANDB_API_KEY')\n",
        "HF_TOKEN = os.getenv('HF_TOKEN')\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Authentication\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if WANDB_API_KEY:\n",
        "    print(f\"✓ WANDB_API_KEY: {WANDB_API_KEY[:8]}...{WANDB_API_KEY[-4:]}\")\n",
        "else:\n",
        "    print(\"✗ WANDB_API_KEY not found\")\n",
        "\n",
        "if HF_TOKEN:\n",
        "    print(f\"✓ HF_TOKEN: {HF_TOKEN[:8]}...{HF_TOKEN[-4:]}\")\n",
        "else:\n",
        "    print(\"✗ HF_TOKEN not found\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "\n",
        "if not WANDB_API_KEY or not HF_TOKEN:\n",
        "    raise ValueError(\"Missing required environment variables\")\n",
        "\n",
        "# Auto-detect HF username\n",
        "from huggingface_hub import HfApi\n",
        "api = HfApi(token=HF_TOKEN)\n",
        "HF_USERNAME = api.whoami()['name']\n",
        "print(f\"✓ HuggingFace user: {HF_USERNAME}\")\n",
        "\n",
        "# Update repo configs\n",
        "HF_DATASET_REPO = f\"{HF_USERNAME}/captaincook4d-features\"\n",
        "HF_BASELINES_REPO = f\"{HF_USERNAME}/captaincook4d-baselines\"\n",
        "\n",
        "\n",
        "\n",
        "    if is_zip:\n",
        "        print(\"Detected zip file, extracting...\")\n",
        "        shutil.copy(checkpoint_path, '/tmp/checkpoints.zip')\n",
        "\n",
        "        try:\n",
        "            subprocess.run(['unzip', '-q', '/tmp/checkpoints.zip', '-d', '/tmp/checkpoints_extracted'], check=True)\n",
        "\n",
        "            # Find error_recognition_best directory\n",
        "            extracted_base = '/tmp/checkpoints_extracted'\n",
        "            extracted_path = None\n",
        "\n",
        "            # Check common locations\n",
        "            if os.path.exists(os.path.join(extracted_base, 'error_recognition_best')):\n",
        "                extracted_path = os.path.join(extracted_base, 'error_recognition_best')\n",
        "            elif os.path.exists(os.path.join(extracted_base, 'MLP')) or os.path.exists(os.path.join(extracted_base, 'Transformer')):\n",
        "                extracted_path = extracted_base\n",
        "            else:\n",
        "                # Search recursively\n",
        "                for root, dirs, files in os.walk(extracted_base):\n",
        "                    if 'error_recognition_best' in dirs:\n",
        "                        extracted_path = os.path.join(root, 'error_recognition_best')\n",
        "                        break\n",
        "                    if 'MLP' in dirs or 'Transformer' in dirs:\n",
        "                        extracted_path = root\n",
        "                        break\n",
        "\n",
        "                if extracted_path is None:\n",
        "                    extracted_path = extracted_base\n",
        "\n",
        "            print(f\"Copying from: {extracted_path}\")\n",
        "            shutil.copytree(extracted_path, 'checkpoints/error_recognition_best', dirs_exist_ok=True)\n",
        "\n",
        "            # Cleanup\n",
        "            shutil.rmtree('/tmp/checkpoints_extracted', ignore_errors=True)\n",
        "            os.remove('/tmp/checkpoints.zip')\n",
        "            print(\"✓ Checkpoints extracted\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Error extracting checkpoints: {e}\")\n",
        "    else:\n",
        "        # It's a directory\n",
        "        print(\"Detected directory, copying...\")\n",
        "        if os.path.basename(checkpoint_path) == 'error_recognition_best':\n",
        "            shutil.copytree(checkpoint_path, 'checkpoints/error_recognition_best', dirs_exist_ok=True)\n",
        "        else:\n",
        "            os.makedirs('checkpoints/error_recognition_best', exist_ok=True)\n",
        "            for item in os.listdir(checkpoint_path):\n",
        "                src = os.path.join(checkpoint_path, item)\n",
        "                dst = os.path.join('checkpoints/error_recognition_best', item)\n",
        "                if os.path.isdir(src):\n",
        "                    shutil.copytree(src, dst, dirs_exist_ok=True)\n",
        "                else:\n",
        "                    shutil.copy2(src, dst)\n",
        "        print(\"✓ Checkpoints copied\")\n",
        "else:\n",
        "    print(\"⚠ Checkpoints not found. This is optional - you can still train the RNN baseline.\")\n",
        "    print(\"   If you want to compare with existing baselines, download checkpoints from:\")\n",
        "    print(\"   https://utdallas.app.box.com/s/uz3s1alrzucz03sleify8kazhuc1ksl3\")\n",
        "\n",
        "# Verify checkpoints\n",
        "if os.path.exists('checkpoints/error_recognition_best'):\n",
        "    pt_files = []\n",
        "    for root, dirs, files in os.walk('checkpoints/error_recognition_best'):\n",
        "        pt_files.extend([os.path.join(root, f) for f in files if f.endswith('.pt')])\n",
        "    print(f\"\\n✓ Found {len(pt_files)} checkpoint files\")\n",
        "else:\n",
        "    print(\"\\n⚠ Checkpoints directory not found (this is optional)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.6. HuggingFace Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize WandB authentication\n",
        "import wandb\n",
        "\n",
        "wandb.login(key=WANDB_API_KEY)\n",
        "print(\"✓ WandB authenticated\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HuggingFace Helper Functions\n",
        "from huggingface_hub import upload_file, upload_folder, create_repo, hf_hub_download\n",
        "import os\n",
        "\n",
        "def get_hf_repo_name(wandb_run_name, backbone):\n",
        "    return f\"{HF_USERNAME}/rnn-{backbone}-{wandb_run_name}\"\n",
        "\n",
        "def upload_checkpoints_to_hf(local_path, wandb_run_name, backbone):\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Uploading to HuggingFace: {backbone}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    if not os.path.exists(local_path):\n",
        "        print(f\"✗ Checkpoint path not found: {local_path}\")\n",
        "        return False\n",
        "    \n",
        "    checkpoint_files = [f for f in os.listdir(local_path) if f.endswith('.pt')]\n",
        "    if not checkpoint_files:\n",
        "        print(f\"✗ No checkpoint files found\")\n",
        "        return False\n",
        "    \n",
        "    repo_id = get_hf_repo_name(wandb_run_name, backbone)\n",
        "    \n",
        "    try:\n",
        "        create_repo(repo_id=repo_id, private=True, exist_ok=True, token=HF_TOKEN)\n",
        "        upload_folder(folder_path=local_path, repo_id=repo_id, token=HF_TOKEN, allow_patterns=\"*.pt\")\n",
        "        print(f\"✓ Uploaded {len(checkpoint_files)} checkpoints to {repo_id}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Upload failed: {e}\")\n",
        "        return False\n",
        "\n",
        "def download_checkpoint_from_hf(repo_id, filename):\n",
        "    try:\n",
        "        return hf_hub_download(repo_id=repo_id, filename=filename, repo_type=\"model\", token=HF_TOKEN)\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Download failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def upload_results_to_hf(csv_path, wandb_run_name, backbone):\n",
        "    if not os.path.exists(csv_path):\n",
        "        return False\n",
        "    \n",
        "    repo_id = get_hf_repo_name(wandb_run_name, backbone)\n",
        "    try:\n",
        "        upload_file(path_or_fileobj=csv_path, path_in_repo=\"results.csv\", repo_id=repo_id, token=HF_TOKEN)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Results upload failed: {e}\")\n",
        "        return False\n",
        "\n",
        "def create_model_card(wandb_run_name, backbone, config, wandb_url):\n",
        "    repo_id = get_hf_repo_name(wandb_run_name, backbone)\n",
        "    card = f\"\"\"---\n",
        "tags:\n",
        "- video-understanding\n",
        "- error-recognition\n",
        "- rnn\n",
        "datasets:\n",
        "- {HF_DATASET_REPO}\n",
        "---\n",
        "\n",
        "# RNN Baseline: {backbone.upper()}\n",
        "\n",
        "## Model Details\n",
        "- Backbone: {backbone}\n",
        "- Architecture: {config['rnn_type']}\n",
        "- Hidden Size: {config['rnn_hidden_size']}\n",
        "- Layers: {config['rnn_num_layers']}\n",
        "\n",
        "## Training\n",
        "- WandB Run: {wandb_url}\n",
        "- Epochs: {config['num_epochs']}\n",
        "- Learning Rate: {config['learning_rate']}\n",
        "\n",
        "## Results\n",
        "See results.csv for detailed metrics.\n",
        "\"\"\"\n",
        "    \n",
        "    try:\n",
        "        upload_file(path_or_fileobj=card.encode(), path_in_repo=\"README.md\", repo_id=repo_id, token=HF_TOKEN)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Model card upload failed: {e}\")\n",
        "        return False\n",
        "\n",
        "print(\"✓ HuggingFace helper functions loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.6. Verify Google Drive and Create Checkpoint Directories\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify Google Drive is mounted and create checkpoint directories\n",
        "import os\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Pre-Training Verification\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check if Google Drive is mounted\n",
        "gdrive_root = \"/content/drive/MyDrive\"\n",
        "if not os.path.exists(gdrive_root):\n",
        "    print(\"✗ ERROR: Google Drive is not mounted!\")\n",
        "    print(\"  Please run the 'Mount Google Drive' cell (Section 2) first.\")\n",
        "    raise RuntimeError(\"Google Drive not mounted\")\n",
        "else:\n",
        "    print(f\"✓ Google Drive is mounted at {gdrive_root}\")\n",
        "\n",
        "# Create checkpoint directories on Google Drive\n",
        "if ENABLE_CHECKPOINT_SYNC:\n",
        "    print(f\"\\n✓ Checkpoint sync is enabled\")\n",
        "    print(f\"Creating directories on Google Drive...\")\n",
        "    \n",
        "    for backbone in [\"omnivore\", \"slowfast\"]:\n",
        "        backbone_path = os.path.join(GDRIVE_CHECKPOINT_PATH, backbone)\n",
        "        os.makedirs(backbone_path, exist_ok=True)\n",
        "        \n",
        "        # Verify directory was created\n",
        "        if os.path.exists(backbone_path):\n",
        "            print(f\"  ✓ Created/verified: {backbone_path}\")\n",
        "        else:\n",
        "            print(f\"  ✗ Failed to create: {backbone_path}\")\n",
        "            raise RuntimeError(f\"Cannot create Google Drive directory: {backbone_path}\")\n",
        "    \n",
        "    # Test write permissions\n",
        "    test_file = os.path.join(GDRIVE_CHECKPOINT_PATH, \"test_write.tmp\")\n",
        "    try:\n",
        "        with open(test_file, 'w') as f:\n",
        "            f.write(\"test\")\n",
        "        os.remove(test_file)\n",
        "        print(f\"\\n✓ Write permissions verified for Google Drive path\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n✗ ERROR: Cannot write to Google Drive path!\")\n",
        "        print(f\"  Error: {e}\")\n",
        "        raise RuntimeError(f\"No write permissions to {GDRIVE_CHECKPOINT_PATH}\")\n",
        "else:\n",
        "    print(\"\\n⚠ Checkpoint sync is disabled\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✓ All pre-training checks passed!\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTKZ78NY__sp"
      },
      "source": [
        "## 3. Train RNN Baseline with Omnivore Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-oMOMxMBgr0",
        "outputId": "48c7360d-5be3-428f-cc8f-be2833edb05e"
      },
      "outputs": [],
      "source": [
        "# Training Configuration\n",
        "TRAINING_CONFIG = {\n",
        "    \"batch_size\": 4,\n",
        "    \"num_epochs\": 20,\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"weight_decay\": 1e-3,\n",
        "    \"rnn_hidden_size\": 256,\n",
        "    \"rnn_num_layers\": 2,\n",
        "    \"rnn_dropout\": 0.2,\n",
        "    \"rnn_bidirectional\": True,\n",
        "    \"rnn_type\": \"LSTM\"\n",
        "}\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Training Configuration\")\n",
        "print(\"=\"*60)\n",
        "for key, value in TRAINING_CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zppt2TWr__sp",
        "outputId": "2f224de6-ae2c-4847-8d98-6e830f15b355"
      },
      "outputs": [],
      "source": [
        "# Train RNN with Omnivore features\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import wandb\n",
        "\n",
        "BACKBONE = \"omnivore\"\n",
        "\n",
        "repo_root = os.getcwd()\n",
        "if not os.path.exists(\"scripts/train_rnn_baseline.py\"):\n",
        "    print(f\"✗ Training script not found\")\n",
        "else:\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Training RNN + {BACKBONE.upper()}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    cmd = [\n",
        "        sys.executable, \"scripts/train_rnn_baseline.py\",\n",
        "        \"--variant\", \"RNN\",\n",
        "        \"--backbone\", BACKBONE,\n",
        "        \"--split\", \"recordings\",\n",
        "        \"--batch_size\", str(TRAINING_CONFIG[\"batch_size\"]),\n",
        "        \"--num_epochs\", str(TRAINING_CONFIG[\"num_epochs\"]),\n",
        "        \"--lr\", str(TRAINING_CONFIG[\"learning_rate\"]),\n",
        "        \"--weight_decay\", str(TRAINING_CONFIG[\"weight_decay\"]),\n",
        "        \"--rnn_hidden_size\", str(TRAINING_CONFIG[\"rnn_hidden_size\"]),\n",
        "        \"--rnn_num_layers\", str(TRAINING_CONFIG[\"rnn_num_layers\"]),\n",
        "        \"--rnn_dropout\", str(TRAINING_CONFIG[\"rnn_dropout\"]),\n",
        "        \"--rnn_bidirectional\", str(TRAINING_CONFIG[\"rnn_bidirectional\"]),\n",
        "        \"--rnn_type\", TRAINING_CONFIG[\"rnn_type\"],\n",
        "    ]\n",
        "\n",
        "    env = os.environ.copy()\n",
        "    env['PYTHONPATH'] = repo_root + (os.pathsep + env.get('PYTHONPATH', ''))\n",
        "    env['WANDB_API_KEY'] = WANDB_API_KEY\n",
        "\n",
        "    result = subprocess.run(cmd, cwd=repo_root, env=env, check=False, \n",
        "                          stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "\n",
        "    if result.stdout:\n",
        "        print(result.stdout)\n",
        "\n",
        "    if result.returncode != 0:\n",
        "        print(f\"\\n✗ Training failed\")\n",
        "    else:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"Training Complete\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        local_ckpt_path = f\"checkpoints/error_recognition/RNN/{BACKBONE}\"\n",
        "        \n",
        "        if wandb.run:\n",
        "            wandb_run_name = wandb.run.name\n",
        "            upload_checkpoints_to_hf(local_ckpt_path, wandb_run_name, BACKBONE)\n",
        "            create_model_card(wandb_run_name, BACKBONE, TRAINING_CONFIG, wandb.run.url)\n",
        "        else:\n",
        "            print(\"⚠ No active wandb run\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f6e782a"
      },
      "source": [
        "## 3.5. Apply Compatibility Patch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecea4870",
        "outputId": "cba491ba-66a0-46f1-ebda-3dc8e268f3fd"
      },
      "outputs": [],
      "source": [
        "# Apply compatibility patch for pack_padded_sequence\n",
        "import os\n",
        "\n",
        "file_path = \"core/models/blocks.py\"\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    original = \"x = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\"\n",
        "    patched = \"x = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\"\n",
        "\n",
        "    if original in content:\n",
        "        content = content.replace(original, patched)\n",
        "        with open(file_path, 'w') as f:\n",
        "            f.write(content)\n",
        "        print(f\"✓ Patched {file_path}\")\n",
        "    else:\n",
        "        print(\"✓ Patch already applied\")\n",
        "else:\n",
        "    print(f\"✗ {file_path} not found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8icm8vE__sp"
      },
      "source": [
        "## 4. Train RNN Baseline with SlowFast Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A041dH-M__sp",
        "outputId": "3653d75b-7909-464f-c075-7d85cde5575a"
      },
      "outputs": [],
      "source": [
        "# Train RNN with SlowFast features\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import wandb\n",
        "\n",
        "BACKBONE = \"slowfast\"\n",
        "\n",
        "repo_root = os.getcwd()\n",
        "if not os.path.exists(\"scripts/train_rnn_baseline.py\"):\n",
        "    print(f\"✗ Training script not found\")\n",
        "else:\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Training RNN + {BACKBONE.upper()}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    cmd = [\n",
        "        sys.executable, \"scripts/train_rnn_baseline.py\",\n",
        "        \"--variant\", \"RNN\",\n",
        "        \"--backbone\", BACKBONE,\n",
        "        \"--split\", \"recordings\",\n",
        "        \"--batch_size\", str(TRAINING_CONFIG[\"batch_size\"]),\n",
        "        \"--num_epochs\", str(TRAINING_CONFIG[\"num_epochs\"]),\n",
        "        \"--lr\", str(TRAINING_CONFIG[\"learning_rate\"]),\n",
        "        \"--weight_decay\", str(TRAINING_CONFIG[\"weight_decay\"]),\n",
        "        \"--rnn_hidden_size\", str(TRAINING_CONFIG[\"rnn_hidden_size\"]),\n",
        "        \"--rnn_num_layers\", str(TRAINING_CONFIG[\"rnn_num_layers\"]),\n",
        "        \"--rnn_dropout\", str(TRAINING_CONFIG[\"rnn_dropout\"]),\n",
        "        \"--rnn_bidirectional\", str(TRAINING_CONFIG[\"rnn_bidirectional\"]),\n",
        "        \"--rnn_type\", TRAINING_CONFIG[\"rnn_type\"],\n",
        "    ]\n",
        "\n",
        "    env = os.environ.copy()\n",
        "    env['PYTHONPATH'] = repo_root + (os.pathsep + env.get('PYTHONPATH', ''))\n",
        "    env['WANDB_API_KEY'] = WANDB_API_KEY\n",
        "\n",
        "    result = subprocess.run(cmd, cwd=repo_root, env=env, check=False, \n",
        "                          stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "\n",
        "    if result.stdout:\n",
        "        print(result.stdout)\n",
        "\n",
        "    if result.returncode != 0:\n",
        "        print(f\"\\n✗ Training failed\")\n",
        "    else:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"Training Complete\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        local_ckpt_path = f\"checkpoints/error_recognition/RNN/{BACKBONE}\"\n",
        "        \n",
        "        if wandb.run:\n",
        "            wandb_run_name = wandb.run.name\n",
        "            upload_checkpoints_to_hf(local_ckpt_path, wandb_run_name, BACKBONE)\n",
        "            create_model_card(wandb_run_name, BACKBONE, TRAINING_CONFIG, wandb.run.url)\n",
        "        else:\n",
        "            print(\"⚠ No active wandb run\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJDkEXNg__sq"
      },
      "source": [
        "## 5. Evaluate Trained Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZbzfA9f__sq",
        "outputId": "3a1d0f19-01d5-49ca-af69-6d56878e00c1"
      },
      "outputs": [],
      "source": [
        "# Evaluate trained RNN models\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "EVAL_THRESHOLD = 0.6\n",
        "\n",
        "def find_best_checkpoint(backbone):\n",
        "    local_dir = f\"checkpoints/error_recognition/RNN/{backbone}\"\n",
        "    \n",
        "    if not os.path.exists(local_dir):\n",
        "        return None\n",
        "    \n",
        "    checkpoint_files = [f for f in os.listdir(local_dir) if f.endswith('_best.pt')]\n",
        "    if checkpoint_files:\n",
        "        return os.path.join(local_dir, checkpoint_files[0])\n",
        "    \n",
        "    checkpoint_files = sorted([f for f in os.listdir(local_dir) if f.endswith('.pt') and 'epoch' in f])\n",
        "    if checkpoint_files:\n",
        "        return os.path.join(local_dir, checkpoint_files[-1])\n",
        "    \n",
        "    return None\n",
        "\n",
        "def evaluate_model(backbone):\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Evaluating RNN + {backbone.upper()}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    checkpoint_path = find_best_checkpoint(backbone)\n",
        "    \n",
        "    if not checkpoint_path or not os.path.exists(checkpoint_path):\n",
        "        print(f\"✗ No checkpoint found\")\n",
        "        return False\n",
        "    \n",
        "    print(f\"Checkpoint: {checkpoint_path}\")\n",
        "    \n",
        "    cmd = [\n",
        "        sys.executable, \"-m\", \"core.evaluate\",\n",
        "        \"--variant\", \"RNN\",\n",
        "        \"--backbone\", backbone,\n",
        "        \"--split\", \"recordings\",\n",
        "        \"--ckpt\", checkpoint_path,\n",
        "        \"--threshold\", str(EVAL_THRESHOLD)\n",
        "    ]\n",
        "    \n",
        "    env = os.environ.copy()\n",
        "    env['PYTHONPATH'] = os.getcwd() + (os.pathsep + env.get('PYTHONPATH', ''))\n",
        "    \n",
        "    result = subprocess.run(cmd, cwd=os.getcwd(), env=env, check=False)\n",
        "    \n",
        "    if result.returncode != 0:\n",
        "        print(f\"✗ Evaluation failed\")\n",
        "        return False\n",
        "    else:\n",
        "        print(f\"✓ Evaluation complete\")\n",
        "        return True\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"RNN Model Evaluation\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "omnivore_success = evaluate_model(\"omnivore\")\n",
        "slowfast_success = evaluate_model(\"slowfast\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Evaluation Summary\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Omnivore: {'✓' if omnivore_success else '✗'}\")\n",
        "print(f\"SlowFast: {'✓' if slowfast_success else '✗'}\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFMYih76__sq"
      },
      "source": [
        "## 6. Compare Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_N6hxVUr__sq"
      },
      "outputs": [],
      "source": [
        "# Comprehensive results comparison: RNN vs MLP vs Transformer\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Configuration\n",
        "threshold = 0.6\n",
        "results_file = f\"results/error_recognition/combined_results/step_True_substep_True_threshold_{threshold}.csv\"\n",
        "\n",
        "print(f\"{'='*80}\")\n",
        "print(\"Comprehensive Model Comparison\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"Results file: {results_file}\")\n",
        "print()\n",
        "\n",
        "if not os.path.exists(results_file):\n",
        "    print(f\"⚠ Results file not found: {results_file}\")\n",
        "    print(\"Please run evaluation first.\")\n",
        "else:\n",
        "    # Load results\n",
        "    df = pd.read_csv(results_file)\n",
        "    \n",
        "    # Filter for recordings split\n",
        "    df_filtered = df[df['Split'] == 'recordings'].copy()\n",
        "    \n",
        "    if len(df_filtered) == 0:\n",
        "        print(\"⚠ No results found for 'recordings' split\")\n",
        "    else:\n",
        "        # Select relevant columns\n",
        "        columns_to_show = ['Variant', 'Backbone', 'Step Precision', 'Step Recall', \n",
        "                          'Step F1', 'Step Accuracy', 'Step AUC']\n",
        "        \n",
        "        # Check which columns exist\n",
        "        existing_columns = [col for col in columns_to_show if col in df_filtered.columns]\n",
        "        \n",
        "        print(\"=\"*80)\n",
        "        print(\"Model Performance Comparison (Recordings Split, Threshold=0.6)\")\n",
        "        print(\"=\"*80)\n",
        "        print()\n",
        "        \n",
        "        # Display full comparison table\n",
        "        comparison_df = df_filtered[existing_columns].copy()\n",
        "        \n",
        "        # Sort by F1 score (descending)\n",
        "        if 'Step F1' in comparison_df.columns:\n",
        "            comparison_df = comparison_df.sort_values('Step F1', ascending=False)\n",
        "        \n",
        "        # Format numeric columns\n",
        "        numeric_columns = comparison_df.select_dtypes(include=[np.number]).columns\n",
        "        for col in numeric_columns:\n",
        "            comparison_df[col] = comparison_df[col].round(4)\n",
        "        \n",
        "        print(comparison_df.to_string(index=False))\n",
        "        print()\n",
        "        \n",
        "        # Highlight best model\n",
        "        if 'Step F1' in comparison_df.columns:\n",
        "            best_idx = comparison_df['Step F1'].idxmax()\n",
        "            best_model = comparison_df.loc[best_idx]\n",
        "            print(\"=\"*80)\n",
        "            print(\"Best Model (by F1 Score):\")\n",
        "            print(\"=\"*80)\n",
        "            print(f\"Variant: {best_model['Variant']}\")\n",
        "            print(f\"Backbone: {best_model['Backbone']}\")\n",
        "            print(f\"F1 Score: {best_model['Step F1']:.4f}\")\n",
        "            print()\n",
        "        \n",
        "        # Create visualizations\n",
        "        print(\"=\"*80)\n",
        "        print(\"Generating Visualizations...\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        # Prepare data for plotting\n",
        "        variants = comparison_df['Variant'].tolist()\n",
        "        backbones = comparison_df['Backbone'].tolist()\n",
        "        labels = [f\"{v}+{b}\" for v, b in zip(variants, backbones)]\n",
        "        \n",
        "        metrics = ['Step Precision', 'Step Recall', 'Step F1', 'Step AUC']\n",
        "        metrics = [m for m in metrics if m in comparison_df.columns]\n",
        "        \n",
        "        if len(metrics) > 0:\n",
        "            # Create subplot for each metric\n",
        "            fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "            fig.suptitle('Model Comparison: RNN vs MLP vs Transformer', fontsize=16, fontweight='bold')\n",
        "            \n",
        "            for idx, metric in enumerate(metrics):\n",
        "                row = idx // 2\n",
        "                col = idx % 2\n",
        "                ax = axes[row, col]\n",
        "                \n",
        "                values = comparison_df[metric].tolist()\n",
        "                colors = ['#1f77b4' if 'RNN' in label else '#ff7f0e' if 'MLP' in label else '#2ca02c' \n",
        "                         for label in labels]\n",
        "                \n",
        "                bars = ax.bar(range(len(labels)), values, color=colors, alpha=0.8, edgecolor='black')\n",
        "                ax.set_xticks(range(len(labels)))\n",
        "                ax.set_xticklabels(labels, rotation=45, ha='right')\n",
        "                ax.set_ylabel(metric, fontsize=12)\n",
        "                ax.set_title(metric, fontsize=13, fontweight='bold')\n",
        "                ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "                ax.set_ylim([0, 1.0])\n",
        "                \n",
        "                # Add value labels on bars\n",
        "                for bar, val in zip(bars, values):\n",
        "                    height = bar.get_height()\n",
        "                    ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                           f'{val:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')\n",
        "            print(\"✓ Saved comparison plot to: model_comparison.png\")\n",
        "            plt.show()\n",
        "            \n",
        "            # Create grouped bar chart by backbone\n",
        "            fig, ax = plt.subplots(figsize=(14, 8))\n",
        "            \n",
        "            backbones_unique = sorted(comparison_df['Backbone'].unique())\n",
        "            variants_unique = sorted(comparison_df['Variant'].unique())\n",
        "            \n",
        "            x = np.arange(len(backbones_unique))\n",
        "            width = 0.25\n",
        "            \n",
        "            for i, variant in enumerate(variants_unique):\n",
        "                variant_data = []\n",
        "                for backbone in backbones_unique:\n",
        "                    matching = comparison_df[(comparison_df['Variant'] == variant) & \n",
        "                                            (comparison_df['Backbone'] == backbone)]\n",
        "                    if len(matching) > 0 and 'Step F1' in matching.columns:\n",
        "                        variant_data.append(matching['Step F1'].iloc[0])\n",
        "                    else:\n",
        "                        variant_data.append(0)\n",
        "                \n",
        "                offset = width * (i - len(variants_unique) / 2 + 0.5)\n",
        "                bars = ax.bar(x + offset, variant_data, width, label=variant, alpha=0.8, edgecolor='black')\n",
        "                \n",
        "                # Add value labels\n",
        "                for bar in bars:\n",
        "                    height = bar.get_height()\n",
        "                    if height > 0:\n",
        "                        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                               f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "            \n",
        "            ax.set_xlabel('Backbone', fontsize=13, fontweight='bold')\n",
        "            ax.set_ylabel('Step F1 Score', fontsize=13, fontweight='bold')\n",
        "            ax.set_title('F1 Score Comparison by Backbone and Variant', fontsize=14, fontweight='bold')\n",
        "            ax.set_xticks(x)\n",
        "            ax.set_xticklabels(backbones_unique)\n",
        "            ax.legend(title='Variant', fontsize=11)\n",
        "            ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "            ax.set_ylim([0, 1.0])\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.savefig('f1_comparison_by_backbone.png', dpi=150, bbox_inches='tight')\n",
        "            print(\"✓ Saved F1 comparison plot to: f1_comparison_by_backbone.png\")\n",
        "            plt.show()\n",
        "            \n",
        "            print()\n",
        "            print(\"=\"*80)\n",
        "            print(\"Analysis Complete!\")\n",
        "            print(\"=\"*80)\n",
        "        else:\n",
        "            print(\"⚠ No metrics found for visualization\")\n",
        "            \n",
        "print()\n",
        "print(\"Note: Make sure you've run training and evaluation for all models before comparison.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
