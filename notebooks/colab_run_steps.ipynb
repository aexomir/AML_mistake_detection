{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# AML - Step 2 & Step 3 Runner (Colab)\n",
        "\n",
        "This notebook runs Step 2 (feature sanity check) and Step 3 (evaluation) using `scripts/run.py`.\n",
        "\n",
        "## Quick Start:\n",
        "1. **Configure your repo** in Section 1\n",
        "2. **Mount Google Drive** (Section 2) if your data is there\n",
        "3. **Set data paths** (Section 3) and load features/checkpoints\n",
        "4. **Run Step 2** to verify features\n",
        "5. **Run Step 3** to evaluate models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 1. Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "configure_repo"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CONFIGURE YOUR REPOSITORY\n",
        "# ============================================\n",
        "# Repository URL (use .git format, not tree/branch URLs)\n",
        "REPO_URL = \"https://github.com/aexomir/AML_mistake_detection.git\"\n",
        "\n",
        "# Branch to checkout (optional, leave empty for default branch)\n",
        "REPO_BRANCH = \"feat/step02\"\n",
        "\n",
        "# Repository directory name (will be cloned into this folder)\n",
        "REPO_DIR = \"aml_repo\"\n",
        "\n",
        "print(f\"Repository URL: {REPO_URL}\")\n",
        "print(f\"Repository branch: {REPO_BRANCH if REPO_BRANCH else 'default'}\")\n",
        "print(f\"Repository directory: {REPO_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_repo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Remove existing directory if it exists\n",
        "if os.path.exists(REPO_DIR):\n",
        "    print(f\"Removing existing {REPO_DIR} directory...\")\n",
        "    shutil.rmtree(REPO_DIR)\n",
        "\n",
        "# Clone repository\n",
        "if REPO_URL:\n",
        "    print(f\"Cloning repository from {REPO_URL}...\")\n",
        "    clone_cmd = f\"git clone {REPO_URL} {REPO_DIR}\"\n",
        "    result = os.system(clone_cmd)\n",
        "    \n",
        "    if result != 0:\n",
        "        print(f\"⚠ Clone failed. Creating {REPO_DIR} directory for manual upload...\")\n",
        "        os.makedirs(REPO_DIR, exist_ok=True)\n",
        "    else:\n",
        "        print(\"✓ Repository cloned successfully\")\n",
        "        \n",
        "        # Checkout specific branch if specified\n",
        "        if REPO_BRANCH:\n",
        "            print(f\"Checking out branch: {REPO_BRANCH}\")\n",
        "            os.chdir(REPO_DIR)\n",
        "            os.system(f\"git checkout {REPO_BRANCH}\")\n",
        "            os.chdir('..')\n",
        "            print(f\"✓ Switched to branch: {REPO_BRANCH}\")\n",
        "else:\n",
        "    print(\"No repo URL configured. Creating directory for manual upload...\")\n",
        "    os.makedirs(REPO_DIR, exist_ok=True)\n",
        "\n",
        "# Change to repository directory\n",
        "if os.path.exists(REPO_DIR):\n",
        "    os.chdir(REPO_DIR)\n",
        "    print(f\"\\n✓ Changed to directory: {os.getcwd()}\")\n",
        "    print(f\"\\nRepository contents:\")\n",
        "    !ls -la\n",
        "else:\n",
        "    print(f\"✗ Error: {REPO_DIR} directory not found!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "verify_repo"
      },
      "outputs": [],
      "source": [
        "# Verify we're in the correct directory and repo structure\n",
        "import os\n",
        "\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "print(f\"\\nChecking repository structure...\")\n",
        "\n",
        "# Check for key files/directories\n",
        "required_items = [\n",
        "    'scripts/run.py',\n",
        "    'core/evaluate.py',\n",
        "    'dataloader',\n",
        "    'base.py',\n",
        "    'constants.py'\n",
        "]\n",
        "\n",
        "missing = []\n",
        "for item in required_items:\n",
        "    if os.path.exists(item):\n",
        "        print(f\"✓ Found: {item}\")\n",
        "    else:\n",
        "        print(f\"✗ Missing: {item}\")\n",
        "        missing.append(item)\n",
        "\n",
        "if missing:\n",
        "    print(f\"\\n⚠ Warning: Some required files/directories are missing!\")\n",
        "    print(f\"Make sure you're in the correct repository directory.\")\n",
        "else:\n",
        "    print(f\"\\n✓ Repository structure looks good!\")\n",
        "    print(f\"\\nYou can now proceed to load data and run steps.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "# Remove PyTorch version constraints to avoid conflicts with Colab's pre-installed version\n",
        "if os.path.exists('requirements.txt'):\n",
        "    !sed -i '/^torch==/d' requirements.txt 2>/dev/null || true\n",
        "    !sed -i '/^torchvision==/d' requirements.txt 2>/dev/null || true\n",
        "\n",
        "# Install PyTorch ecosystem with compatible versions\n",
        "# Use Colab-compatible PyTorch versions\n",
        "!pip install -q torch==2.2.0+cu118 torchvision==0.17.0+cu118 torchaudio==2.2.0+cu118 --index-url https://download.pytorch.org/whl/cu118 2>/dev/null || \\\n",
        "  pip install -q torch torchvision torchaudio\n",
        "\n",
        "# Install torcheval for evaluation metrics (requires torchaudio)\n",
        "!pip install -q torcheval\n",
        "\n",
        "# Install all remaining dependencies from requirements.txt\n",
        "if os.path.exists('requirements.txt'):\n",
        "    !pip install -q -r requirements.txt\n",
        "elif os.path.exists('requirements-cpu.txt'):\n",
        "    !pip install -q -r requirements-cpu.txt\n",
        "\n",
        "print(\"✓ All dependencies installed successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mount_drive"
      },
      "source": [
        "## 2. Mount Google Drive (Optional)\n",
        "\n",
        "If your features and checkpoints are in Google Drive, mount it here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_data"
      },
      "source": [
        "## 3. Load Data & Checkpoints\n",
        "\n",
        "Specify paths to your unzipped directories on Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_paths"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# UPDATE THESE PATHS TO YOUR DRIVE LOCATIONS\n",
        "# ============================================\n",
        "\n",
        "# Path to unzipped omnivore features directory on Drive\n",
        "# Should contain .npz files directly or in a subdirectory\n",
        "OMNIVORE_DRIVE_PATH = \"/content/drive/MyDrive/AML/AML_MistakeDetection/features/omnivore\"\n",
        "\n",
        "# Path to unzipped slowfast features directory on Drive\n",
        "# Should contain .npz files directly or in a subdirectory\n",
        "SLOWFAST_DRIVE_PATH = \"/content/drive/MyDrive/AML/AML_MistakeDetection/features/slowfast\"\n",
        "\n",
        "# Path to error_recognition_best checkpoints on Drive\n",
        "# Can be either:\n",
        "#   - A zip file (e.g., \"/content/drive/MyDrive/path/to/error_recognition_best.zip\")\n",
        "#   - An unzipped directory (e.g., \"/content/drive/MyDrive/path/to/error_recognition_best\")\n",
        "# Should contain MLP/ and Transformer/ subdirectories\n",
        "CHECKPOINTS_DRIVE_PATH = \"/content/drive/MyDrive/AML/AML_MistakeDetection/error_recognition_best.zip\"\n",
        "\n",
        "# Path to annotations directory on Drive (if annotations are stored separately)\n",
        "# Should contain annotation_json/ and optionally data_splits/ and er_annotations/ subdirectories\n",
        "ANNOTATIONS_DRIVE_PATH = \"/content/drive/MyDrive/AML/AML_MistakeDetection/annotations\"\n",
        "\n",
        "print(\"Paths configured:\")\n",
        "print(f\"  Omnivore: {OMNIVORE_DRIVE_PATH}\")\n",
        "print(f\"  SlowFast: {SLOWFAST_DRIVE_PATH}\")\n",
        "print(f\"  Checkpoints: {CHECKPOINTS_DRIVE_PATH}\")\n",
        "print(f\"  Annotations: {ANNOTATIONS_DRIVE_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# LOAD ANNOTATIONS\n",
        "# ============================================\n",
        "# The evaluation script requires annotation files. You need to either:\n",
        "# 1. Have them in your repository (they may be gitignored)\n",
        "# 2. Upload them to Google Drive and copy them here\n",
        "# 3. Download them from a shared location\n",
        "#\n",
        "# Required files:\n",
        "# - annotations/annotation_json/step_annotations.json\n",
        "# - annotations/annotation_json/error_annotations.json\n",
        "# - er_annotations/recordings_combined_splits.json (for step split - should be in repo)\n",
        "# - annotations/data_splits/{split}_data_split_combined.json (for recordings split)\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Create annotations directory structure\n",
        "# Note: ANNOTATIONS_DRIVE_PATH is defined in the previous cell (set_paths)\n",
        "os.makedirs('annotations/annotation_json', exist_ok=True)\n",
        "os.makedirs('annotations/data_splits', exist_ok=True)\n",
        "os.makedirs('er_annotations', exist_ok=True)\n",
        "\n",
        "# Check if annotations exist in Drive and copy them\n",
        "if os.path.exists(ANNOTATIONS_DRIVE_PATH):\n",
        "    print(f\"Found annotations in Drive: {ANNOTATIONS_DRIVE_PATH}\")\n",
        "    \n",
        "    # Copy annotation_json directory\n",
        "    annotation_json_src = os.path.join(ANNOTATIONS_DRIVE_PATH, 'annotation_json')\n",
        "    if os.path.exists(annotation_json_src):\n",
        "        print(\"Copying annotation_json files...\")\n",
        "        for file in os.listdir(annotation_json_src):\n",
        "            src = os.path.join(annotation_json_src, file)\n",
        "            dst = os.path.join('annotations/annotation_json', file)\n",
        "            if os.path.isfile(src):\n",
        "                shutil.copy2(src, dst)\n",
        "                print(f\"  ✓ Copied {file}\")\n",
        "    \n",
        "    # Copy data_splits directory\n",
        "    data_splits_src = os.path.join(ANNOTATIONS_DRIVE_PATH, 'data_splits')\n",
        "    if os.path.exists(data_splits_src):\n",
        "        print(\"Copying data_splits files...\")\n",
        "        for file in os.listdir(data_splits_src):\n",
        "            src = os.path.join(data_splits_src, file)\n",
        "            dst = os.path.join('annotations/data_splits', file)\n",
        "            if os.path.isfile(src):\n",
        "                shutil.copy2(src, dst)\n",
        "                print(f\"  ✓ Copied {file}\")\n",
        "    \n",
        "    # Copy er_annotations if in Drive\n",
        "    er_annotations_src = os.path.join(ANNOTATIONS_DRIVE_PATH, 'er_annotations')\n",
        "    if os.path.exists(er_annotations_src):\n",
        "        print(\"Copying er_annotations files...\")\n",
        "        for file in os.listdir(er_annotations_src):\n",
        "            src = os.path.join(er_annotations_src, file)\n",
        "            dst = os.path.join('er_annotations', file)\n",
        "            if os.path.isfile(src):\n",
        "                shutil.copy2(src, dst)\n",
        "                print(f\"  ✓ Copied {file}\")\n",
        "else:\n",
        "    print(f\"⚠ Annotations not found at {ANNOTATIONS_DRIVE_PATH}\")\n",
        "    print(\"  If annotations are in your repository, they should already be available.\")\n",
        "    print(\"  Otherwise, please upload them to Drive or update ANNOTATIONS_DRIVE_PATH.\")\n",
        "\n",
        "# Verify required annotation files exist\n",
        "print(\"\\nVerifying annotation files...\")\n",
        "required_files = [\n",
        "    'annotations/annotation_json/step_annotations.json',\n",
        "    'annotations/annotation_json/error_annotations.json',\n",
        "    'er_annotations/recordings_combined_splits.json'\n",
        "]\n",
        "\n",
        "missing = []\n",
        "for file in required_files:\n",
        "    if os.path.exists(file):\n",
        "        print(f\"✓ Found: {file}\")\n",
        "    else:\n",
        "        print(f\"✗ Missing: {file}\")\n",
        "        missing.append(file)\n",
        "\n",
        "if missing:\n",
        "    print(f\"\\n⚠ Warning: {len(missing)} required annotation file(s) are missing!\")\n",
        "    print(\"  You need to provide these files before running Step 3.\")\n",
        "    print(\"  Please check your repository or upload them to Google Drive.\")\n",
        "else:\n",
        "    print(\"\\n✓ All required annotation files are present!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_features"
      },
      "outputs": [],
      "source": [
        "# Create data directory structure\n",
        "import os\n",
        "os.makedirs('data/video/omnivore', exist_ok=True)\n",
        "os.makedirs('data/video/slowfast', exist_ok=True)\n",
        "\n",
        "# Copy omnivore features from Drive\n",
        "print(\"Copying Omnivore features...\")\n",
        "if os.path.exists(OMNIVORE_DRIVE_PATH):\n",
        "    !cp -r \"{OMNIVORE_DRIVE_PATH}\"/* data/video/omnivore/ 2>/dev/null || \\\n",
        "      (echo \"Trying alternative: finding .npz files...\" && \\\n",
        "       find \"{OMNIVORE_DRIVE_PATH}\" -name \"*.npz\" -exec cp {} data/video/omnivore/ \\; 2>/dev/null || true)\n",
        "    print(\"✓ Omnivore features copied\")\n",
        "else:\n",
        "    print(f\"⚠ Warning: {OMNIVORE_DRIVE_PATH} not found\")\n",
        "\n",
        "# Copy slowfast features from Drive\n",
        "print(\"\\nCopying SlowFast features...\")\n",
        "if os.path.exists(SLOWFAST_DRIVE_PATH):\n",
        "    !cp -r \"{SLOWFAST_DRIVE_PATH}\"/* data/video/slowfast/ 2>/dev/null || \\\n",
        "      (echo \"Trying alternative: finding .npz files...\" && \\\n",
        "       find \"{SLOWFAST_DRIVE_PATH}\" -name \"*.npz\" -exec cp {} data/video/slowfast/ \\; 2>/dev/null || true)\n",
        "    print(\"✓ SlowFast features copied\")\n",
        "else:\n",
        "    print(f\"⚠ Warning: {SLOWFAST_DRIVE_PATH} not found\")\n",
        "\n",
        "# Verify features were copied\n",
        "print(\"\\nVerifying features...\")\n",
        "omnivore_count = len([f for f in os.listdir('data/video/omnivore') if f.endswith('.npz')]) if os.path.exists('data/video/omnivore') else 0\n",
        "slowfast_count = len([f for f in os.listdir('data/video/slowfast') if f.endswith('.npz')]) if os.path.exists('data/video/slowfast') else 0\n",
        "print(f\"Omnivore files: {omnivore_count} .npz files\")\n",
        "print(f\"SlowFast files: {slowfast_count} .npz files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_checkpoints"
      },
      "outputs": [],
      "source": [
        "# Create checkpoints directory\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "\n",
        "checkpoint_path = CHECKPOINTS_DRIVE_PATH\n",
        "\n",
        "# Check if path is a zip file or directory\n",
        "is_zip = checkpoint_path.lower().endswith('.zip')\n",
        "if not is_zip:\n",
        "    is_zip = os.path.isfile(checkpoint_path) if os.path.exists(checkpoint_path) else False\n",
        "\n",
        "if is_zip:\n",
        "    print(f\"Detected zip file: {checkpoint_path}\")\n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        print(f\"✗ Error: {checkpoint_path} not found!\")\n",
        "    else:\n",
        "        print(\"Copying zip file to temporary location...\")\n",
        "        shutil.copy(checkpoint_path, '/tmp/checkpoints.zip')\n",
        "        \n",
        "        print(\"Unzipping checkpoints...\")\n",
        "        try:\n",
        "            subprocess.run(['unzip', '-q', '/tmp/checkpoints.zip', '-d', '/tmp/checkpoints_extracted'], check=True)\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"✗ Error unzipping: {e}\")\n",
        "        else:\n",
        "            # Find the error_recognition_best directory in the extracted files\n",
        "            extracted_base = '/tmp/checkpoints_extracted'\n",
        "            extracted_path = None\n",
        "            \n",
        "            # Check if error_recognition_best is at the root\n",
        "            if os.path.exists(os.path.join(extracted_base, 'error_recognition_best')):\n",
        "                extracted_path = os.path.join(extracted_base, 'error_recognition_best')\n",
        "            else:\n",
        "                # Check if MLP/Transformer are directly in extracted_base\n",
        "                if os.path.exists(os.path.join(extracted_base, 'MLP')) or os.path.exists(os.path.join(extracted_base, 'Transformer')):\n",
        "                    extracted_path = extracted_base\n",
        "                else:\n",
        "                    # Search for error_recognition_best in subdirectories\n",
        "                    for root, dirs, files in os.walk(extracted_base):\n",
        "                        if 'error_recognition_best' in dirs:\n",
        "                            extracted_path = os.path.join(root, 'error_recognition_best')\n",
        "                            break\n",
        "                        # Or check if MLP/Transformer are here\n",
        "                        if 'MLP' in dirs or 'Transformer' in dirs:\n",
        "                            extracted_path = root\n",
        "                            break\n",
        "                    \n",
        "                    if extracted_path is None:\n",
        "                        extracted_path = extracted_base\n",
        "            \n",
        "            print(f\"Copying from extracted location: {extracted_path}\")\n",
        "            shutil.copytree(extracted_path, 'checkpoints/error_recognition_best', dirs_exist_ok=True)\n",
        "            \n",
        "            # Cleanup\n",
        "            shutil.rmtree('/tmp/checkpoints_extracted', ignore_errors=True)\n",
        "            os.remove('/tmp/checkpoints.zip')\n",
        "            print(\"✓ Checkpoints extracted and copied\")\n",
        "else:\n",
        "    print(f\"Detected directory: {checkpoint_path}\")\n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        print(f\"✗ Error: {checkpoint_path} not found!\")\n",
        "    else:\n",
        "        print(\"Copying checkpoints from directory...\")\n",
        "        if os.path.basename(checkpoint_path) == 'error_recognition_best':\n",
        "            shutil.copytree(checkpoint_path, 'checkpoints/error_recognition_best', dirs_exist_ok=True)\n",
        "        else:\n",
        "            os.makedirs('checkpoints/error_recognition_best', exist_ok=True)\n",
        "            for item in os.listdir(checkpoint_path):\n",
        "                src = os.path.join(checkpoint_path, item)\n",
        "                dst = os.path.join('checkpoints/error_recognition_best', item)\n",
        "                if os.path.isdir(src):\n",
        "                    shutil.copytree(src, dst, dirs_exist_ok=True)\n",
        "                else:\n",
        "                    shutil.copy2(src, dst)\n",
        "        print(\"✓ Checkpoints copied\")\n",
        "\n",
        "# Verify checkpoints were loaded\n",
        "print(\"\\nVerifying checkpoints...\")\n",
        "if os.path.exists('checkpoints/error_recognition_best'):\n",
        "    print(\"✓ Checkpoints directory exists\")\n",
        "    # Count .pt files\n",
        "    pt_files = []\n",
        "    for root, dirs, files in os.walk('checkpoints/error_recognition_best'):\n",
        "        pt_files.extend([os.path.join(root, f) for f in files if f.endswith('.pt')])\n",
        "    print(f\"Found {len(pt_files)} checkpoint files (.pt)\")\n",
        "    if pt_files:\n",
        "        print(\"\\nSample checkpoint files:\")\n",
        "        for f in pt_files[:5]:\n",
        "            print(f\"  {f}\")\n",
        "    \n",
        "    # Show directory structure\n",
        "    print(\"\\nDirectory structure:\")\n",
        "    for root, dirs, files in os.walk('checkpoints/error_recognition_best'):\n",
        "        level = root.replace('checkpoints/error_recognition_best', '').count(os.sep)\n",
        "        indent = ' ' * 2 * level\n",
        "        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "        if level < 3:  # Limit depth for readability\n",
        "            subindent = ' ' * 2 * (level + 1)\n",
        "            for d in dirs[:5]:  # Show first 5 dirs\n",
        "                print(f\"{subindent}{d}/\")\n",
        "else:\n",
        "    print(\"✗ Checkpoints directory not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2"
      },
      "source": [
        "## 4. Step 2: Feature Sanity Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_step2"
      },
      "outputs": [],
      "source": [
        "# Run Step 2 with default path (data/)\n",
        "!python scripts/run.py step2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_step2_custom"
      },
      "outputs": [],
      "source": [
        "# Or specify custom features root\n",
        "# !python scripts/run.py step2 --features_root /path/to/features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step3"
      },
      "source": [
        "## 5. Step 3: Evaluation Reproduction\n",
        "\n",
        "Run evaluations for different backbones, variants, and splits. Update checkpoint paths with actual epoch numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_step3_omnivore_mlp_step"
      },
      "outputs": [],
      "source": [
        "# Omnivore - MLP - Step split\n",
        "!python scripts/run.py step3 --split step --backbone omnivore --variant MLP \\\n",
        "  --ckpt checkpoints/error_recognition_best/MLP/omnivore/error_recognition_MLP_omnivore_step_epoch_43.pt \\\n",
        "  --threshold 0.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_step3_omnivore_mlp_recordings"
      },
      "outputs": [],
      "source": [
        "# Omnivore - MLP - Recordings split\n",
        "!python scripts/run.py step3 --split recordings --backbone omnivore --variant MLP \\\n",
        "  --ckpt checkpoints/error_recognition_best/MLP/omnivore/error_recognition_MLP_omnivore_recordings_epoch_XX.pt \\\n",
        "  --threshold 0.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_step3_omnivore_transformer_step"
      },
      "outputs": [],
      "source": [
        "# Omnivore - Transformer - Step split\n",
        "!python scripts/run.py step3 --split step --backbone omnivore --variant Transformer \\\n",
        "  --ckpt checkpoints/error_recognition_best/Transformer/omnivore/error_recognition_Transformer_omnivore_step_epoch_XX.pt \\\n",
        "  --threshold 0.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_step3_omnivore_transformer_recordings"
      },
      "outputs": [],
      "source": [
        "# Omnivore - Transformer - Recordings split\n",
        "!python scripts/run.py step3 --split recordings --backbone omnivore --variant Transformer \\\n",
        "  --ckpt checkpoints/error_recognition_best/Transformer/omnivore/error_recognition_Transformer_omnivore_recordings_epoch_XX.pt \\\n",
        "  --threshold 0.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_step3_slowfast_mlp_step"
      },
      "outputs": [],
      "source": [
        "# SlowFast - MLP - Step split\n",
        "!python scripts/run.py step3 --split step --backbone slowfast --variant MLP \\\n",
        "  --ckpt checkpoints/error_recognition_best/MLP/slowfast/error_recognition_MLP_slowfast_step_epoch_XX.pt \\\n",
        "  --threshold 0.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_step3_slowfast_mlp_recordings"
      },
      "outputs": [],
      "source": [
        "# SlowFast - MLP - Recordings split\n",
        "!python scripts/run.py step3 --split recordings --backbone slowfast --variant MLP \\\n",
        "  --ckpt checkpoints/error_recognition_best/MLP/slowfast/error_recognition_MLP_slowfast_recordings_epoch_XX.pt \\\n",
        "  --threshold 0.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_step3_slowfast_transformer_step"
      },
      "outputs": [],
      "source": [
        "# SlowFast - Transformer - Step split\n",
        "!python scripts/run.py step3 --split step --backbone slowfast --variant Transformer \\\n",
        "  --ckpt checkpoints/error_recognition_best/Transformer/slowfast/error_recognition_Transformer_slowfast_step_epoch_XX.pt \\\n",
        "  --threshold 0.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_step3_slowfast_transformer_recordings"
      },
      "outputs": [],
      "source": [
        "# SlowFast - Transformer - Recordings split\n",
        "!python scripts/run.py step3 --split recordings --backbone slowfast --variant Transformer \\\n",
        "  --ckpt checkpoints/error_recognition_best/Transformer/slowfast/error_recognition_Transformer_slowfast_recordings_epoch_XX.pt \\\n",
        "  --threshold 0.4"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
