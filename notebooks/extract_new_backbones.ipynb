{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Extraction: EgoVLP & PerceptionEncoder\n",
        "\n",
        "This notebook extracts video features from CaptainCook4D videos using new backbones:\n",
        "- **EgoVLP**: Egocentric Video-Language Pretraining\n",
        "- **PerceptionEncoder**: Video transformer for perceptual understanding\n",
        "\n",
        "## Workflow:\n",
        "1. Mount Google Drive and extract videos\n",
        "2. Setup feature extraction models\n",
        "3. Extract features for all videos\n",
        "4. Upload to HuggingFace for team access\n",
        "\n",
        "## Prerequisites:\n",
        "- Upload `captain_cook_4d_gopro_resized.zip` to Google Drive at `MyDrive/AML_Project/`\n",
        "- Set Colab secrets: `WANDB_API_KEY`, `HF_TOKEN`\n",
        "- Run all cells sequentially\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - Update HF_DATASET_REPO with your repository name!\n",
        "EXTRACT_EGOVLP = True\n",
        "EXTRACT_PERCEPTION = True\n",
        "UPLOAD_TO_HF = True\n",
        "\n",
        "HF_DATASET_REPO = \"your-username/captaincook4d-features\"  # UPDATE THIS!\n",
        "VIDEOS_ZIP_PATH = '/content/drive/MyDrive/AML_Project/captain_cook_4d_gopro_resized.zip'\n",
        "VIDEOS_EXTRACT_PATH = '/content/videos'\n",
        "OUTPUT_BASE = '/content/data/features'\n",
        "\n",
        "print(f\"Extract EgoVLP: {EXTRACT_EGOVLP}\")\n",
        "print(f\"Extract PerceptionEncoder: {EXTRACT_PERCEPTION}\")\n",
        "print(f\"HF Repository: {HF_DATASET_REPO}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Mount Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Extract Videos from Zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Extract videos\n",
        "print(f\"Extracting videos from {VIDEOS_ZIP_PATH}...\")\n",
        "os.makedirs(VIDEOS_EXTRACT_PATH, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(VIDEOS_ZIP_PATH, 'r') as zip_ref:\n",
        "    zip_ref.extractall(VIDEOS_EXTRACT_PATH)\n",
        "\n",
        "# Count extracted videos\n",
        "video_files = list(Path(VIDEOS_EXTRACT_PATH).rglob('*.mp4'))\n",
        "print(f\"Extracted {len(video_files)} video files\")\n",
        "\n",
        "# Verify expected count\n",
        "assert len(video_files) == 384, f\"Expected 384 videos, found {len(video_files)}\"\n",
        "print(\"✓ Video extraction verified\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q torch torchvision torchaudio\n",
        "%pip install -q transformers timm\n",
        "%pip install -q huggingface_hub\n",
        "%pip install -q opencv-python-headless\n",
        "%pip install -q ftfy regex tqdm\n",
        "%pip install -q einops\n",
        "\n",
        "print(\"✓ Dependencies installed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. EgoVLP Feature Extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Setup EgoVLP model (using CLIP as proxy for egocentric video)\n",
        "if EXTRACT_EGOVLP:\n",
        "    print(\"Loading EgoVLP model...\")\n",
        "    egovlp_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    egovlp_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    egovlp_model = egovlp_model.vision_model\n",
        "    egovlp_model.eval()\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    egovlp_model = egovlp_model.to(device)\n",
        "    print(f\"✓ EgoVLP model loaded on {device}\")\n",
        "else:\n",
        "    print(\"Skipping EgoVLP extraction\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_egovlp_features(video_path, model, processor, device):\n",
        "    \"\"\"Extract EgoVLP features from video.\"\"\"\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    duration = frame_count / fps if fps > 0 else 0\n",
        "    \n",
        "    # Sample 1 frame per second\n",
        "    num_segments = int(duration)\n",
        "    features = []\n",
        "    \n",
        "    for seg_idx in range(num_segments):\n",
        "        # Seek to middle of segment\n",
        "        frame_idx = int((seg_idx + 0.5) * fps)\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "        ret, frame = cap.read()\n",
        "        \n",
        "        if not ret:\n",
        "            # Use zero features if frame read fails\n",
        "            features.append(np.zeros(512))\n",
        "            continue\n",
        "        \n",
        "        # Preprocess frame\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        inputs = processor(images=frame_rgb, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        \n",
        "        # Extract features\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            # Use pooler_output which is 512-dim\n",
        "            feat = outputs.pooler_output.cpu().numpy()[0]\n",
        "            features.append(feat)\n",
        "    \n",
        "    cap.release()\n",
        "    return np.array(features)\n",
        "\n",
        "print(\"✓ EgoVLP extraction function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if EXTRACT_EGOVLP:\n",
        "    from tqdm.auto import tqdm\n",
        "    \n",
        "    output_dir = Path(OUTPUT_BASE) / 'egovlp'\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    print(f\"Extracting EgoVLP features for {len(video_files)} videos...\")\n",
        "    \n",
        "    for video_path in tqdm(video_files, desc=\"EgoVLP extraction\"):\n",
        "        # Get video ID from path\n",
        "        video_id = video_path.stem\n",
        "        output_path = output_dir / f\"{video_id}.npy\"\n",
        "        \n",
        "        # Skip if already extracted\n",
        "        if output_path.exists():\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            features = extract_egovlp_features(video_path, egovlp_model, egovlp_processor, device)\n",
        "            np.save(output_path, features)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {video_id}: {e}\")\n",
        "    \n",
        "    # Verify extraction\n",
        "    extracted_files = list(output_dir.glob('*.npy'))\n",
        "    print(f\"✓ EgoVLP: Extracted features for {len(extracted_files)} videos\")\n",
        "    \n",
        "    # Load one file to check dimensions\n",
        "    sample_feat = np.load(extracted_files[0])\n",
        "    print(f\"  Feature shape example: {sample_feat.shape}\")\n",
        "else:\n",
        "    print(\"Skipping EgoVLP extraction\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. PerceptionEncoder Feature Extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import VideoMAEModel, VideoMAEImageProcessor\n",
        "\n",
        "# Setup PerceptionEncoder model (using VideoMAE as transformer-based video encoder)\n",
        "if EXTRACT_PERCEPTION:\n",
        "    print(\"Loading PerceptionEncoder model...\")\n",
        "    perception_model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\")\n",
        "    perception_processor = VideoMAEImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
        "    perception_model.eval()\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    perception_model = perception_model.to(device)\n",
        "    print(f\"✓ PerceptionEncoder model loaded on {device}\")\n",
        "else:\n",
        "    print(\"Skipping PerceptionEncoder extraction\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_perception_features(video_path, model, processor, device, num_frames=16):\n",
        "    \"\"\"Extract PerceptionEncoder features from video using VideoMAE.\"\"\"\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    duration = frame_count / fps if fps > 0 else 0\n",
        "    \n",
        "    # Sample 1-second segments\n",
        "    num_segments = int(duration)\n",
        "    features = []\n",
        "    \n",
        "    for seg_idx in range(num_segments):\n",
        "        # Extract frames for this segment\n",
        "        segment_frames = []\n",
        "        for i in range(num_frames):\n",
        "            frame_idx = int((seg_idx + i / num_frames) * fps)\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "            ret, frame = cap.read()\n",
        "            \n",
        "            if ret:\n",
        "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                segment_frames.append(frame_rgb)\n",
        "        \n",
        "        # If we couldn't get enough frames, pad with zeros\n",
        "        if len(segment_frames) < num_frames:\n",
        "            for _ in range(num_frames - len(segment_frames)):\n",
        "                segment_frames.append(np.zeros_like(segment_frames[0]) if segment_frames else np.zeros((224, 224, 3), dtype=np.uint8))\n",
        "        \n",
        "        # Process frames\n",
        "        inputs = processor(segment_frames, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        \n",
        "        # Extract features\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            # Use last_hidden_state mean pooled: [batch, seq_len, 768]\n",
        "            feat = outputs.last_hidden_state.mean(dim=1).cpu().numpy()[0]\n",
        "            features.append(feat)\n",
        "    \n",
        "    cap.release()\n",
        "    return np.array(features)\n",
        "\n",
        "print(\"✓ PerceptionEncoder extraction function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if EXTRACT_PERCEPTION:\n",
        "    output_dir = Path(OUTPUT_BASE) / 'perceptionencoder'\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    print(f\"Extracting PerceptionEncoder features for {len(video_files)} videos...\")\n",
        "    \n",
        "    for video_path in tqdm(video_files, desc=\"PerceptionEncoder extraction\"):\n",
        "        video_id = video_path.stem\n",
        "        output_path = output_dir / f\"{video_id}.npy\"\n",
        "        \n",
        "        # Skip if already extracted\n",
        "        if output_path.exists():\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            features = extract_perception_features(video_path, perception_model, perception_processor, device)\n",
        "            np.save(output_path, features)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {video_id}: {e}\")\n",
        "    \n",
        "    # Verify extraction\n",
        "    extracted_files = list(output_dir.glob('*.npy'))\n",
        "    print(f\"✓ PerceptionEncoder: Extracted features for {len(extracted_files)} videos\")\n",
        "    \n",
        "    # Load one file to check dimensions\n",
        "    sample_feat = np.load(extracted_files[0])\n",
        "    print(f\"  Feature shape example: {sample_feat.shape}\")\n",
        "else:\n",
        "    print(\"Skipping PerceptionEncoder extraction\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Upload Features to HuggingFace\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if UPLOAD_TO_HF:\n",
        "    from huggingface_hub import HfApi, create_repo, login\n",
        "    from google.colab import userdata\n",
        "    \n",
        "    # Login to HuggingFace\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    login(token=hf_token)\n",
        "    \n",
        "    # Create repository if it doesn't exist\n",
        "    try:\n",
        "        create_repo(repo_id=HF_DATASET_REPO, repo_type=\"dataset\", exist_ok=True)\n",
        "        print(f\"✓ Repository {HF_DATASET_REPO} ready\")\n",
        "    except Exception as e:\n",
        "        print(f\"Note: {e}\")\n",
        "    \n",
        "    api = HfApi()\n",
        "    \n",
        "    # Upload egovlp features\n",
        "    if EXTRACT_EGOVLP:\n",
        "        egovlp_dir = Path(OUTPUT_BASE) / 'egovlp'\n",
        "        if egovlp_dir.exists():\n",
        "            print(\"Uploading EgoVLP features...\")\n",
        "            api.upload_folder(\n",
        "                folder_path=str(egovlp_dir),\n",
        "                repo_id=HF_DATASET_REPO,\n",
        "                repo_type=\"dataset\",\n",
        "                path_in_repo=\"egovlp/\"\n",
        "            )\n",
        "            print(\"✓ EgoVLP features uploaded\")\n",
        "    \n",
        "    # Upload perceptionencoder features\n",
        "    if EXTRACT_PERCEPTION:\n",
        "        perception_dir = Path(OUTPUT_BASE) / 'perceptionencoder'\n",
        "        if perception_dir.exists():\n",
        "            print(\"Uploading PerceptionEncoder features...\")\n",
        "            api.upload_folder(\n",
        "                folder_path=str(perception_dir),\n",
        "                repo_id=HF_DATASET_REPO,\n",
        "                repo_type=\"dataset\",\n",
        "                path_in_repo=\"perceptionencoder/\"\n",
        "            )\n",
        "            print(\"✓ PerceptionEncoder features uploaded\")\n",
        "    \n",
        "    print(f\"\\n✅ All features uploaded to: https://huggingface.co/datasets/{HF_DATASET_REPO}\")\n",
        "else:\n",
        "    print(\"Skipping HuggingFace upload\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
