{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aexomir/AML_mistake_detection/blob/feat%2Frnn/notebooks/colab_reproduce_results.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbXK2y_qUdbb"
      },
      "source": [
        "# AML/DAAI 2025 - Mistake Detection: Reproduce Results on Colab\n",
        "\n",
        "This notebook helps you reproduce the results from the paper using `scripts/run.py`.\n",
        "\n",
        "## What this notebook does:\n",
        "1. **Step 2**: Feature sanity check - verifies your features are loaded correctly\n",
        "2. **Step 3**: Evaluation reproduction - runs the evaluation to reproduce paper results\n",
        "\n",
        "## Prerequisites:\n",
        "You need to have:\n",
        "- Pre-extracted features (Omnivore and SlowFast) in `.npz` format\n",
        "- Checkpoints from the official release (`error_recognition_best` directory)\n",
        "- Annotation files (should be in the repository or uploaded separately)\n",
        "\n",
        "## Quick Start:\n",
        "1. Upload your data to Google Drive (or use direct upload)\n",
        "2. Configure paths in Section 1\n",
        "3. Run all cells sequentially\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBNnxXN2Udbc"
      },
      "source": [
        "## 1. Setup: Clone Repository & Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snQdZoi4Udbc",
        "outputId": "019b3f1b-b298-4e53-b76c-46d8cdc0e2cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repository URL: https://github.com/aexomir/AML_mistake_detection.git\n",
            "Repository branch: feat/step02\n",
            "Repository directory: aml_repo_v2\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# CONFIGURE YOUR REPOSITORY\n",
        "# ============================================\n",
        "# Option 1: Clone from GitHub (recommended)\n",
        "REPO_URL = \"https://github.com/aexomir/AML_mistake_detection.git\"\n",
        "REPO_BRANCH = \"feat/step02\"  # Leave empty for default branch, or specify branch name\n",
        "\n",
        "# Option 2: Manual upload - set REPO_URL to empty string and upload files manually\n",
        "# REPO_URL = \"\"\n",
        "\n",
        "REPO_DIR = \"aml_repo_v2\"\n",
        "\n",
        "print(f\"Repository URL: {REPO_URL if REPO_URL else 'Manual upload mode'}\")\n",
        "print(f\"Repository branch: {REPO_BRANCH if REPO_BRANCH else 'default'}\")\n",
        "print(f\"Repository directory: {REPO_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GU4eR39Udbd",
        "outputId": "5b09e220-2004-43e1-f2b2-6aa3b665704c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning repository from https://github.com/aexomir/AML_mistake_detection.git...\n",
            "✓ Repository cloned successfully\n",
            "Checking out branch: feat/step02\n",
            "✓ Switched to branch: feat/step02\n",
            "\n",
            "✓ Changed to directory: /content/aml_repo_v2\n",
            "\n",
            "Repository contents:\n",
            "total 6020\n",
            "drwxr-xr-x 8 root root    4096 Dec 27 07:40 .\n",
            "drwxr-xr-x 1 root root    4096 Dec 27 07:40 ..\n",
            "-rw-r--r-- 1 root root 6042142 Dec 27 07:40 3_Mistake_Detection.pdf\n",
            "-rw-r--r-- 1 root root   18838 Dec 27 07:40 base.py\n",
            "-rw-r--r-- 1 root root    1661 Dec 27 07:40 constants.py\n",
            "drwxr-xr-x 3 root root    4096 Dec 27 07:40 core\n",
            "drwxr-xr-x 2 root root    4096 Dec 27 07:40 dataloader\n",
            "-rw-r--r-- 1 root root    6148 Dec 27 07:40 .DS_Store\n",
            "drwxr-xr-x 2 root root    4096 Dec 27 07:40 er_annotations\n",
            "drwxr-xr-x 8 root root    4096 Dec 27 07:40 .git\n",
            "-rw-r--r-- 1 root root      65 Dec 27 07:40 .gitignore\n",
            "-rwxr-xr-x 1 root root    1904 Dec 27 07:40 install_deps.py\n",
            "-rw-r--r-- 1 root root   11357 Dec 27 07:40 LICENSE\n",
            "drwxr-xr-x 2 root root    4096 Dec 27 07:40 notebooks\n",
            "-rw-r--r-- 1 root root    2298 Dec 27 07:40 overview.md\n",
            "-rw-r--r-- 1 root root    2496 Dec 27 07:40 README.md\n",
            "-rw-r--r-- 1 root root     124 Dec 27 07:40 requirements-cpu.txt\n",
            "-rw-r--r-- 1 root root     193 Dec 27 07:40 requirements-cuda.txt\n",
            "-rw-r--r-- 1 root root     304 Dec 27 07:40 requirements.txt\n",
            "drwxr-xr-x 2 root root    4096 Dec 27 07:40 scripts\n",
            "-rw-r--r-- 1 root root    4596 Dec 27 07:40 step_guide.md\n",
            "-rwxr-xr-x 1 root root    1027 Dec 27 07:40 train_er.py\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Remove existing directory if it exists\n",
        "if os.path.exists(REPO_DIR):\n",
        "    print(f\"Removing existing {REPO_DIR} directory...\")\n",
        "    shutil.rmtree(REPO_DIR)\n",
        "\n",
        "# Clone repository\n",
        "if REPO_URL:\n",
        "    print(f\"Cloning repository from {REPO_URL}...\")\n",
        "    clone_cmd = f\"git clone {REPO_URL} {REPO_DIR}\"\n",
        "    result = os.system(clone_cmd)\n",
        "\n",
        "    if result != 0:\n",
        "        print(f\"⚠ Clone failed. Please check the URL or upload files manually.\")\n",
        "        os.makedirs(REPO_DIR, exist_ok=True)\n",
        "    else:\n",
        "        print(\"✓ Repository cloned successfully\")\n",
        "\n",
        "        # Checkout specific branch if specified\n",
        "        if REPO_BRANCH:\n",
        "            print(f\"Checking out branch: {REPO_BRANCH}\")\n",
        "            os.chdir(REPO_DIR)\n",
        "            os.system(f\"git checkout {REPO_BRANCH}\")\n",
        "            os.chdir('..')\n",
        "            print(f\"✓ Switched to branch: {REPO_BRANCH}\")\n",
        "else:\n",
        "    print(\"Manual upload mode: Creating directory...\")\n",
        "    os.makedirs(REPO_DIR, exist_ok=True)\n",
        "\n",
        "# Change to repository directory\n",
        "if os.path.exists(REPO_DIR):\n",
        "    os.chdir(REPO_DIR)\n",
        "    print(f\"\\n✓ Changed to directory: {os.getcwd()}\")\n",
        "    print(f\"\\nRepository contents:\")\n",
        "    !ls -la\n",
        "else:\n",
        "    print(f\"✗ Error: {REPO_DIR} directory not found!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AM1hHgs5Udbd",
        "outputId": "d5a6514e-e587-4e01-c952-87b386264e0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: /content/aml_repo_v2\n",
            "\n",
            "Checking repository structure...\n",
            "✓ Found: scripts/run.py\n",
            "✓ Found: core/evaluate.py\n",
            "✓ Found: dataloader\n",
            "✓ Found: base.py\n",
            "✓ Found: constants.py\n",
            "\n",
            "✓ Repository structure looks good!\n"
          ]
        }
      ],
      "source": [
        "# Verify repository structure\n",
        "import os\n",
        "\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "print(f\"\\nChecking repository structure...\")\n",
        "\n",
        "required_items = [\n",
        "    'scripts/run.py',\n",
        "    'core/evaluate.py',\n",
        "    'dataloader',\n",
        "    'base.py',\n",
        "    'constants.py'\n",
        "]\n",
        "\n",
        "missing = []\n",
        "for item in required_items:\n",
        "    if os.path.exists(item):\n",
        "        print(f\"✓ Found: {item}\")\n",
        "    else:\n",
        "        print(f\"✗ Missing: {item}\")\n",
        "        missing.append(item)\n",
        "\n",
        "if missing:\n",
        "    print(f\"\\n⚠ Warning: Some required files/directories are missing!\")\n",
        "    print(f\"Please ensure all files are present before proceeding.\")\n",
        "else:\n",
        "    print(f\"\\n✓ Repository structure looks good!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8GZbD1-Udbd",
        "outputId": "9c835ce9-6858-46b6-d046-06d19383c3f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.4/454.4 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for gcloud (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langsmith 0.4.59 requires requests-toolbelt>=1.0.0, but you have requests-toolbelt 0.10.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m✓ All dependencies installed successfully\n",
            "\n",
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "CUDA device: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "# Colab comes with PyTorch pre-installed, so we'll work with that\n",
        "# Remove PyTorch version constraints to avoid conflicts\n",
        "if os.path.exists('requirements.txt'):\n",
        "    !sed -i '/^torch==/d' requirements.txt 2>/dev/null || true\n",
        "    !sed -i '/^torchvision==/d' requirements.txt 2>/dev/null || true\n",
        "\n",
        "# Install torcheval (required for evaluation metrics)\n",
        "!pip install -q torcheval\n",
        "\n",
        "# Install all remaining dependencies from requirements.txt\n",
        "if os.path.exists('requirements.txt'):\n",
        "    !pip install -q -r requirements.txt\n",
        "elif os.path.exists('requirements-cpu.txt'):\n",
        "    !pip install -q -r requirements-cpu.txt\n",
        "\n",
        "print(\"✓ All dependencies installed successfully\")\n",
        "\n",
        "# Verify PyTorch installation\n",
        "import torch\n",
        "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnK0NhW_Udbd"
      },
      "source": [
        "## 2. Load Data: Features, Checkpoints, and Annotations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFpEVEmrUdbe",
        "outputId": "86eeb745-74cd-4ae4-e12a-a8eddfe9e0b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data paths configured:\n",
            "  Use Google Drive: True\n",
            "  Omnivore: /content/drive/MyDrive/AML_mistake_detection/omnivore.zip\n",
            "  SlowFast: /content/drive/MyDrive/AML_mistake_detection/slowfast.zip\n",
            "  Checkpoints: /content/drive/MyDrive/AML_mistake_detection/error_recognition_best.zip\n",
            "  Annotations: /content/drive/MyDrive/AML_mistake_detection/annotations\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# CONFIGURE DATA PATHS\n",
        "# ============================================\n",
        "# Option 1: From Google Drive (recommended for large files)\n",
        "USE_GOOGLE_DRIVE = True  # Set to False if uploading directly\n",
        "\n",
        "# Paths on Google Drive (update these to match your Drive structure)\n",
        "OMNIVORE_DRIVE_PATH = \"/content/drive/MyDrive/AML_mistake_detection/omnivore.zip\"  # Can be .zip or directory\n",
        "SLOWFAST_DRIVE_PATH = \"/content/drive/MyDrive/AML_mistake_detection/slowfast.zip\"  # Can be .zip or directory\n",
        "CHECKPOINTS_DRIVE_PATH = \"/content/drive/MyDrive/AML_mistake_detection/error_recognition_best.zip\"  # Can be .zip or directory\n",
        "ANNOTATIONS_DRIVE_PATH = \"/content/drive/MyDrive/AML_mistake_detection/annotations\"  # Optional if in repo\n",
        "\n",
        "# Option 2: Direct upload - set USE_GOOGLE_DRIVE = False and upload files in next cell\n",
        "\n",
        "print(\"Data paths configured:\")\n",
        "print(f\"  Use Google Drive: {USE_GOOGLE_DRIVE}\")\n",
        "print(f\"  Omnivore: {OMNIVORE_DRIVE_PATH}\")\n",
        "print(f\"  SlowFast: {SLOWFAST_DRIVE_PATH}\")\n",
        "print(f\"  Checkpoints: {CHECKPOINTS_DRIVE_PATH}\")\n",
        "print(f\"  Annotations: {ANNOTATIONS_DRIVE_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U009SuJ-Udbe",
        "outputId": "ce2e09fc-8c3a-416b-a9de-34504b6ff1e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "✓ Google Drive mounted\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive if using it\n",
        "if USE_GOOGLE_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"✓ Google Drive mounted\")\n",
        "else:\n",
        "    print(\"⚠ Google Drive not mounted. Please upload files directly using the file browser.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5cuI84XUdbe",
        "outputId": "a6103cdd-d8be-4f73-95d5-9ddc46901986"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Directory structure created\n"
          ]
        }
      ],
      "source": [
        "# Create data directory structure\n",
        "import os\n",
        "os.makedirs('data/video/omnivore', exist_ok=True)\n",
        "os.makedirs('data/video/slowfast', exist_ok=True)\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "os.makedirs('annotations/annotation_json', exist_ok=True)\n",
        "os.makedirs('annotations/data_splits', exist_ok=True)\n",
        "os.makedirs('er_annotations', exist_ok=True)\n",
        "\n",
        "print(\"✓ Directory structure created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTscGipyUdbe",
        "outputId": "9c844c50-1748-4c4a-c36f-b6ac2575961d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Omnivore features from: /content/drive/MyDrive/AML_mistake_detection/omnivore.zip\n",
            "  Detected zip file, extracting...\n",
            "  ✓ Extracted and copied 384 .npz files\n",
            "Loading SlowFast features from: /content/drive/MyDrive/AML_mistake_detection/slowfast.zip\n",
            "  Detected zip file, extracting...\n",
            "  ✓ Extracted and copied 384 .npz files\n",
            "\n",
            "Feature file counts:\n",
            "  Omnivore: 384 .npz files\n",
            "  SlowFast: 384 .npz files\n"
          ]
        }
      ],
      "source": [
        "# Load features from Google Drive or direct upload\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import glob\n",
        "\n",
        "def load_features(source_path, dest_path, feature_name):\n",
        "    \"\"\"Load features from source (zip file or directory) to destination.\"\"\"\n",
        "    if not os.path.exists(source_path):\n",
        "        print(f\"⚠ {feature_name}: Source path not found: {source_path}\")\n",
        "        return False\n",
        "\n",
        "    print(f\"Loading {feature_name} features from: {source_path}\")\n",
        "\n",
        "    # Check if it's a zip file\n",
        "    is_zip = source_path.lower().endswith('.zip') or (os.path.isfile(source_path) and 'zip' in str(source_path))\n",
        "\n",
        "    if is_zip:\n",
        "        print(f\"  Detected zip file, extracting...\")\n",
        "        temp_zip = f'/tmp/{feature_name.lower()}.zip'\n",
        "        temp_extracted = f'/tmp/{feature_name.lower()}_extracted'\n",
        "\n",
        "        try:\n",
        "            shutil.copy(source_path, temp_zip)\n",
        "            subprocess.run(['unzip', '-q', temp_zip, '-d', temp_extracted], check=True)\n",
        "\n",
        "            # Find .npz files in extracted directory\n",
        "            npz_files = glob.glob(os.path.join(temp_extracted, '**/*.npz'), recursive=True)\n",
        "\n",
        "            if npz_files:\n",
        "                # Copy all .npz files to destination\n",
        "                for npz_file in npz_files:\n",
        "                    shutil.copy2(npz_file, dest_path)\n",
        "                print(f\"  ✓ Extracted and copied {len(npz_files)} .npz files\")\n",
        "\n",
        "                # Cleanup\n",
        "                shutil.rmtree(temp_extracted, ignore_errors=True)\n",
        "                os.remove(temp_zip)\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"  ⚠ No .npz files found in extracted zip\")\n",
        "                shutil.rmtree(temp_extracted, ignore_errors=True)\n",
        "                os.remove(temp_zip)\n",
        "                return False\n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ Error extracting {feature_name} zip: {e}\")\n",
        "            if os.path.exists(temp_extracted):\n",
        "                shutil.rmtree(temp_extracted, ignore_errors=True)\n",
        "            if os.path.exists(temp_zip):\n",
        "                os.remove(temp_zip)\n",
        "            return False\n",
        "    else:\n",
        "        # It's a directory\n",
        "        print(f\"  Detected directory, copying .npz files...\")\n",
        "        npz_files = glob.glob(os.path.join(source_path, '**/*.npz'), recursive=True)\n",
        "\n",
        "        if npz_files:\n",
        "            # Copy all .npz files to destination\n",
        "            for npz_file in npz_files:\n",
        "                shutil.copy2(npz_file, dest_path)\n",
        "            print(f\"  ✓ Copied {len(npz_files)} .npz files\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"  ⚠ No .npz files found in {source_path}\")\n",
        "            return False\n",
        "\n",
        "# Load Omnivore and SlowFast features\n",
        "if USE_GOOGLE_DRIVE:\n",
        "    load_features(OMNIVORE_DRIVE_PATH, 'data/video/omnivore', 'Omnivore')\n",
        "    load_features(SLOWFAST_DRIVE_PATH, 'data/video/slowfast', 'SlowFast')\n",
        "else:\n",
        "    print(\"⚠ Please upload features manually:\")\n",
        "    print(\"  1. Use the file browser to upload .npz files or zip files\")\n",
        "    print(\"  2. Extract/copy them to data/video/omnivore/ and data/video/slowfast/\")\n",
        "\n",
        "# Verify features\n",
        "omnivore_count = len([f for f in os.listdir('data/video/omnivore') if f.endswith('.npz')]) if os.path.exists('data/video/omnivore') else 0\n",
        "slowfast_count = len([f for f in os.listdir('data/video/slowfast') if f.endswith('.npz')]) if os.path.exists('data/video/slowfast') else 0\n",
        "print(f\"\\nFeature file counts:\")\n",
        "print(f\"  Omnivore: {omnivore_count} .npz files\")\n",
        "print(f\"  SlowFast: {slowfast_count} .npz files\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c50LOW2CUdbe",
        "outputId": "2828d955-f1ae-476b-edfc-b7d8c084afc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading checkpoints from: /content/drive/MyDrive/AML_mistake_detection/error_recognition_best.zip\n",
            "Detected zip file, extracting...\n",
            "Copying from: /tmp/checkpoints_extracted/error_recognition_best\n",
            "✓ Checkpoints extracted\n",
            "\n",
            "✓ Found 54 checkpoint files\n",
            "\n",
            "Sample checkpoint files:\n",
            "  checkpoints/error_recognition_best/Transformer/slowfast/error_recognition_Transformer_slowfast_environment_epoch_23.pt\n",
            "  checkpoints/error_recognition_best/Transformer/slowfast/error_recognition_Transformer_slowfast_person_epoch_20.pt\n",
            "  checkpoints/error_recognition_best/Transformer/slowfast/error_recognition_Transformer_slowfast_step_epoch_25.pt\n"
          ]
        }
      ],
      "source": [
        "# Load checkpoints\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "\n",
        "checkpoint_path = CHECKPOINTS_DRIVE_PATH if USE_GOOGLE_DRIVE else None\n",
        "\n",
        "if checkpoint_path and os.path.exists(checkpoint_path):\n",
        "    print(f\"Loading checkpoints from: {checkpoint_path}\")\n",
        "\n",
        "    # Check if it's a zip file\n",
        "    is_zip = checkpoint_path.lower().endswith('.zip') or (os.path.isfile(checkpoint_path) and 'zip' in str(checkpoint_path))\n",
        "\n",
        "    if is_zip:\n",
        "        print(\"Detected zip file, extracting...\")\n",
        "        shutil.copy(checkpoint_path, '/tmp/checkpoints.zip')\n",
        "\n",
        "        try:\n",
        "            subprocess.run(['unzip', '-q', '/tmp/checkpoints.zip', '-d', '/tmp/checkpoints_extracted'], check=True)\n",
        "\n",
        "            # Find error_recognition_best directory\n",
        "            extracted_base = '/tmp/checkpoints_extracted'\n",
        "            extracted_path = None\n",
        "\n",
        "            # Check common locations\n",
        "            if os.path.exists(os.path.join(extracted_base, 'error_recognition_best')):\n",
        "                extracted_path = os.path.join(extracted_base, 'error_recognition_best')\n",
        "            elif os.path.exists(os.path.join(extracted_base, 'MLP')) or os.path.exists(os.path.join(extracted_base, 'Transformer')):\n",
        "                extracted_path = extracted_base\n",
        "            else:\n",
        "                # Search recursively\n",
        "                for root, dirs, files in os.walk(extracted_base):\n",
        "                    if 'error_recognition_best' in dirs:\n",
        "                        extracted_path = os.path.join(root, 'error_recognition_best')\n",
        "                        break\n",
        "                    if 'MLP' in dirs or 'Transformer' in dirs:\n",
        "                        extracted_path = root\n",
        "                        break\n",
        "\n",
        "                if extracted_path is None:\n",
        "                    extracted_path = extracted_base\n",
        "\n",
        "            print(f\"Copying from: {extracted_path}\")\n",
        "            shutil.copytree(extracted_path, 'checkpoints/error_recognition_best', dirs_exist_ok=True)\n",
        "\n",
        "            # Cleanup\n",
        "            shutil.rmtree('/tmp/checkpoints_extracted', ignore_errors=True)\n",
        "            os.remove('/tmp/checkpoints.zip')\n",
        "            print(\"✓ Checkpoints extracted\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Error extracting checkpoints: {e}\")\n",
        "    else:\n",
        "        # It's a directory\n",
        "        print(\"Detected directory, copying...\")\n",
        "        if os.path.basename(checkpoint_path) == 'error_recognition_best':\n",
        "            shutil.copytree(checkpoint_path, 'checkpoints/error_recognition_best', dirs_exist_ok=True)\n",
        "        else:\n",
        "            os.makedirs('checkpoints/error_recognition_best', exist_ok=True)\n",
        "            for item in os.listdir(checkpoint_path):\n",
        "                src = os.path.join(checkpoint_path, item)\n",
        "                dst = os.path.join('checkpoints/error_recognition_best', item)\n",
        "                if os.path.isdir(src):\n",
        "                    shutil.copytree(src, dst, dirs_exist_ok=True)\n",
        "                else:\n",
        "                    shutil.copy2(src, dst)\n",
        "        print(\"✓ Checkpoints copied\")\n",
        "else:\n",
        "    print(\"⚠ Checkpoints not found. Please upload manually:\")\n",
        "    print(\"  1. Download from: https://utdallas.app.box.com/s/uz3s1alrzucz03sleify8kazhuc1ksl3\")\n",
        "    print(\"  2. Extract error_recognition_best directory\")\n",
        "    print(\"  3. Upload to checkpoints/error_recognition_best/\")\n",
        "\n",
        "# Verify checkpoints\n",
        "if os.path.exists('checkpoints/error_recognition_best'):\n",
        "    pt_files = []\n",
        "    for root, dirs, files in os.walk('checkpoints/error_recognition_best'):\n",
        "        pt_files.extend([os.path.join(root, f) for f in files if f.endswith('.pt')])\n",
        "    print(f\"\\n✓ Found {len(pt_files)} checkpoint files\")\n",
        "    if pt_files:\n",
        "        print(\"\\nSample checkpoint files:\")\n",
        "        for f in pt_files[:3]:\n",
        "            print(f\"  {f}\")\n",
        "else:\n",
        "    print(\"\\n✗ Checkpoints directory not found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSxArZ6jUdbf",
        "outputId": "26c6952e-0163-42f6-b043-1682ef4f4fc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading annotations from: /content/drive/MyDrive/AML_mistake_detection/annotations\n",
            "  ✓ Copied error_category_idx.json\n",
            "  ✓ Copied activity_idx_step_idx.json\n",
            "  ✓ Copied error_annotations.json\n",
            "  ✓ Copied step_annotations.json\n",
            "  ✓ Copied complete_step_annotations.json\n",
            "  ✓ Copied step_idx_description.json\n",
            "  ✓ Copied recording_id_step_idx.json\n",
            "  ✓ Copied environment_data_split_combined.json\n",
            "  ✓ Copied person_data_split_normal.json\n",
            "  ✓ Copied recordings_data_split_normal.json\n",
            "  ✓ Copied person_data_split_combined.json\n",
            "  ✓ Copied environment_data_split_normal.json\n",
            "  ✓ Copied recordings_data_split_combined.json\n",
            "  ✓ Copied recipes_data_split_normal.json\n",
            "  ✓ Copied recipes_data_split_combined.json\n",
            "\n",
            "Verifying annotation files...\n",
            "✓ Found: annotations/annotation_json/step_annotations.json\n",
            "✓ Found: annotations/annotation_json/error_annotations.json\n",
            "✓ Found: er_annotations/recordings_combined_splits.json\n",
            "\n",
            "✓ All required annotation files are present!\n"
          ]
        }
      ],
      "source": [
        "# Load annotations (if not already in repository)\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "if USE_GOOGLE_DRIVE and os.path.exists(ANNOTATIONS_DRIVE_PATH):\n",
        "    print(f\"Loading annotations from: {ANNOTATIONS_DRIVE_PATH}\")\n",
        "\n",
        "    # Copy annotation_json\n",
        "    annotation_json_src = os.path.join(ANNOTATIONS_DRIVE_PATH, 'annotation_json')\n",
        "    if os.path.exists(annotation_json_src):\n",
        "        for file in os.listdir(annotation_json_src):\n",
        "            src = os.path.join(annotation_json_src, file)\n",
        "            dst = os.path.join('annotations/annotation_json', file)\n",
        "            if os.path.isfile(src):\n",
        "                shutil.copy2(src, dst)\n",
        "                print(f\"  ✓ Copied {file}\")\n",
        "\n",
        "    # Copy data_splits\n",
        "    data_splits_src = os.path.join(ANNOTATIONS_DRIVE_PATH, 'data_splits')\n",
        "    if os.path.exists(data_splits_src):\n",
        "        for file in os.listdir(data_splits_src):\n",
        "            src = os.path.join(data_splits_src, file)\n",
        "            dst = os.path.join('annotations/data_splits', file)\n",
        "            if os.path.isfile(src):\n",
        "                shutil.copy2(src, dst)\n",
        "                print(f\"  ✓ Copied {file}\")\n",
        "\n",
        "    # Copy er_annotations\n",
        "    er_annotations_src = os.path.join(ANNOTATIONS_DRIVE_PATH, 'er_annotations')\n",
        "    if os.path.exists(er_annotations_src):\n",
        "        for file in os.listdir(er_annotations_src):\n",
        "            src = os.path.join(er_annotations_src, file)\n",
        "            dst = os.path.join('er_annotations', file)\n",
        "            if os.path.isfile(src):\n",
        "                shutil.copy2(src, dst)\n",
        "                print(f\"  ✓ Copied {file}\")\n",
        "else:\n",
        "    print(\"⚠ Annotations not found in Drive. Checking repository...\")\n",
        "\n",
        "# Verify required annotation files\n",
        "print(\"\\nVerifying annotation files...\")\n",
        "required_files = [\n",
        "    'annotations/annotation_json/step_annotations.json',\n",
        "    'annotations/annotation_json/error_annotations.json',\n",
        "    'er_annotations/recordings_combined_splits.json'\n",
        "]\n",
        "\n",
        "missing = []\n",
        "for file in required_files:\n",
        "    if os.path.exists(file):\n",
        "        print(f\"✓ Found: {file}\")\n",
        "    else:\n",
        "        print(f\"✗ Missing: {file}\")\n",
        "        missing.append(file)\n",
        "\n",
        "if missing:\n",
        "    print(f\"\\n⚠ Warning: {len(missing)} required annotation file(s) are missing!\")\n",
        "    print(\"Please ensure these files are available before running Step 3.\")\n",
        "else:\n",
        "    print(\"\\n✓ All required annotation files are present!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMZmSwCyUdbf"
      },
      "source": [
        "## 3. Step 2: Feature Sanity Check\n",
        "\n",
        "This step verifies that your features are loaded correctly and can be read.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rUsm7e0Udbf",
        "outputId": "49fc1daf-75d7-4fa7-89f1-b0c4d0f67a0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Step 2: Feature Sanity Check\n",
            "============================================================\n",
            "\n",
            "Features root: /content/aml_repo_v2/data\n",
            "\n",
            "--- Checking OMNIVORE ---\n",
            "✓ Directory found: /content/aml_repo_v2/data/video/omnivore\n",
            "  Found 384 .npz files\n",
            "  Example files:\n",
            "    - 10_16_360p.mp4_1s_1s.npz\n",
            "    - 10_18_360p.mp4_1s_1s.npz\n",
            "    - 10_24_360p.mp4_1s_1s.npz\n",
            "    - 10_26_360p.mp4_1s_1s.npz\n",
            "    - 10_31_360p.mp4_1s_1s.npz\n",
            "\n",
            "  Loading sample file: 10_16_360p.mp4_1s_1s.npz\n",
            "  Keys in file: ['arr_0']\n",
            "  Shape: (974, 1024)\n",
            "  Dtype: float32\n",
            "  Min: -3.1500, Max: 3.1098, Mean: -0.0166\n",
            "\n",
            "--- Checking SLOWFAST ---\n",
            "✓ Directory found: /content/aml_repo_v2/data/video/slowfast\n",
            "  Found 384 .npz files\n",
            "  Example files:\n",
            "    - 10_16_360p.mp4_1s_1s.npz\n",
            "    - 10_18_360p.mp4_1s_1s.npz\n",
            "    - 10_24_360p.mp4_1s_1s.npz\n",
            "    - 10_26_360p.mp4_1s_1s.npz\n",
            "    - 10_31_360p.mp4_1s_1s.npz\n",
            "\n",
            "  Loading sample file: 10_16_360p.mp4_1s_1s.npz\n",
            "  Keys in file: ['arr_0']\n",
            "  Shape: (974, 400)\n",
            "  Dtype: float32\n",
            "  Min: -9.9709, Max: 26.4474, Mean: -0.0000\n",
            "\n",
            "============================================================\n",
            "Summary:\n",
            "============================================================\n",
            "✓ OMNIVORE: exists=True, files=384\n",
            "    Sample shape: (974, 1024), dtype: float32\n",
            "✓ SLOWFAST: exists=True, files=384\n",
            "    Sample shape: (974, 400), dtype: float32\n"
          ]
        }
      ],
      "source": [
        "# Run Step 2: Feature sanity check\n",
        "!python scripts/run.py step2 --features_root data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnTpjYrYUdbf"
      },
      "source": [
        "## 4. Step 3: Evaluation Reproduction\n",
        "\n",
        "Run evaluations to reproduce the results from the paper. Update checkpoint paths with actual epoch numbers from your checkpoints directory.\n",
        "\n",
        "**Note**: Use threshold 0.6 for `step` split and 0.4 for `recordings` split.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nQKdPjmUdbf",
        "outputId": "897f7e5a-4904-464c-8ddc-57005c1731ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Step 3: Evaluation Reproduction\n",
            "============================================================\n",
            "\n",
            "Running: /usr/bin/python3 -m core.evaluate --split step --backbone omnivore --variant MLP --ckpt checkpoints/error_recognition_best/MLP/omnivore/error_recognition_MLP_omnivore_step_epoch_43.pt --threshold 0.6\n",
            "\n",
            "Loaded annotations...... \n",
            "Loading recording ids from recordings_combined_splits.json\n",
            "test Progress: 42347/798: 100% 798/798 [00:10<00:00, 78.94it/s]\n",
            "----------------------------------------------------------------\n",
            "test Sub Step Level Metrics: {'precision': 0.4096162736939436, 'recall': 0.2989708115404083, 'f1': 0.3456549302643129, 'accuracy': 0.6831416629277163, 'auc': np.float64(0.6541560352028618), 'pr_auc': tensor(0.3187)}\n",
            "test Step Level Metrics: {'precision': 0.6607142857142857, 'recall': 0.14859437751004015, 'f1': 0.24262295081967214, 'accuracy': 0.7105263157894737, 'auc': np.float64(0.7573902166041213), 'pr_auc': tensor(0.3638)}\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Example: Omnivore - MLP - Step split\n",
        "# This should reproduce: F1=24.26, AUC=75.74\n",
        "!python scripts/run.py step3 --split step --backbone omnivore --variant MLP \\\n",
        "  --ckpt checkpoints/error_recognition_best/MLP/omnivore/error_recognition_MLP_omnivore_step_epoch_43.pt \\\n",
        "  --threshold 0.6\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEmnB6HWUdbf",
        "outputId": "62a76c3e-62c9-48eb-c5f6-9d61f9fdb3ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Step 3: Evaluation Reproduction\n",
            "============================================================\n",
            "\n",
            "Running: /usr/bin/python3 -m core.evaluate --split recordings --backbone omnivore --variant MLP --ckpt checkpoints/error_recognition_best/MLP/omnivore/error_recognition_MLP_omnivore_recordings_epoch_33.pt --threshold 0.4\n",
            "\n",
            "Loaded annotations...... \n",
            "Loading recording ids from recordings_combined_splits.json\n",
            "test Progress: 38340/671: 100% 671/671 [00:04<00:00, 166.89it/s]\n",
            "----------------------------------------------------------------\n",
            "test Sub Step Level Metrics: {'precision': 0.3964945261528254, 'recall': 0.5688109780280797, 'f1': 0.46727266803505685, 'accuracy': 0.5735263432446531, 'auc': np.float64(0.5988330748775713), 'pr_auc': tensor(0.3673)}\n",
            "test Step Level Metrics: {'precision': 0.4090909090909091, 'recall': 0.8589211618257261, 'f1': 0.5542168674698795, 'accuracy': 0.503725782414307, 'auc': np.float64(0.6302808067162018), 'pr_auc': tensor(0.4020)}\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Example: Omnivore - MLP - Recordings split\n",
        "# This should reproduce: F1=55.42, AUC=63.03\n",
        "# Update the epoch number in the checkpoint path\n",
        "!python scripts/run.py step3 --split recordings --backbone omnivore --variant MLP \\\n",
        "  --ckpt checkpoints/error_recognition_best/MLP/omnivore/error_recognition_MLP_omnivore_recordings_epoch_33.pt \\\n",
        "  --threshold 0.4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKlkNzjXUdbg",
        "outputId": "dc3e1c9a-e46a-4d95-c294-424719d5f334"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Step 3: Evaluation Reproduction\n",
            "============================================================\n",
            "\n",
            "Running: /usr/bin/python3 -m core.evaluate --split step --backbone omnivore --variant Transformer --ckpt checkpoints/error_recognition_best/Transformer/omnivore/error_recognition_Transformer_omnivore_step_epoch_9.pt --threshold 0.6\n",
            "\n",
            "Loaded annotations...... \n",
            "Loading recording ids from recordings_combined_splits.json\n",
            "test Progress: 42347/798: 100% 798/798 [00:05<00:00, 146.67it/s]\n",
            "----------------------------------------------------------------\n",
            "test Sub Step Level Metrics: {'precision': 0.4445452483556362, 'recall': 0.6613801248523705, 'f1': 0.5317056629365887, 'accuracy': 0.6738848088412402, 'auc': np.float64(0.7461755308526944), 'pr_auc': tensor(0.3888)}\n",
            "test Step Level Metrics: {'precision': 0.5155709342560554, 'recall': 0.5983935742971888, 'f1': 0.5539033457249071, 'accuracy': 0.6992481203007519, 'auc': np.float64(0.7561832027563805), 'pr_auc': tensor(0.4338)}\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Example: Omnivore - Transformer - Step split\n",
        "# This should reproduce: F1=55.39, AUC=75.62\n",
        "# Update the epoch number in the checkpoint path\n",
        "!python scripts/run.py step3 --split step --backbone omnivore --variant Transformer \\\n",
        "  --ckpt checkpoints/error_recognition_best/Transformer/omnivore/error_recognition_Transformer_omnivore_step_epoch_9.pt \\\n",
        "  --threshold 0.6\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QVKa7uSUdbg",
        "outputId": "21fba981-e776-4db1-deaa-318812495555"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Step 3: Evaluation Reproduction\n",
            "============================================================\n",
            "\n",
            "Running: /usr/bin/python3 -m core.evaluate --split recordings --backbone omnivore --variant Transformer --ckpt checkpoints/error_recognition_best/Transformer/omnivore/error_recognition_Transformer_omnivore_recordings_epoch_31.pt --threshold 0.4\n",
            "\n",
            "Loaded annotations...... \n",
            "Loading recording ids from recordings_combined_splits.json\n",
            "test Progress: 38340/671: 100% 671/671 [00:04<00:00, 143.71it/s]\n",
            "----------------------------------------------------------------\n",
            "test Sub Step Level Metrics: {'precision': 0.4491327720864185, 'recall': 0.35123344173871657, 'f1': 0.39419567346212053, 'accuracy': 0.645018257694314, 'auc': np.float64(0.6254427005929003), 'pr_auc': tensor(0.3711)}\n",
            "test Step Level Metrics: {'precision': 0.45408163265306123, 'recall': 0.36929460580912865, 'f1': 0.4073226544622426, 'accuracy': 0.6140089418777943, 'auc': np.float64(0.6226768310334846), 'pr_auc': tensor(0.3942)}\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Example: Omnivore - Transformer - Recordings split\n",
        "# This should reproduce: F1=40.73, AUC=62.27\n",
        "# Update the epoch number in the checkpoint path\n",
        "!python scripts/run.py step3 --split recordings --backbone omnivore --variant Transformer \\\n",
        "  --ckpt checkpoints/error_recognition_best/Transformer/omnivore/error_recognition_Transformer_omnivore_recordings_epoch_31.pt \\\n",
        "  --threshold 0.4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLtDRdRawHqH",
        "outputId": "60d6f518-c8d7-4e86-d416-7b0444ba9325"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Step 3: Evaluation Reproduction\n",
            "============================================================\n",
            "\n",
            "Running: /usr/bin/python3 -m core.evaluate --split step --backbone slowfast --variant MLP --ckpt checkpoints/error_recognition_best/MLP/slowfast/error_recognition_MLP_slowfast_step_epoch_15.pt --threshold 0.6\n",
            "\n",
            "Loaded annotations...... \n",
            "Loading recording ids from recordings_combined_splits.json\n",
            "test Progress: 42347/798: 100% 798/798 [00:04<00:00, 175.90it/s]\n",
            "----------------------------------------------------------------\n",
            "test Sub Step Level Metrics: {'precision': 0.3910761154855643, 'recall': 0.03770879028176143, 'f1': 0.06878510425482803, 'accuracy': 0.7141946300800529, 'auc': np.float64(0.5777348914133424), 'pr_auc': tensor(0.2841)}\n",
            "test Step Level Metrics: {'precision': 0.31917631917631917, 'recall': 0.9959839357429718, 'f1': 0.4834307992202729, 'accuracy': 0.3358395989974937, 'auc': np.float64(0.6309610024798646), 'pr_auc': tensor(0.3191)}\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Example: Slowfast - MLP - Step split\n",
        "!python scripts/run.py step3 --split step --backbone slowfast --variant MLP \\\n",
        "  --ckpt checkpoints/error_recognition_best/MLP/slowfast/error_recognition_MLP_slowfast_step_epoch_15.pt \\\n",
        "  --threshold 0.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLltp2huxTcG",
        "outputId": "f716ade5-079b-43de-d502-963817a0b597"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Step 3: Evaluation Reproduction\n",
            "============================================================\n",
            "\n",
            "Running: /usr/bin/python3 -m core.evaluate --split recordings --backbone slowfast --variant MLP --ckpt checkpoints/error_recognition_best/MLP/slowfast/error_recognition_MLP_slowfast_recordings_epoch_31.pt --threshold 0.4\n",
            "\n",
            "Loaded annotations...... \n",
            "Loading recording ids from recordings_combined_splits.json\n",
            "test Progress: 38340/671: 100% 671/671 [00:02<00:00, 333.60it/s]\n",
            "----------------------------------------------------------------\n",
            "test Sub Step Level Metrics: {'precision': 0.34477329974811083, 'recall': 0.17371301657809154, 'f1': 0.23102484308244106, 'accuracy': 0.6197443922796035, 'auc': np.float64(0.5378167726294552), 'pr_auc': tensor(0.3316)}\n",
            "test Step Level Metrics: {'precision': 0.4138755980861244, 'recall': 0.7178423236514523, 'f1': 0.5250379362670713, 'accuracy': 0.533532041728763, 'auc': np.float64(0.5689375663417928), 'pr_auc': tensor(0.3984)}\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Example: Slowfast - MLP - Recordings split\n",
        "!python scripts/run.py step3 --split recordings --backbone slowfast --variant MLP \\\n",
        "  --ckpt checkpoints/error_recognition_best/MLP/slowfast/error_recognition_MLP_slowfast_recordings_epoch_31.pt \\\n",
        "  --threshold 0.4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSNGrvRfxt8F",
        "outputId": "7b610c10-94e5-4ab7-830e-efc15530ca96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Step 3: Evaluation Reproduction\n",
            "============================================================\n",
            "\n",
            "Running: /usr/bin/python3 -m core.evaluate --split step --backbone slowfast --variant Transformer --ckpt checkpoints/error_recognition_best/Transformer/slowfast/error_recognition_Transformer_slowfast_step_epoch_25.pt --threshold 0.6\n",
            "\n",
            "Loaded annotations...... \n",
            "Loading recording ids from recordings_combined_splits.json\n",
            "test Progress: 42347/798: 100% 798/798 [00:02<00:00, 278.93it/s]\n",
            "----------------------------------------------------------------\n",
            "test Sub Step Level Metrics: {'precision': 0.44196376388077147, 'recall': 0.3189640627636241, 'f1': 0.3705228085648488, 'accuracy': 0.6966254988547005, 'auc': np.float64(0.6529493037622427), 'pr_auc': tensor(0.3316)}\n",
            "test Step Level Metrics: {'precision': 0.47692307692307695, 'recall': 0.24899598393574296, 'f1': 0.32717678100263853, 'accuracy': 0.6804511278195489, 'auc': np.float64(0.6713630478196941), 'pr_auc': tensor(0.3531)}\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Example: Slowfast - Transformer - Step split\n",
        "!python scripts/run.py step3 --split step --backbone slowfast --variant Transformer \\\n",
        "  --ckpt checkpoints/error_recognition_best/Transformer/slowfast/error_recognition_Transformer_slowfast_step_epoch_25.pt \\\n",
        "  --threshold 0.6\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjAWOGiJyF1Z",
        "outputId": "0109cbe8-f6c6-44d4-fa74-9b9ecc8f5748"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Step 3: Evaluation Reproduction\n",
            "============================================================\n",
            "\n",
            "Running: /usr/bin/python3 -m core.evaluate --split recordings --backbone slowfast --variant Transformer --ckpt checkpoints/error_recognition_best/Transformer/slowfast/error_recognition_Transformer_slowfast_recordings_epoch_49.pt --threshold 0.4\n",
            "\n",
            "Loaded annotations...... \n",
            "Loading recording ids from recordings_combined_splits.json\n",
            "test Progress: 38340/671: 100% 671/671 [00:02<00:00, 271.76it/s]\n",
            "----------------------------------------------------------------\n",
            "test Sub Step Level Metrics: {'precision': 0.3889220960350667, 'recall': 0.4645038470690886, 'f1': 0.4233661075766339, 'accuracy': 0.5839332290036515, 'auc': np.float64(0.6017853112151881), 'pr_auc': tensor(0.3567)}\n",
            "test Step Level Metrics: {'precision': 0.4115755627009646, 'recall': 0.5311203319502075, 'f1': 0.463768115942029, 'accuracy': 0.5588673621460507, 'auc': np.float64(0.5982727009553218), 'pr_auc': tensor(0.3870)}\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Example: Slowfast - Transformer - Recordings split\n",
        "!python scripts/run.py step3 --split recordings --backbone slowfast --variant Transformer \\\n",
        "  --ckpt checkpoints/error_recognition_best/Transformer/slowfast/error_recognition_Transformer_slowfast_recordings_epoch_49.pt \\\n",
        "  --threshold 0.4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAEkeXGsUdbg"
      },
      "source": [
        "## Additional Evaluations\n",
        "\n",
        "You can also run evaluations for SlowFast backbone or other configurations. Make sure to update the checkpoint paths with the correct epoch numbers from your checkpoints directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PIwzH8kUdbg",
        "outputId": "1f9015df-0088-4323-8542-e0a490dac25e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available checkpoints:\n",
            "  checkpoints/error_recognition_best/MLP/3dresnet/error_recognition_MLP_3dresnet_environment_epoch_11.pt\n",
            "  checkpoints/error_recognition_best/MLP/3dresnet/error_recognition_MLP_3dresnet_person_epoch_39.pt\n",
            "  checkpoints/error_recognition_best/MLP/3dresnet/error_recognition_MLP_3dresnet_recordings_epoch_45.pt\n",
            "  checkpoints/error_recognition_best/MLP/3dresnet/error_recognition_MLP_3dresnet_step_epoch_41.pt\n",
            "  checkpoints/error_recognition_best/MLP/imagebind/error_recognition_MLP_imagebind_audio_environment_epoch_50.pt\n",
            "  checkpoints/error_recognition_best/MLP/imagebind/error_recognition_MLP_imagebind_audio_person_epoch_8.pt\n",
            "  checkpoints/error_recognition_best/MLP/imagebind/error_recognition_MLP_imagebind_audio_recordings_epoch_2.pt\n",
            "  checkpoints/error_recognition_best/MLP/imagebind/error_recognition_MLP_imagebind_audio_step_epoch_28.pt\n",
            "  checkpoints/error_recognition_best/MLP/imagebind/error_recognition_MLP_imagebind_video_audio_environment_epoch_31.pt\n",
            "  checkpoints/error_recognition_best/MLP/imagebind/error_recognition_MLP_imagebind_video_audio_person_epoch_21.pt\n",
            "  checkpoints/error_recognition_best/MLP/imagebind/error_recognition_MLP_imagebind_video_audio_recordings_epoch_41.pt\n",
            "  checkpoints/error_recognition_best/MLP/imagebind/error_recognition_MLP_imagebind_video_audio_step_epoch_38.pt\n",
            "  checkpoints/error_recognition_best/MLP/imagebind/error_recognition_MLP_imagebind_video_environment_epoch_11.pt\n",
            "  checkpoints/error_recognition_best/MLP/imagebind/error_recognition_MLP_imagebind_video_person_epoch_48.pt\n",
            "  checkpoints/error_recognition_best/MLP/imagebind/error_recognition_MLP_imagebind_video_recordings_epoch_42.pt\n",
            "  checkpoints/error_recognition_best/MLP/imagebind/error_recognition_MLP_imagebind_video_step_epoch_37.pt\n",
            "  checkpoints/error_recognition_best/MLP/omnivore/error_recognition_MLP_omnivore_environment_epoch_5.pt\n",
            "  checkpoints/error_recognition_best/MLP/omnivore/error_recognition_MLP_omnivore_person_epoch_28.pt\n",
            "  checkpoints/error_recognition_best/MLP/omnivore/error_recognition_MLP_omnivore_recordings_epoch_33.pt\n",
            "  checkpoints/error_recognition_best/MLP/omnivore/error_recognition_MLP_omnivore_step_epoch_43.pt\n",
            "  checkpoints/error_recognition_best/MLP/slowfast/error_recognition_MLP_slowfast_environment_epoch_7.pt\n",
            "  checkpoints/error_recognition_best/MLP/slowfast/error_recognition_MLP_slowfast_person_epoch_3.pt\n",
            "  checkpoints/error_recognition_best/MLP/slowfast/error_recognition_MLP_slowfast_recordings_epoch_31.pt\n",
            "  checkpoints/error_recognition_best/MLP/slowfast/error_recognition_MLP_slowfast_step_epoch_15.pt\n",
            "  checkpoints/error_recognition_best/MLP/x3d/error_recognition_MLP_x3d_environment_epoch_2.pt\n",
            "  checkpoints/error_recognition_best/MLP/x3d/error_recognition_MLP_x3d_person_epoch_13.pt\n",
            "  checkpoints/error_recognition_best/MLP/x3d/error_recognition_MLP_x3d_recordings_epoch_5.pt\n",
            "  checkpoints/error_recognition_best/MLP/x3d/error_recognition_MLP_x3d_step_epoch_15.pt\n",
            "  checkpoints/error_recognition_best/Transformer/3dresnet/error_recognition_Transformer_3dresnet_environment_epoch_39.pt\n",
            "  checkpoints/error_recognition_best/Transformer/3dresnet/error_recognition_Transformer_3dresnet_person_epoch_50.pt\n",
            "  checkpoints/error_recognition_best/Transformer/3dresnet/error_recognition_Transformer_3dresnet_recordings_epoch_25.pt\n",
            "  checkpoints/error_recognition_best/Transformer/3dresnet/error_recognition_Transformer_3dresnet_step_epoch_3.pt\n",
            "  checkpoints/error_recognition_best/Transformer/imagebind/error_recognition_Transformer_imagebind_audio_environment_epoch_4.pt\n",
            "  checkpoints/error_recognition_best/Transformer/imagebind/error_recognition_Transformer_imagebind_audio_person_epoch_19.pt\n",
            "  checkpoints/error_recognition_best/Transformer/imagebind/error_recognition_Transformer_imagebind_audio_recordings_epoch_31.pt\n",
            "  checkpoints/error_recognition_best/Transformer/imagebind/error_recognition_Transformer_imagebind_audio_step_epoch_44.pt\n",
            "  checkpoints/error_recognition_best/Transformer/imagebind/error_recognition_Transformer_imagebind_video_audio_environment_epoch_8.pt\n",
            "  checkpoints/error_recognition_best/Transformer/imagebind/error_recognition_Transformer_imagebind_video_audio_person_epoch_17.pt\n",
            "  checkpoints/error_recognition_best/Transformer/imagebind/error_recognition_Transformer_imagebind_video_audio_recordings_epoch_5.pt\n",
            "  checkpoints/error_recognition_best/Transformer/imagebind/error_recognition_Transformer_imagebind_video_audio_step_epoch_9.pt\n",
            "  checkpoints/error_recognition_best/Transformer/imagebind/error_recognition_Transformer_imagebind_video_recordings_epoch_27.pt\n",
            "  checkpoints/error_recognition_best/Transformer/imagebind/error_recognition_Transformer_imagebind_video_step_epoch_20.pt\n",
            "  checkpoints/error_recognition_best/Transformer/omnivore/error_recognition_Transformer_omnivore_environment_epoch_6.pt\n",
            "  checkpoints/error_recognition_best/Transformer/omnivore/error_recognition_Transformer_omnivore_person_epoch_4.pt\n",
            "  checkpoints/error_recognition_best/Transformer/omnivore/error_recognition_Transformer_omnivore_recordings_epoch_31.pt\n",
            "  checkpoints/error_recognition_best/Transformer/omnivore/error_recognition_Transformer_omnivore_step_epoch_9.pt\n",
            "  checkpoints/error_recognition_best/Transformer/slowfast/error_recognition_Transformer_slowfast_environment_epoch_23.pt\n",
            "  checkpoints/error_recognition_best/Transformer/slowfast/error_recognition_Transformer_slowfast_person_epoch_20.pt\n",
            "  checkpoints/error_recognition_best/Transformer/slowfast/error_recognition_Transformer_slowfast_recordings_epoch_49.pt\n",
            "  checkpoints/error_recognition_best/Transformer/slowfast/error_recognition_Transformer_slowfast_step_epoch_25.pt\n",
            "  checkpoints/error_recognition_best/Transformer/x3d/error_recognition_Transformer_x3d_environment_epoch_49.pt\n",
            "  checkpoints/error_recognition_best/Transformer/x3d/error_recognition_Transformer_x3d_person_epoch_6.pt\n",
            "  checkpoints/error_recognition_best/Transformer/x3d/error_recognition_Transformer_x3d_recordings_epoch_43.pt\n",
            "  checkpoints/error_recognition_best/Transformer/x3d/error_recognition_Transformer_x3d_step_epoch_40.pt\n"
          ]
        }
      ],
      "source": [
        "# List available checkpoints to find correct epoch numbers\n",
        "import os\n",
        "import glob\n",
        "\n",
        "checkpoint_base = 'checkpoints/error_recognition_best'\n",
        "if os.path.exists(checkpoint_base):\n",
        "    print(\"Available checkpoints:\")\n",
        "    for ckpt_file in sorted(glob.glob(os.path.join(checkpoint_base, '**/*.pt'), recursive=True)):\n",
        "        print(f\"  {ckpt_file}\")\n",
        "else:\n",
        "    print(\"Checkpoints directory not found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieSc99u6Udbg"
      },
      "source": [
        "## Troubleshooting\n",
        "\n",
        "### Common Issues:\n",
        "\n",
        "1. **Missing features**: Make sure you have uploaded both Omnivore and SlowFast features\n",
        "2. **Missing checkpoints**: Download from the official Box link and extract properly\n",
        "3. **Missing annotations**: Ensure annotation files are in the correct directories\n",
        "4. **Wrong checkpoint path**: Use the cell above to list available checkpoints and update paths\n",
        "5. **CUDA errors**: The code will automatically fall back to CPU if CUDA is not available\n",
        "\n",
        "### Getting Help:\n",
        "- Check the README.md in the repository\n",
        "- Verify all file paths are correct\n",
        "- Ensure all dependencies are installed\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
