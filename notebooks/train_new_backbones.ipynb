{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training with EgoVLP Backbone\n",
        "\n",
        "Train MLP, Transformer, and RNN models with EgoVLP video backbone.\n",
        "\n",
        "## Workflow:\n",
        "1. Mount Google Drive\n",
        "2. Clone repository and install dependencies\n",
        "3. **Load annotations from Google Drive**\n",
        "4. **Load pre-extracted EgoVLP features from Google Drive**\n",
        "5. Train models (MLP, Transformer, RNN) with EgoVLP backbone\n",
        "6. Evaluate and compare results\n",
        "\n",
        "## Prerequisites:\n",
        "- Set Colab secrets: `WANDB_API_KEY`\n",
        "- **Upload EgoVLP features zip file** (`egovlp.zip`) to Google Drive at `MyDrive/AML_mistake_detection/features/`\n",
        "- **Upload CaptainCook4D annotations to Google Drive** at `MyDrive/AML_mistake_detection/annotations/`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "HF_DATASET_REPO = \"aexomir/captaincook4d-features\" \n",
        "REPO_URL = \"https://github.com/aexomir/AML_mistake_detection.git\"\n",
        "REPO_BRANCH = \"feat/extend-baseline\"\n",
        "\n",
        "# Google Drive path for annotations\n",
        "ANNOTATIONS_DRIVE_PATH = '/content/drive/MyDrive/AML_mistake_detection/annotations'\n",
        "\n",
        "# Backbone to train (EgoVLP only)\n",
        "BACKBONE = \"egovlp\"\n",
        "# Model variants\n",
        "VARIANTS = [\"MLP\", \"Transformer\", \"RNN\"]\n",
        "\n",
        "print(f\"Will train {len(VARIANTS)} variants with {BACKBONE} backbone\")\n",
        "print(f\"Backbone: {BACKBONE}\")\n",
        "print(f\"Variants: {VARIANTS}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Mount Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Clone Repository and Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash -s \"$REPO_URL\" \"$REPO_BRANCH\"\n",
        "cd /content\n",
        "if [ ! -d \"AML_mistake_detection\" ]; then\n",
        "    git clone --branch \"$2\" \"$1\" AML_mistake_detection\n",
        "else\n",
        "    cd AML_mistake_detection && git pull\n",
        "fi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/AML_mistake_detection')\n",
        "\n",
        "%pip install -q -r requirements.txt\n",
        "%pip install -q wandb huggingface_hub\n",
        "\n",
        "print(\"✓ Dependencies installed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Annotations from Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "print(\"Loading CaptainCook4D annotations...\")\n",
        "\n",
        "if os.path.exists(ANNOTATIONS_DRIVE_PATH):\n",
        "    print(f\"✓ Found annotations at: {ANNOTATIONS_DRIVE_PATH}\")\n",
        "    \n",
        "    # Create destination directories if they don't exist\n",
        "    os.makedirs('annotations/annotation_json', exist_ok=True)\n",
        "    os.makedirs('annotations/data_splits', exist_ok=True)\n",
        "    \n",
        "    # Copy annotation_json (step_annotations.json, error_annotations.json)\n",
        "    annotation_json_src = os.path.join(ANNOTATIONS_DRIVE_PATH, 'annotation_json')\n",
        "    if os.path.exists(annotation_json_src):\n",
        "        print(\"  Copying annotation_json files...\")\n",
        "        for file in os.listdir(annotation_json_src):\n",
        "            src = os.path.join(annotation_json_src, file)\n",
        "            dst = os.path.join('annotations/annotation_json', file)\n",
        "            if os.path.isfile(src):\n",
        "                shutil.copy2(src, dst)\n",
        "                print(f\"    ✓ Copied {file}\")\n",
        "    else:\n",
        "        print(f\"  ⚠️  annotation_json not found at {annotation_json_src}\")\n",
        "    \n",
        "    # Copy data_splits (optional)\n",
        "    data_splits_src = os.path.join(ANNOTATIONS_DRIVE_PATH, 'data_splits')\n",
        "    if os.path.exists(data_splits_src):\n",
        "        print(\"  Copying data_splits files...\")\n",
        "        for file in os.listdir(data_splits_src):\n",
        "            src = os.path.join(data_splits_src, file)\n",
        "            dst = os.path.join('annotations/data_splits', file)\n",
        "            if os.path.isfile(src):\n",
        "                shutil.copy2(src, dst)\n",
        "                print(f\"    ✓ Copied {file}\")\n",
        "    \n",
        "    # Verify required files\n",
        "    required_files = [\n",
        "        'annotations/annotation_json/step_annotations.json',\n",
        "        'annotations/annotation_json/error_annotations.json',\n",
        "        'er_annotations/recordings_combined_splits.json'\n",
        "    ]\n",
        "    \n",
        "    missing_files = [f for f in required_files if not os.path.exists(f)]\n",
        "    \n",
        "    if missing_files:\n",
        "        print(f\"\\n❌ ERROR: Missing required annotation files:\")\n",
        "        for file in missing_files:\n",
        "            print(f\"  - {file}\")\n",
        "        print(\"\\nPlease ensure these files are in your Google Drive at:\")\n",
        "        print(f\"  {ANNOTATIONS_DRIVE_PATH}/\")\n",
        "        raise FileNotFoundError(\"Missing required annotation files\")\n",
        "    else:\n",
        "        print(\"\\n✅ All required annotation files loaded successfully!\")\n",
        "        \n",
        "else:\n",
        "    print(f\"❌ ERROR: Annotations directory not found at: {ANNOTATIONS_DRIVE_PATH}\")\n",
        "    print(\"\\nPlease upload CaptainCook4D annotations to Google Drive:\")\n",
        "    print(\"  Required structure:\")\n",
        "    print(\"  MyDrive/AML_mistake_detection/annotations/\")\n",
        "    print(\"    ├── annotation_json/\")\n",
        "    print(\"    │   ├── step_annotations.json\")\n",
        "    print(\"    │   └── error_annotations.json\")\n",
        "    print(\"    └── data_splits/ (optional)\")\n",
        "    print(\"\\nDownload from: https://captaincook4d.github.io/captain-cook/\")\n",
        "    raise FileNotFoundError(\"Annotations directory not found in Google Drive\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load EgoVLP Features from Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Google Drive path for features zip file\n",
        "FEATURES_DRIVE_PATH = '/content/drive/MyDrive/AML_mistake_detection/features'\n",
        "ZIP_FILE_NAME = f'{BACKBONE}.zip'\n",
        "zip_file_path = os.path.join(FEATURES_DRIVE_PATH, ZIP_FILE_NAME)\n",
        "\n",
        "# Create data/features directory structure\n",
        "features_base = Path('/content/AML_mistake_detection/data/features')\n",
        "backbone_dir = features_base / BACKBONE\n",
        "backbone_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Loading {BACKBONE} features from Google Drive...\")\n",
        "\n",
        "# Check if zip file exists\n",
        "if not os.path.exists(zip_file_path):\n",
        "    print(f\"❌ ERROR: Zip file not found at: {zip_file_path}\")\n",
        "    print(f\"\\nPlease ensure {ZIP_FILE_NAME} is uploaded to Google Drive at:\")\n",
        "    print(f\"  {FEATURES_DRIVE_PATH}/\")\n",
        "    raise FileNotFoundError(f\"Features zip file not found: {zip_file_path}\")\n",
        "\n",
        "print(f\"✓ Found zip file: {zip_file_path}\")\n",
        "\n",
        "# Extract zip file if directory is empty or doesn't exist\n",
        "if not list(backbone_dir.glob('*.npy')):\n",
        "    print(f\"Extracting {BACKBONE} features...\")\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        # Extract all files\n",
        "        zip_ref.extractall(backbone_dir)\n",
        "        # If files were extracted with subdirectory structure, move them to root\n",
        "        for root, dirs, files in os.walk(backbone_dir):\n",
        "            for file in files:\n",
        "                if file.endswith('.npy'):\n",
        "                    src = Path(root) / file\n",
        "                    dst = backbone_dir / file\n",
        "                    if src != dst and not dst.exists():\n",
        "                        src.rename(dst)\n",
        "        # Clean up empty subdirectories\n",
        "        for root, dirs, files in os.walk(backbone_dir, topdown=False):\n",
        "            for dir_name in dirs:\n",
        "                dir_path = Path(root) / dir_name\n",
        "                try:\n",
        "                    if not any(dir_path.iterdir()):\n",
        "                        dir_path.rmdir()\n",
        "                except OSError:\n",
        "                    pass\n",
        "    print(f\"✓ Extracted {BACKBONE} features\")\n",
        "else:\n",
        "    print(f\"✓ Features already extracted (found {len(list(backbone_dir.glob('*.npy')))} files)\")\n",
        "\n",
        "print(f\"✓ Total files: {len(list(backbone_dir.glob('*.npy')))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Setup WandB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "# Login to WandB\n",
        "wandb_key = userdata.get('WANDB_API_KEY')\n",
        "wandb.login(key=wandb_key)\n",
        "\n",
        "print(\"✓ WandB configured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Train Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Training configuration\n",
        "SPLIT = \"recordings\"\n",
        "THRESHOLD = 0.6\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "training_results = {}\n",
        "\n",
        "for variant in VARIANTS:\n",
        "    model_name = f\"{variant}_{BACKBONE}\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training: {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Select appropriate training script\n",
        "    if variant == \"RNN\":\n",
        "        train_script = \"scripts/train_rnn_baseline.py\"\n",
        "    else:\n",
        "        train_script = \"train_er.py\"\n",
        "    \n",
        "    # Build training command\n",
        "    cmd = [\n",
        "        sys.executable, train_script,\n",
        "        \"--backbone\", BACKBONE,\n",
        "        \"--variant\", variant,\n",
        "        \"--split\", SPLIT,\n",
        "        \"--threshold\", str(THRESHOLD),\n",
        "        \"--epochs\", str(EPOCHS),\n",
        "        \"--batch_size\", str(BATCH_SIZE),\n",
        "        \"--use_wandb\"\n",
        "    ]\n",
        "    \n",
        "    try:\n",
        "        # Run training\n",
        "        result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
        "        print(result.stdout)\n",
        "        training_results[model_name] = \"SUCCESS\"\n",
        "        print(f\"✓ {model_name} training completed\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"✗ {model_name} training failed:\")\n",
        "        print(e.stderr)\n",
        "        training_results[model_name] = \"FAILED\"\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Training Summary:\")\n",
        "print(f\"{'='*60}\")\n",
        "for model, status in training_results.items():\n",
        "    status_icon = \"✓\" if status == \"SUCCESS\" else \"✗\"\n",
        "    print(f\"{status_icon} {model}: {status}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluate Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation_results = {}\n",
        "\n",
        "for variant in VARIANTS:\n",
        "    model_name = f\"{variant}_{BACKBONE}\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Evaluating: {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Build evaluation command\n",
        "    cmd = [\n",
        "        sys.executable, \"core/evaluate.py\",\n",
        "        \"--backbone\", BACKBONE,\n",
        "        \"--variant\", variant,\n",
        "        \"--split\", SPLIT,\n",
        "        \"--threshold\", str(THRESHOLD)\n",
        "    ]\n",
        "    \n",
        "    try:\n",
        "        # Run evaluation\n",
        "        result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
        "        print(result.stdout)\n",
        "        evaluation_results[model_name] = \"SUCCESS\"\n",
        "        print(f\"✓ {model_name} evaluation completed\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"✗ {model_name} evaluation failed:\")\n",
        "        print(e.stderr)\n",
        "        evaluation_results[model_name] = \"FAILED\"\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Evaluation Summary:\")\n",
        "print(f\"{'='*60}\")\n",
        "for model, status in evaluation_results.items():\n",
        "    status_icon = \"✓\" if status == \"SUCCESS\" else \"✗\"\n",
        "    print(f\"{status_icon} {model}: {status}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Generate Comparison Report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract metrics from this notebook's outputs\n",
        "print(\"Extracting metrics...\")\n",
        "cmd = [sys.executable, \"analysis/extract_metrics.py\"]\n",
        "subprocess.run(cmd, check=True)\n",
        "\n",
        "# Generate comparison tables\n",
        "print(\"\\nGenerating comparison tables...\")\n",
        "cmd = [sys.executable, \"analysis/comparison_tables.py\"]\n",
        "subprocess.run(cmd, check=True)\n",
        "\n",
        "# Generate visualizations\n",
        "print(\"\\nGenerating visualizations...\")\n",
        "cmd = [sys.executable, \"analysis/comparison_visualizations.py\"]\n",
        "subprocess.run(cmd, check=True)\n",
        "\n",
        "print(\"\\n✓ All analysis complete!\")\n",
        "print(\"Check analysis/outputs/ for results\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training with New Backbones: EgoVLP & PerceptionEncoder\n",
        "\n",
        "Train MLP, Transformer, and RNN models with new video backbones.\n",
        "\n",
        "## Workflow:\n",
        "1. Clone repository and install dependencies\n",
        "2. **Load annotations from Google Drive**\n",
        "3. **Load pre-extracted features from Google Drive**\n",
        "4. Train models with EgoVLP and PerceptionEncoder backbones\n",
        "5. Evaluate and compare results\n",
        "\n",
        "## Prerequisites:\n",
        "- Set Colab secrets: `WANDB_API_KEY`\n",
        "- **Upload feature zip files** (e.g., `egovlp.zip`, `perceptionencoder.zip`) to Google Drive at `MyDrive/AML_mistake_detection/features/`\n",
        "- **Upload CaptainCook4D annotations to Google Drive** at `MyDrive/AML_mistake_detection/annotations/`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "HF_DATASET_REPO = \"aexomir/captaincook4d-features\" \n",
        "REPO_URL = \"https://github.com/aexomir/AML_mistake_detection.git\"\n",
        "REPO_BRANCH = \"feat/extend-baseline\"\n",
        "\n",
        "# Google Drive path for annotations\n",
        "ANNOTATIONS_DRIVE_PATH = '/content/drive/MyDrive/AML_mistake_detection/annotations'\n",
        "\n",
        "# Backbones to train\n",
        "BACKBONES = [\"egovlp\"]\n",
        "# Model variants\n",
        "VARIANTS = [\"MLP\", \"Transformer\", \"RNN\"]\n",
        "\n",
        "print(f\"Will train {len(BACKBONES)} backbones × {len(VARIANTS)} variants = {len(BACKBONES) * len(VARIANTS)} models\")\n",
        "print(f\"Backbones: {BACKBONES}\")\n",
        "print(f\"Variants: {VARIANTS}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Mount Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Clone Repository and Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash -s \"$REPO_URL\" \"$REPO_BRANCH\"\n",
        "cd /content\n",
        "if [ ! -d \"AML_mistake_detection\" ]; then\n",
        "    git clone --branch \"$2\" \"$1\" AML_mistake_detection\n",
        "else\n",
        "    cd AML_mistake_detection && git pull\n",
        "fi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/AML_mistake_detection')\n",
        "\n",
        "%pip install -q -r requirements.txt\n",
        "%pip install -q wandb huggingface_hub\n",
        "\n",
        "print(\"✓ Dependencies installed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Annotations from Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "print(\"Loading CaptainCook4D annotations...\")\n",
        "\n",
        "if os.path.exists(ANNOTATIONS_DRIVE_PATH):\n",
        "    print(f\"✓ Found annotations at: {ANNOTATIONS_DRIVE_PATH}\")\n",
        "    \n",
        "    # Create destination directories if they don't exist\n",
        "    os.makedirs('annotations/annotation_json', exist_ok=True)\n",
        "    os.makedirs('annotations/data_splits', exist_ok=True)\n",
        "    \n",
        "    # Copy annotation_json (step_annotations.json, error_annotations.json)\n",
        "    annotation_json_src = os.path.join(ANNOTATIONS_DRIVE_PATH, 'annotation_json')\n",
        "    if os.path.exists(annotation_json_src):\n",
        "        print(\"  Copying annotation_json files...\")\n",
        "        for file in os.listdir(annotation_json_src):\n",
        "            src = os.path.join(annotation_json_src, file)\n",
        "            dst = os.path.join('annotations/annotation_json', file)\n",
        "            if os.path.isfile(src):\n",
        "                shutil.copy2(src, dst)\n",
        "                print(f\"    ✓ Copied {file}\")\n",
        "    else:\n",
        "        print(f\"  ⚠️  annotation_json not found at {annotation_json_src}\")\n",
        "    \n",
        "    # Copy data_splits (optional)\n",
        "    data_splits_src = os.path.join(ANNOTATIONS_DRIVE_PATH, 'data_splits')\n",
        "    if os.path.exists(data_splits_src):\n",
        "        print(\"  Copying data_splits files...\")\n",
        "        for file in os.listdir(data_splits_src):\n",
        "            src = os.path.join(data_splits_src, file)\n",
        "            dst = os.path.join('annotations/data_splits', file)\n",
        "            if os.path.isfile(src):\n",
        "                shutil.copy2(src, dst)\n",
        "                print(f\"    ✓ Copied {file}\")\n",
        "    \n",
        "    # Verify required files\n",
        "    required_files = [\n",
        "        'annotations/annotation_json/step_annotations.json',\n",
        "        'annotations/annotation_json/error_annotations.json',\n",
        "        'er_annotations/recordings_combined_splits.json'\n",
        "    ]\n",
        "    \n",
        "    missing_files = [f for f in required_files if not os.path.exists(f)]\n",
        "    \n",
        "    if missing_files:\n",
        "        print(f\"\\n❌ ERROR: Missing required annotation files:\")\n",
        "        for file in missing_files:\n",
        "            print(f\"  - {file}\")\n",
        "        print(\"\\nPlease ensure these files are in your Google Drive at:\")\n",
        "        print(f\"  {ANNOTATIONS_DRIVE_PATH}/\")\n",
        "        raise FileNotFoundError(\"Missing required annotation files\")\n",
        "    else:\n",
        "        print(\"\\n✅ All required annotation files loaded successfully!\")\n",
        "        \n",
        "else:\n",
        "    print(f\"❌ ERROR: Annotations directory not found at: {ANNOTATIONS_DRIVE_PATH}\")\n",
        "    print(\"\\nPlease upload CaptainCook4D annotations to Google Drive:\")\n",
        "    print(\"  Required structure:\")\n",
        "    print(\"  MyDrive/AML_mistake_detection/annotations/\")\n",
        "    print(\"    ├── annotation_json/\")\n",
        "    print(\"    │   ├── step_annotations.json\")\n",
        "    print(\"    │   └── error_annotations.json\")\n",
        "    print(\"    └── data_splits/ (optional)\")\n",
        "    print(\"\\nDownload from: https://captaincook4d.github.io/captain-cook/\")\n",
        "    raise FileNotFoundError(\"Annotations directory not found in Google Drive\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load Features from Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Google Drive path for features zip files\n",
        "FEATURES_DRIVE_PATH = '/content/drive/MyDrive/AML_mistake_detection/features'\n",
        "\n",
        "# Create data/features directory structure\n",
        "features_base = Path('/content/AML_mistake_detection/data/features')\n",
        "\n",
        "for backbone in BACKBONES:\n",
        "    backbone_dir = features_base / backbone\n",
        "    backbone_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    zip_file_name = f'{backbone}.zip'\n",
        "    zip_file_path = os.path.join(FEATURES_DRIVE_PATH, zip_file_name)\n",
        "    \n",
        "    print(f\"Loading {backbone} features from Google Drive...\")\n",
        "    \n",
        "    # Check if zip file exists\n",
        "    if not os.path.exists(zip_file_path):\n",
        "        print(f\"❌ ERROR: Zip file not found at: {zip_file_path}\")\n",
        "        print(f\"\\nPlease ensure {zip_file_name} is uploaded to Google Drive at:\")\n",
        "        print(f\"  {FEATURES_DRIVE_PATH}/\")\n",
        "        raise FileNotFoundError(f\"Features zip file not found: {zip_file_path}\")\n",
        "    \n",
        "    print(f\"✓ Found zip file: {zip_file_path}\")\n",
        "    \n",
        "    # Extract zip file if directory is empty or doesn't exist\n",
        "    if not list(backbone_dir.glob('*.npy')):\n",
        "        print(f\"Extracting {backbone} features...\")\n",
        "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "            # Extract all files\n",
        "            zip_ref.extractall(backbone_dir)\n",
        "            # If files were extracted with subdirectory structure, move them to root\n",
        "            for root, dirs, files in os.walk(backbone_dir):\n",
        "                for file in files:\n",
        "                    if file.endswith('.npy'):\n",
        "                        src = Path(root) / file\n",
        "                        dst = backbone_dir / file\n",
        "                        if src != dst and not dst.exists():\n",
        "                            src.rename(dst)\n",
        "            # Clean up empty subdirectories\n",
        "            for root, dirs, files in os.walk(backbone_dir, topdown=False):\n",
        "                for dir_name in dirs:\n",
        "                    dir_path = Path(root) / dir_name\n",
        "                    try:\n",
        "                        if not any(dir_path.iterdir()):\n",
        "                            dir_path.rmdir()\n",
        "                    except OSError:\n",
        "                        pass\n",
        "        print(f\"✓ Extracted {backbone} features\")\n",
        "    else:\n",
        "        print(f\"✓ Features already extracted (found {len(list(backbone_dir.glob('*.npy')))} files)\")\n",
        "    \n",
        "    print(f\"✓ Total files for {backbone}: {len(list(backbone_dir.glob('*.npy')))}\")\n",
        "\n",
        "print(\"\\n✓ All features loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Setup WandB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "# Login to WandB\n",
        "wandb_key = userdata.get('WANDB_API_KEY')\n",
        "wandb.login(key=wandb_key)\n",
        "\n",
        "print(\"✓ WandB configured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Train Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Training configuration\n",
        "SPLIT = \"recordings\"\n",
        "THRESHOLD = 0.6\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "training_results = {}\n",
        "\n",
        "for backbone in BACKBONES:\n",
        "    for variant in VARIANTS:\n",
        "        model_name = f\"{variant}_{backbone}\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Training: {model_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        # Select appropriate training script\n",
        "        if variant == \"RNN\":\n",
        "            train_script = \"scripts/train_rnn_baseline.py\"\n",
        "        else:\n",
        "            train_script = \"train_er.py\"\n",
        "        \n",
        "        # Build training command\n",
        "        cmd = [\n",
        "            sys.executable, train_script,\n",
        "            \"--backbone\", backbone,\n",
        "            \"--variant\", variant,\n",
        "            \"--split\", SPLIT,\n",
        "            \"--threshold\", str(THRESHOLD),\n",
        "            \"--epochs\", str(EPOCHS),\n",
        "            \"--batch_size\", str(BATCH_SIZE),\n",
        "            \"--use_wandb\"\n",
        "        ]\n",
        "        \n",
        "        try:\n",
        "            # Run training\n",
        "            result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
        "            print(result.stdout)\n",
        "            training_results[model_name] = \"SUCCESS\"\n",
        "            print(f\"✓ {model_name} training completed\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"✗ {model_name} training failed:\")\n",
        "            print(e.stderr)\n",
        "            training_results[model_name] = \"FAILED\"\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Training Summary:\")\n",
        "print(f\"{'='*60}\")\n",
        "for model, status in training_results.items():\n",
        "    status_icon = \"✓\" if status == \"SUCCESS\" else \"✗\"\n",
        "    print(f\"{status_icon} {model}: {status}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluate Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation_results = {}\n",
        "\n",
        "for backbone in BACKBONES:\n",
        "    for variant in VARIANTS:\n",
        "        model_name = f\"{variant}_{backbone}\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Evaluating: {model_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        # Build evaluation command\n",
        "        cmd = [\n",
        "            sys.executable, \"core/evaluate.py\",\n",
        "            \"--backbone\", backbone,\n",
        "            \"--variant\", variant,\n",
        "            \"--split\", SPLIT,\n",
        "            \"--threshold\", str(THRESHOLD)\n",
        "        ]\n",
        "        \n",
        "        try:\n",
        "            # Run evaluation\n",
        "            result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
        "            print(result.stdout)\n",
        "            evaluation_results[model_name] = \"SUCCESS\"\n",
        "            print(f\"✓ {model_name} evaluation completed\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"✗ {model_name} evaluation failed:\")\n",
        "            print(e.stderr)\n",
        "            evaluation_results[model_name] = \"FAILED\"\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Evaluation Summary:\")\n",
        "print(f\"{'='*60}\")\n",
        "for model, status in evaluation_results.items():\n",
        "    status_icon = \"✓\" if status == \"SUCCESS\" else \"✗\"\n",
        "    print(f\"{status_icon} {model}: {status}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Generate Comparison Report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract metrics from this notebook's outputs\n",
        "print(\"Extracting metrics...\")\n",
        "cmd = [sys.executable, \"analysis/extract_metrics.py\"]\n",
        "subprocess.run(cmd, check=True)\n",
        "\n",
        "# Generate comparison tables\n",
        "print(\"\\nGenerating comparison tables...\")\n",
        "cmd = [sys.executable, \"analysis/comparison_tables.py\"]\n",
        "subprocess.run(cmd, check=True)\n",
        "\n",
        "# Generate visualizations\n",
        "print(\"\\nGenerating visualizations...\")\n",
        "cmd = [sys.executable, \"analysis/comparison_visualizations.py\"]\n",
        "subprocess.run(cmd, check=True)\n",
        "\n",
        "print(\"\\n✓ All analysis complete!\")\n",
        "print(\"Check analysis/outputs/ for results\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training with New Backbones: EgoVLP & PerceptionEncoder\n",
        "\n",
        "Train MLP, Transformer, and RNN models with new video backbones.\n",
        "\n",
        "## Workflow:\n",
        "1. Clone repository and install dependencies\n",
        "2. **Load pre-extracted features from Google Drive**\n",
        "3. Load annotations\n",
        "4. Train models with EgoVLP and PerceptionEncoder backbones\n",
        "5. Evaluate and compare results\n",
        "\n",
        "## Prerequisites:\n",
        "- Set Colab secrets: `WANDB_API_KEY`\n",
        "- **Upload feature zip files** (e.g., `egovlp.zip`, `perceptionencoder.zip`) to Google Drive at `MyDrive/AML_mistake_detection/features/`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "HF_DATASET_REPO = \"aexomir/captaincook4d-features\" \n",
        "REPO_URL = \"https://github.com/aexomir/AML_mistake_detection.git\"\n",
        "REPO_BRANCH = \"feat/extend-baseline\"\n",
        "\n",
        "# Backbones to train\n",
        "BACKBONES = [\"egovlp\", \"perceptionencoder\"]\n",
        "# Model variants\n",
        "VARIANTS = [\"MLP\", \"Transformer\", \"RNN\"]\n",
        "\n",
        "print(f\"Will train {len(BACKBONES)} backbones × {len(VARIANTS)} variants = {len(BACKBONES) * len(VARIANTS)} models\")\n",
        "print(f\"Backbones: {BACKBONES}\")\n",
        "print(f\"Variants: {VARIANTS}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Clone Repository and Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash -s \"$REPO_URL\" \"$REPO_BRANCH\"\n",
        "cd /content\n",
        "if [ ! -d \"AML_mistake_detection\" ]; then\n",
        "    git clone --branch \"$2\" \"$1\" AML_mistake_detection\n",
        "else\n",
        "    cd AML_mistake_detection && git pull\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/AML_mistake_detection')\n",
        "\n",
        "%pip install -q -r requirements.txt\n",
        "%pip install -q wandb huggingface_hub\n",
        "\n",
        "print(\"✓ Dependencies installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Features from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Google Drive path for features zip files\n",
        "FEATURES_DRIVE_PATH = '/content/drive/MyDrive/AML_mistake_detection/features'\n",
        "\n",
        "# Create data/features directory structure\n",
        "features_base = Path('/content/AML_mistake_detection/data/features')\n",
        "\n",
        "for backbone in BACKBONES:\n",
        "    backbone_dir = features_base / backbone\n",
        "    backbone_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    zip_file_name = f'{backbone}.zip'\n",
        "    zip_file_path = os.path.join(FEATURES_DRIVE_PATH, zip_file_name)\n",
        "    \n",
        "    print(f\"Loading {backbone} features from Google Drive...\")\n",
        "    \n",
        "    # Check if zip file exists\n",
        "    if not os.path.exists(zip_file_path):\n",
        "        print(f\"❌ ERROR: Zip file not found at: {zip_file_path}\")\n",
        "        print(f\"\\nPlease ensure {zip_file_name} is uploaded to Google Drive at:\")\n",
        "        print(f\"  {FEATURES_DRIVE_PATH}/\")\n",
        "        raise FileNotFoundError(f\"Features zip file not found: {zip_file_path}\")\n",
        "    \n",
        "    print(f\"✓ Found zip file: {zip_file_path}\")\n",
        "    \n",
        "    # Extract zip file if directory is empty or doesn't exist\n",
        "    if not list(backbone_dir.glob('*.npy')):\n",
        "        print(f\"Extracting {backbone} features...\")\n",
        "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "            # Extract all files\n",
        "            zip_ref.extractall(backbone_dir)\n",
        "            # If files were extracted with subdirectory structure, move them to root\n",
        "            for root, dirs, files in os.walk(backbone_dir):\n",
        "                for file in files:\n",
        "                    if file.endswith('.npy'):\n",
        "                        src = Path(root) / file\n",
        "                        dst = backbone_dir / file\n",
        "                        if src != dst and not dst.exists():\n",
        "                            src.rename(dst)\n",
        "            # Clean up empty subdirectories\n",
        "            for root, dirs, files in os.walk(backbone_dir, topdown=False):\n",
        "                for dir_name in dirs:\n",
        "                    dir_path = Path(root) / dir_name\n",
        "                    try:\n",
        "                        if not any(dir_path.iterdir()):\n",
        "                            dir_path.rmdir()\n",
        "                    except OSError:\n",
        "                        pass\n",
        "        print(f\"✓ Extracted {backbone} features\")\n",
        "    else:\n",
        "        print(f\"✓ Features already extracted (found {len(list(backbone_dir.glob('*.npy')))} files)\")\n",
        "    \n",
        "    print(f\"✓ Total files for {backbone}: {len(list(backbone_dir.glob('*.npy')))}\")\n",
        "\n",
        "print(\"\\n✓ All features loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Setup WandB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "# Login to WandB\n",
        "wandb_key = userdata.get('WANDB_API_KEY')\n",
        "wandb.login(key=wandb_key)\n",
        "\n",
        "print(\"✓ WandB configured\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Training configuration\n",
        "SPLIT = \"recordings\"\n",
        "THRESHOLD = 0.6\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "training_results = {}\n",
        "\n",
        "for backbone in BACKBONES:\n",
        "    for variant in VARIANTS:\n",
        "        model_name = f\"{variant}_{backbone}\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Training: {model_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        # Select appropriate training script\n",
        "        if variant == \"RNN\":\n",
        "            train_script = \"scripts/train_rnn_baseline.py\"\n",
        "        else:\n",
        "            train_script = \"train_er.py\"\n",
        "        \n",
        "        # Build training command\n",
        "        cmd = [\n",
        "            sys.executable, train_script,\n",
        "            \"--backbone\", backbone,\n",
        "            \"--variant\", variant,\n",
        "            \"--split\", SPLIT,\n",
        "            \"--threshold\", str(THRESHOLD),\n",
        "            \"--epochs\", str(EPOCHS),\n",
        "            \"--batch_size\", str(BATCH_SIZE),\n",
        "            \"--use_wandb\"\n",
        "        ]\n",
        "        \n",
        "        try:\n",
        "            # Run training\n",
        "            result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
        "            print(result.stdout)\n",
        "            training_results[model_name] = \"SUCCESS\"\n",
        "            print(f\"✓ {model_name} training completed\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"✗ {model_name} training failed:\")\n",
        "            print(e.stderr)\n",
        "            training_results[model_name] = \"FAILED\"\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Training Summary:\")\n",
        "print(f\"{'='*60}\")\n",
        "for model, status in training_results.items():\n",
        "    status_icon = \"✓\" if status == \"SUCCESS\" else \"✗\"\n",
        "    print(f\"{status_icon} {model}: {status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluate Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation_results = {}\n",
        "\n",
        "for backbone in BACKBONES:\n",
        "    for variant in VARIANTS:\n",
        "        model_name = f\"{variant}_{backbone}\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Evaluating: {model_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        # Build evaluation command\n",
        "        cmd = [\n",
        "            sys.executable, \"core/evaluate.py\",\n",
        "            \"--backbone\", backbone,\n",
        "            \"--variant\", variant,\n",
        "            \"--split\", SPLIT,\n",
        "            \"--threshold\", str(THRESHOLD)\n",
        "        ]\n",
        "        \n",
        "        try:\n",
        "            # Run evaluation\n",
        "            result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
        "            print(result.stdout)\n",
        "            evaluation_results[model_name] = \"SUCCESS\"\n",
        "            print(f\"✓ {model_name} evaluation completed\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"✗ {model_name} evaluation failed:\")\n",
        "            print(e.stderr)\n",
        "            evaluation_results[model_name] = \"FAILED\"\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Evaluation Summary:\")\n",
        "print(f\"{'='*60}\")\n",
        "for model, status in evaluation_results.items():\n",
        "    status_icon = \"✓\" if status == \"SUCCESS\" else \"✗\"\n",
        "    print(f\"{status_icon} {model}: {status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Generate Comparison Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract metrics from this notebook's outputs\n",
        "print(\"Extracting metrics...\")\n",
        "cmd = [sys.executable, \"analysis/extract_metrics.py\"]\n",
        "subprocess.run(cmd, check=True)\n",
        "\n",
        "# Generate comparison tables\n",
        "print(\"\\nGenerating comparison tables...\")\n",
        "cmd = [sys.executable, \"analysis/comparison_tables.py\"]\n",
        "subprocess.run(cmd, check=True)\n",
        "\n",
        "# Generate visualizations\n",
        "print(\"\\nGenerating visualizations...\")\n",
        "cmd = [sys.executable, \"analysis/comparison_visualizations.py\"]\n",
        "subprocess.run(cmd, check=True)\n",
        "\n",
        "print(\"\\n✓ All analysis complete!\")\n",
        "print(\"Check analysis/outputs/ for results\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
